{"title":"[PyTorch学习笔记] AUTOGRAD: Automatic Differentiation","date":"2019-07-22T19:20:28.000Z","thumbnail":"/static/image/pytorch.png","link":"pytorch-learning-note-2","comments":true,"tags":["pytorch"],"categories":["machine-learning"],"updated":"2022-01-04T08:35:48.627Z","content":"<p>本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] AUTOGRAD: Automatic Differentiation 的学习笔记</p>\n<a id=\"more\"></a>\n\n\n\n<h1 id=\"Autograd-自动求导机制\">Autograd: 自动求导机制<a href=\"pytorch-learning-note-2#Autograd-自动求导机制\"></a></h1><p>PyTorch 中所有神经网络的核心是 <code>autograd</code> 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。</p>\n<p><code>autograd</code>包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</p>\n<h2 id=\"张量-Tensor\">张量 Tensor<a href=\"pytorch-learning-note-2#张量-Tensor\"></a></h2><p><code>torch.Tensor</code>是这个包的核心类。如果设置 <code>.requires_grad</code> 为 <code>True</code>，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 <code>.backward()</code>，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 <code>.grad</code> 属性。</p>\n<p>要阻止张量跟踪历史记录，可以调用<code>.detach()</code>方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。</p>\n<p>为了防止跟踪历史记录（和使用内存），可以将代码块包装在<code>with torch.no_grad()：</code>中。 在评估模型时特别有用，因为模型可能具有<code>requires_grad = True</code>的可训练参数，但是我们不需要梯度计算。</p>\n<p>在自动梯度计算中还有另外一个重要的类<code>Function</code>.</p>\n<p><code>Tensor</code> 和 <code>Function</code>互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个<code>.grad_fn</code>属性，这个属性引用了一个创建了<code>Tensor</code>的<code>Function</code>（除非这个张量是用户手动创建的，即，这个张量的 <code>grad_fn</code> 是 <code>None</code>）。</p>\n<p>如果需要计算导数，你可以在<code>Tensor</code>上调用<code>.backward()</code>。 如果<code>Tensor</code>是一个标量（即它包含一个元素数据）则不需要为<code>backward()</code>指定任何参数， 但是如果它有更多的元素，你需要指定一个<code>gradient</code> 参数来匹配张量的形状。</p>\n<blockquote>\n<p>在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在<a href=\"https://pytorch.org/docs/stable/autograd.html#variable-deprecated\" target=\"_blank\" rel=\"noopener\">这里</a></p>\n</blockquote>\n<p>创建一个张量并设置 <code>requires_grad=True</code> 用来追踪他的计算历史</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\">y = x + <span class=\"number\">2</span>  <span class=\"comment\"># 对x进行操作</span></span><br><span class=\"line\">print(y)</span><br><span class=\"line\">print(<span class=\"string\">\"y.grad_fn: \"</span>, y.grad_fn)  <span class=\"comment\"># 结果y已经被计算出来了，所以，grad_fn已经被自动生成了</span></span><br><span class=\"line\"></span><br><span class=\"line\">z = y * y * <span class=\"number\">3</span>  <span class=\"comment\"># 对y进行操作</span></span><br><span class=\"line\">out = z.mean()</span><br><span class=\"line\">print(z)</span><br><span class=\"line\">print(out)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">1.</span>, <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [<span class=\"number\">1.</span>, <span class=\"number\">1.</span>]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">tensor([[<span class=\"number\">3.</span>, <span class=\"number\">3.</span>],</span><br><span class=\"line\">        [<span class=\"number\">3.</span>, <span class=\"number\">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">y.grad_fn:  &lt;AddBackward0 object at <span class=\"number\">0x00000202022DAE48</span>&gt;</span><br><span class=\"line\">tensor([[<span class=\"number\">27.</span>, <span class=\"number\">27.</span>],</span><br><span class=\"line\">        [<span class=\"number\">27.</span>, <span class=\"number\">27.</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class=\"line\">tensor(<span class=\"number\">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></div></figure>\n\n<p><code>.requires_grad_( ... )</code> 可以改变现有张量的 <code>requires_grad</code>属性。 如果没有指定的话，默认输入的flag是 <code>False</code>。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">a = (a * <span class=\"number\">3</span>) / (a - <span class=\"number\">1</span>)</span><br><span class=\"line\">print(a.requires_grad)</span><br><span class=\"line\">a.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(a.requires_grad)</span><br><span class=\"line\">b = (a * a).sum()</span><br><span class=\"line\">print(b.grad_fn)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\">&lt;SumBackward0 object at <span class=\"number\">0x0000028141CBADA0</span>&gt;</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"梯度\">梯度<a href=\"pytorch-learning-note-2#梯度\"></a></h2><p>反向传播 因为 <code>out</code>是一个纯量（scalar），<code>out.backward()</code> 等于<code>out.backward(torch.tensor(1))</code>。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">out.backward()</span><br></pre></td></tr></table></div></figure>\n\n<p>print gradients <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"7.158ex\" height=\"5.843ex\" style=\"vertical-align: -2.005ex;\" viewbox=\"0 -1652.5 3082 2515.6\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">\\frac{d(out)}{dx}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-75\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-74\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n<g transform=\"translate(120,0)\">\n<rect stroke=\"none\" width=\"2842\" height=\"60\" x=\"0\" y=\"220\"/>\n<g transform=\"translate(60,770)\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"523\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"913\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-75\" x=\"1398\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-74\" x=\"1971\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"2332\" y=\"0\"/>\n</g>\n<g transform=\"translate(873,-715)\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"523\" y=\"0\"/>\n</g>\n</g>\n</g>\n</svg></p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.ones(<span class=\"number\">2</span>, <span class=\"number\">2</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x + <span class=\"number\">2</span>  </span><br><span class=\"line\">z = y * y * <span class=\"number\">3</span>  </span><br><span class=\"line\">out = z.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">out.backward()</span><br><span class=\"line\">print(x.grad)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">4.5000</span>, <span class=\"number\">4.5000</span>],</span><br><span class=\"line\">        [<span class=\"number\">4.5000</span>, <span class=\"number\">4.5000</span>]])</span><br></pre></td></tr></table></div></figure>\n\n<p>可以用自动求导做更多操作</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.randn(<span class=\"number\">3</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">y = x * <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> y.data.norm() &lt; <span class=\"number\">1000</span>:</span><br><span class=\"line\">    y = y * <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(y)</span><br><span class=\"line\">gradients = torch.tensor([<span class=\"number\">0.1</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.0001</span>], dtype=torch.float)</span><br><span class=\"line\">y.backward(gradients)</span><br><span class=\"line\"></span><br><span class=\"line\">print(x.grad)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([<span class=\"number\">-790.8533</span>,  <span class=\"number\">793.1236</span>,  <span class=\"number\">307.1018</span>], grad_fn=&lt;MulBackward0&gt;)</span><br><span class=\"line\">tensor([<span class=\"number\">5.1200e+01</span>, <span class=\"number\">5.1200e+02</span>, <span class=\"number\">5.1200e-02</span>])</span><br></pre></td></tr></table></div></figure>\n\n<p>如果<code>.requires_grad=True</code>但是你又不希望进行autograd的计算， 那么可以将变量包裹在 <code>with torch.no_grad()</code>中:</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(x.requires_grad)</span><br><span class=\"line\">print((x ** <span class=\"number\">2</span>).requires_grad)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    print(x.requires_grad)</span><br><span class=\"line\">    print((x ** <span class=\"number\">2</span>).requires_grad)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"literal\">False</span></span><br></pre></td></tr></table></div></figure>\n\n<blockquote>\n<p><strong>更多阅读：</strong> autograd 和 Function 的<a href=\"https://pytorch.org/docs/autograd\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n</blockquote>\n<h2 id=\"拓展Autograd\">拓展Autograd<a href=\"pytorch-learning-note-2#拓展Autograd\"></a></h2><p>如果需要自定义autograd扩展新的功能，就需要扩展Function类。因为Function使用autograd来计算结果和梯度，并对操作历史进行编码。 在Function类中最主要的方法就是<code>forward()</code>和<code>backward()</code>他们分别代表了前向传播和反向传播。</p>\n<p>一个自定义的Function需要一下三个方法：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">__init__ (optional)：如果这个操作需要额外的参数则需要定义这个Function的构造函数，不需要的话可以忽略。</span><br><span class=\"line\"></span><br><span class=\"line\">forward()：执行前向传播的计算代码</span><br><span class=\"line\"></span><br><span class=\"line\">backward()：反向传播时梯度计算的代码。 参数的个数和forward返回值的个数一样，每个参数代表传回到此操作的梯度。</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 引入Function便于扩展</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd.function <span class=\"keyword\">import</span> Function</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个乘以常数的操作(输入参数是张量)</span></span><br><span class=\"line\"><span class=\"comment\"># 方法必须是静态方法，所以要加上@staticmethod </span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MulConstant</span><span class=\"params\">(Function)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"meta\">    @staticmethod </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(ctx, tensor, constant)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># ctx 用来保存信息这里类似self，并且ctx的属性可以在backward中调用</span></span><br><span class=\"line\">        ctx.constant=constant</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tensor *constant</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"meta\">    @staticmethod</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backward</span><span class=\"params\">(ctx, grad_output)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># 返回的参数要与输入的参数一样.</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一个输入为3x3的张量，第二个为一个常数</span></span><br><span class=\"line\">        <span class=\"comment\"># 常数的梯度必须是 None.</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> grad_output, <span class=\"literal\">None</span></span><br></pre></td></tr></table></div></figure>\n\n<p>定义完我们的新操作后，我们来进行测试</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=torch.rand(<span class=\"number\">3</span>,<span class=\"number\">3</span>,requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b=MulConstant.apply(a,<span class=\"number\">5</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"a:\"</span>+str(a))</span><br><span class=\"line\">print(<span class=\"string\">\"b:\"</span>+str(b)) <span class=\"comment\"># b为a的元素乘以5</span></span><br></pre></td></tr></table></div></figure>\n\n<p>反向传播，返回值不是标量，所以<code>backward</code>方法需要参数</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.backward(torch.ones_like(a))</span><br></pre></td></tr></table></div></figure>\n\n","prev":{"title":"[PyTorch学习笔记] Neural Networks","link":"pytorch-learning-note-3"},"next":{"title":"[PyTorch学习笔记] What is PyTorch?","link":"pytorch-learning-note-1"},"plink":"https://yuxinzhao.net/pytorch-learning-note-2/","toc":[{"title":"Autograd: 自动求导机制","id":"Autograd-自动求导机制","index":"1","children":[{"title":"张量 Tensor","id":"张量-Tensor","index":"1.1"},{"title":"梯度","id":"梯度","index":"1.2"},{"title":"拓展Autograd","id":"拓展Autograd","index":"1.3"}]}],"reward":true,"copyright":{"author":"Yuxin Zhao","link":"<a href=\"https://yuxinzhao.net/pytorch-learning-note-2/\" title=\"[PyTorch学习笔记] AUTOGRAD: Automatic Differentiation\">https://yuxinzhao.net/pytorch-learning-note-2/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}