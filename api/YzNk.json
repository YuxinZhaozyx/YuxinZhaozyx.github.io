{"title":"[论文笔记] Learning Spatiotemporal Features with 3D Convolutional Networks","date":"2019-02-18T15:36:20.000Z","link":"c3d","comments":true,"tags":["C3D","action-recognition","action-similarity-labeling","object-recognition","scene-recognition"],"categories":["paper-note"],"updated":"2022-01-04T08:35:48.539Z","content":"<p>The <a href=\"https://arxiv.org/pdf/1412.0767.pdf\" title=\"Learning Spatiotemporal Features with 3D Convolutional Networks\" target=\"_blank\" rel=\"noopener\">paper</a> is by Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri.</p>\n<p><strong>Publication date :</strong> 7 Oct 2015</p>\n<p><strong>C3D source code and pre-trained model</strong> are available at <a href=\"http://vlg.cs.dartmouth.edu/c3d\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<a id=\"more\"></a>\n\n\n\n<h1 id=\"Basic-Idea\">Basic Idea<a href=\"c3d#Basic-Idea\"></a></h1><p>The authors build <strong>C3D (Convolutional 3D)</strong> model to obtain a <strong>generic, compact, efficient, and simple video descriptor</strong>.</p>\n<p>The authors’s contributions in this paper are:</p>\n<ul>\n<li>They show 3D convolutional deep networks are good feature learning machines that <strong>model appearance and motion simultaneously</strong>.</li>\n<li>They find that <strong>3 × 3 × 3 convolution kernel for all layers to work best</strong> among the limited set of explored architectures. </li>\n<li>The proposed features with a simple linear model outperform or approach the current best methods on 4 different tasks and 6 different benchmarks (see Table 1). They are also <strong>compact and efficient to compute</strong>.</li>\n</ul>\n<p><img src=\"c3d/Table%201.png\" alt=\"Table 1\" class=\"article-img\"></p>\n<h2 id=\"3D-Convolution-and-3D-Pooling\">3D Convolution and 3D Pooling<a href=\"c3d#3D-Convolution-and-3D-Pooling\"></a></h2><p>The difference between 2D and 3D convolution operations is showed in Figure 1.</p>\n<p><img src=\"c3d/Figure%201.png\" alt=\"Figure 1\" class=\"article-img\"></p>\n<h3 id=\"Notations\">Notations<a href=\"c3d#Notations\"></a></h3><p><strong>Video Clip size:</strong>    <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"13.224ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 5693.8 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">c \\times l \\times h \\times w</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-63\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-68\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-77\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"655\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"1656\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"2177\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-68\" x=\"3177\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"3976\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-77\" x=\"4977\" y=\"0\"/>\n</g>\n</svg>   (<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.007ex\" height=\"1.676ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -576.1 433.5 721.6\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">c</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-63\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n</g>\n</svg> is the number of channels, <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"0.693ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 298.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">l</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n</g>\n</svg> is length in number of frames, <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.339ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 576.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">h</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-68\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-68\" x=\"0\" y=\"0\"/>\n</g>\n</svg> and <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.664ex\" height=\"1.676ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -576.1 716.5 721.6\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">w</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-77\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-77\" x=\"0\" y=\"0\"/>\n</g>\n</svg> are the height and width of the frame, respectively)</p>\n<p><strong>3D convolution and pooling kernel size:</strong>   <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"9.319ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 4012.4 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">d \\times k \\times k</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6B\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"745\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6B\" x=\"1746\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-D7\" x=\"2490\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6B\" x=\"3490\" y=\"0\"/>\n</g>\n</svg>   (<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.216ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 523.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">d</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n</g>\n</svg> is kernel temporal depth, <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.211ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 521.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">k</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6B\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6B\" x=\"0\" y=\"0\"/>\n</g>\n</svg> is kernel spatial size)</p>\n<h1 id=\"Exploring-kernel-temporal-depth\">Exploring kernel temporal depth<a href=\"c3d#Exploring-kernel-temporal-depth\"></a></h1><h2 id=\"Common-network-settings\">Common network settings<a href=\"c3d#Common-network-settings\"></a></h2><p>All video frames are resized into 128 × 171, and then the authors use jittering by using <strong>random crops</strong> with a size of <strong>3 × 16 × 112 × 112</strong> of the input clips during training.</p>\n<p><img src=\"c3d/3D%20Networks.png\" alt=\"3D Networks\" class=\"article-img\"></p>\n<p>The networks have 5 convolution layers and 5 pooling layers (each convolution layer is immediately followed by a pooling layer), 2 fully-connected layers and a softmax loss layer to predict action labels.</p>\n<p><strong>All convolution kernel size:</strong>   <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.216ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 523.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">d</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n</g>\n</svg> × 3 × 3   (the authors vary the value of <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.216ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 523.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">d</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n</g>\n</svg> to search for a good 3D architecture)</p>\n<p><strong>All pooling kernel size:</strong>   2 × 2 × 2   (except for the Pool1 whose size is 1 × 2 × 2, not to merge the temporal signal too early)</p>\n<p><strong>training:</strong> </p>\n<ul>\n<li>from scratch</li>\n<li>mini-batch of 30 clips</li>\n<li>Initial learning rate: 0.003. The learning rate is divided by 10 after every 4 epochs. The train is stopped after 16 epochs. </li>\n<li>dataset: UCF101</li>\n</ul>\n<h2 id=\"Varying-network-architectures\">Varying network architectures<a href=\"c3d#Varying-network-architectures\"></a></h2><p>The author experiment with two types of architectures:</p>\n<ul>\n<li><strong>homogeneous temporal depth</strong><ul>\n<li>all convolution layers have the same kernel temporal depth.</li>\n<li>The authors experiment with 4 networks having kernel temporal depth of <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.216ex\" height=\"2.176ex\" style=\"vertical-align: -0.338ex;\" viewbox=\"0 -791.3 523.5 936.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">d</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-64\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-64\" x=\"0\" y=\"0\"/>\n</g>\n</svg> equal to 1, 3, 5, and 7, and name these networks as <strong>depth-d</strong>.</li>\n<li><strong>depth-1, depth-3, depth-5, depth-7</strong>.</li>\n<li>depth-1 is the same as 2D ConvNet.</li>\n</ul>\n</li>\n<li><strong>varying temporal depth</strong><ul>\n<li>kernel temporal depth is changing across the layers.</li>\n<li>The authors experiment 2 networks with temporal depth <strong>increasing</strong>: 3-3-5-5-7 and <strong>decreasing</strong>: 7-5-5-3-3 from the first to the fifth convolution layer respectively.</li>\n<li><strong>increase, decrease.</strong></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Result\">Result<a href=\"c3d#Result\"></a></h2><p><img src=\"c3d/Figure%202.png\" alt=\"Figure 2\" class=\"article-img\"></p>\n<p>2D ConvNet performs worst and 3D ConvNet with 3 × 3 × 3 kernels performs best among the experimented nets.</p>\n<p>The best kernel temporal depth is 3.</p>\n<h1 id=\"C3D-Convolutional-3D\">C3D (Convolutional 3D)<a href=\"c3d#C3D-Convolutional-3D\"></a></h1><h2 id=\"C3D-architecture\">C3D architecture<a href=\"c3d#C3D-architecture\"></a></h2><p><img src=\"c3d/C3D.png\" alt=\"C3D\" class=\"article-img\"></p>\n<p>C3D net has 8 convolution, 5 max-pooling, and 2 fully connected layers, followed by a softmax output layer. All 3D convolution kernels are 3 × 3 × 3 with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from <em>pool1</em> to <em>pool5</em>. All pooling kernels are 2 × 2 × 2, except for <em>pool1</em> is 1 × 2 × 2 with the intention of preserving the temporal information in the early phase. Each fully connected layer has 4096 output units.</p>\n<h2 id=\"Dataset\">Dataset<a href=\"c3d#Dataset\"></a></h2><p><strong>Dataset:</strong>   Sports-1M</p>\n<ul>\n<li>1.1 million sport videos.</li>\n<li>each video belongs to one of 487 sports categories.</li>\n</ul>\n<h2 id=\"Training\">Training<a href=\"c3d#Training\"></a></h2><p><strong>input:</strong></p>\n<ul>\n<li>randomly extract five 2-second long clips from every training video.</li>\n<li>Clips are resized to have a frame size of 128 × 171.</li>\n<li>On training, clips are randomly crop into 16 × 112 × 112 crops for spatial and temporal jittering.</li>\n<li>horizontally flip clips with 50% probability.</li>\n</ul>\n<p><strong>optimizer:</strong></p>\n<ul>\n<li>SGD with mini-batch size of 30 examples.</li>\n<li>Initial learning rate is 0.003, and is divided by 2 every 150K iterations. The optimization is stopped at 1.9M iterations (about 13 epochs).</li>\n</ul>\n<p>The authors experiment with C3D net trained from scratch and C3D net fine-tuned from the model pre-trained on I380K dataset.</p>\n<h2 id=\"Result-1\">Result<a href=\"c3d#Result-1\"></a></h2><p><img src=\"c3d/Table%202.png\" alt=\"Table 2\" class=\"article-img\"></p>\n<h2 id=\"C3D-video-descriptor\">C3D video descriptor<a href=\"c3d#C3D-video-descriptor\"></a></h2><p>After training, C3D can be used as a feature extractor for other video analysis task.</p>\n<ul>\n<li>To extract C3D feature, <strong>a video is split into 16 frame long clips with a 8-frame overlap between two consecutive clips</strong>.</li>\n<li>These clips are passed to the C3D network to <strong>extract <em>fc6</em> activations</strong>.</li>\n<li>These clip <strong><em>fc6</em> activations are averaged to form a 4096-dim video descriptor/feature</strong> which is then followed by an <strong>L2-normalization</strong>.</li>\n</ul>\n<h2 id=\"C3D-is-compact\">C3D is compact<a href=\"c3d#C3D-is-compact\"></a></h2><p>The authors use PCA to project the features into lower dimensions and report the classification accuracy of the projected features on UCF101 using a linear SVM.</p>\n<p><img src=\"c3d/Figure%205.png\" alt=\"Figure 5\" class=\"article-img\"></p>\n<h1 id=\"Action-Recogniton\">Action Recogniton<a href=\"c3d#Action-Recogniton\"></a></h1><h2 id=\"Dataset-1\">Dataset<a href=\"c3d#Dataset-1\"></a></h2><p>the authors use the tree split setting provided with UCF101 dataset which consists of 13,320 videos of 101 human action categories.</p>\n<h2 id=\"Model\">Model<a href=\"c3d#Model\"></a></h2><p>The authors <strong>extract C3D features</strong> and input them to a <strong>multi-class linear SVM</strong> for training models.</p>\n<ul>\n<li><strong>C3D (1 net) + linear SVM</strong><ul>\n<li>C3D is trained on I380K dataset</li>\n</ul>\n</li>\n<li><strong>C3D (3 nets) + linear SVM</strong><ul>\n<li>use 3 different nets:<ul>\n<li>C3D trained on 1380K dataset</li>\n<li>C3D trained on Sports-1M dataset</li>\n<li>C3D trained on I380K and fine-tuned on Sports-1M</li>\n</ul>\n</li>\n<li><strong>concatenate the L2-normalized C3D features</strong> of these nets.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Result-2\">Result<a href=\"c3d#Result-2\"></a></h2><p><img src=\"c3d/Table%203.png\" alt=\"Table 3\" class=\"article-img\"></p>\n<h1 id=\"Action-Similarity-Labeling\">Action Similarity Labeling<a href=\"c3d#Action-Similarity-Labeling\"></a></h1><h2 id=\"Dataset-2\">Dataset<a href=\"c3d#Dataset-2\"></a></h2><p>The ASLAN dataset consists of 3,631 videos from 432 action classes. The task is to predict if a given pair of videos belong to the same or different action.</p>\n<p>The authors use the prescribed 10-fold cross validation with the splits provided with the dataset.</p>\n<h2 id=\"Features\">Features<a href=\"c3d#Features\"></a></h2><ul>\n<li><strong>split videos into 16-frame clips with an overlap of 8 frames</strong></li>\n<li><strong>extract C3D features: <em>prob(softmax), fc7, fc6, pool5</em> for each clip</strong>.</li>\n<li><strong>averaging the clip features separately for each type of feature</strong>, followed by an <strong>L2 normalization</strong>.</li>\n</ul>\n<h2 id=\"Model-1\">Model<a href=\"c3d#Model-1\"></a></h2><p>The authors use the model of Kliper-Gross et al. to <strong>compute the 12 different distances</strong>.</p>\n<p>With 4 types of features, the authors <strong>obtain 48-dimensional feature vector (12 × 4 = 48) for each video pair</strong>. </p>\n<p>As these 48 distances are not comparable to each other, the authors <strong>normalize them independently such that each dimension has zero mean and unit variance</strong>.</p>\n<p><strong>A linear SVM is trained to classify video pairs into same or different</strong> on these 48-dim feature vectors.</p>\n<h2 id=\"Result-3\">Result<a href=\"c3d#Result-3\"></a></h2><p><img src=\"c3d/Table%204.png\" alt=\"Table 4\" class=\"article-img\"></p>\n<p><img src=\"c3d/Figure%207.png\" alt=\"Figure 7\" class=\"article-img\"></p>\n<h1 id=\"Scene-and-Object-Recognition\">Scene and Object Recognition<a href=\"c3d#Scene-and-Object-Recognition\"></a></h1><h2 id=\"Dataset-3\">Dataset<a href=\"c3d#Dataset-3\"></a></h2><p><strong>Scene recognition:</strong></p>\n<ul>\n<li><strong>YUPENN</strong>: 420 videos of 14 scene categories.</li>\n<li><strong>Maryland</strong>: 130 videos of 13 scene categories.</li>\n</ul>\n<p><strong>Object recogniton:</strong></p>\n<ul>\n<li>an egocentric dataset which consists 42 types of everyday objects.</li>\n</ul>\n<h2 id=\"Model-2\">Model<a href=\"c3d#Model-2\"></a></h2><p><strong>Scene recognition:</strong></p>\n<ul>\n<li>use the same setup of feature extraction and linear SVM for classification and follow the same leave-one-out evaluation protocol as described by the authors of these datasets.</li>\n</ul>\n<p><strong>Object recognition:</strong></p>\n<ul>\n<li>slide a window of 16 frames to extract C3D features. </li>\n<li>choose the ground truth label for each clip to be the most frequently occurring label of the clip. If the most frequent label in a clip occurs fewer than 8 frames, the authors consider it as negative clip with no object and discard it in both training and testing.</li>\n<li>train and test C3D features using linear SVM.</li>\n</ul>\n<h2 id=\"Result-4\">Result<a href=\"c3d#Result-4\"></a></h2><p><strong>Scene recognition:</strong></p>\n<p><img src=\"c3d/Table%205.png\" alt=\"Table 5\" class=\"article-img\"></p>\n<p><strong>Object recognition:</strong></p>\n<p>C3D obtains 22.3% accuracy and outperforms by 10.3% with only linear SVM where the comparing method used RBF-kernel  on strong SIFT-RANSAC feature matching. Compared with Imagenet baseline, C3D is still 3.4% worse.</p>\n<h1 id=\"Runtime-Analysis\">Runtime Analysis<a href=\"c3d#Runtime-Analysis\"></a></h1><p>The authors report runtime of the three above-mentioned methods to extract features (including I/O) for the whole UCF101 dataset using a single CPU or a single K40 Tesla GPU.</p>\n<p><img src=\"c3d/Table%206.png\" alt=\"Table 6\" class=\"article-img\"></p>\n<p>C3D is much faster than real-time, processing at <strong>313 fps</strong> while the other two methods have a processing speed of less than 4 fps.</p>\n","prev":{"title":"[论文笔记] Long-term Recurrent Convolutional Networks for Visual Recognition and Description","link":"lrcn"},"plink":"https://yuxinzhao.net/c3d/","toc":[{"title":"Basic Idea","id":"Basic-Idea","index":"1","children":[{"title":"3D Convolution and 3D Pooling","id":"3D-Convolution-and-3D-Pooling","index":"1.1","children":[{"title":"Notations","id":"Notations","index":"1.1.1"}]}]},{"title":"Exploring kernel temporal depth","id":"Exploring-kernel-temporal-depth","index":"2","children":[{"title":"Common network settings","id":"Common-network-settings","index":"2.1"},{"title":"Varying network architectures","id":"Varying-network-architectures","index":"2.2"},{"title":"Result","id":"Result","index":"2.3"}]},{"title":"C3D (Convolutional 3D)","id":"C3D-Convolutional-3D","index":"3","children":[{"title":"C3D architecture","id":"C3D-architecture","index":"3.1"},{"title":"Dataset","id":"Dataset","index":"3.2"},{"title":"Training","id":"Training","index":"3.3"},{"title":"Result","id":"Result-1","index":"3.4"},{"title":"C3D video descriptor","id":"C3D-video-descriptor","index":"3.5"},{"title":"C3D is compact","id":"C3D-is-compact","index":"3.6"}]},{"title":"Action Recogniton","id":"Action-Recogniton","index":"4","children":[{"title":"Dataset","id":"Dataset-1","index":"4.1"},{"title":"Model","id":"Model","index":"4.2"},{"title":"Result","id":"Result-2","index":"4.3"}]},{"title":"Action Similarity Labeling","id":"Action-Similarity-Labeling","index":"5","children":[{"title":"Dataset","id":"Dataset-2","index":"5.1"},{"title":"Features","id":"Features","index":"5.2"},{"title":"Model","id":"Model-1","index":"5.3"},{"title":"Result","id":"Result-3","index":"5.4"}]},{"title":"Scene and Object Recognition","id":"Scene-and-Object-Recognition","index":"6","children":[{"title":"Dataset","id":"Dataset-3","index":"6.1"},{"title":"Model","id":"Model-2","index":"6.2"},{"title":"Result","id":"Result-4","index":"6.3"}]},{"title":"Runtime Analysis","id":"Runtime-Analysis","index":"7"}],"reward":true,"copyright":{"author":"Yuxin Zhao","link":"<a href=\"https://yuxinzhao.net/c3d/\" title=\"[论文笔记] Learning Spatiotemporal Features with 3D Convolutional Networks\">https://yuxinzhao.net/c3d/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}