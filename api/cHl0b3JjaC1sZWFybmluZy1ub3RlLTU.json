{"title":"[PyTorch学习笔记] Data Parallelism","date":"2019-07-23T11:10:54.000Z","thumbnail":"/static/image/pytorch.png","link":"pytorch-learning-note-5","comments":true,"tags":["pytorch"],"categories":["machine-learning"],"updated":"2022-01-04T08:35:48.627Z","content":"<p>本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] Data Parallelism 的学习笔记</p>\n<p>本文将学习如何使用 <code>DataParallel</code> 来使用多GPU。</p>\n<a id=\"more\"></a>\n\n\n\n\n\n<p>PyTorch非常容易就可以使用多GPU，用如下方式把一个模型放到GPU上：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">\"cuda:0\"</span>)</span><br><span class=\"line\">model.to(device)</span><br></pre></td></tr></table></div></figure>\n\n<p>然后复制所有的张量到GPU上：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></div></figure>\n\n<blockquote>\n<p>只调用<code>my_tensor.to(device)</code>并没有复制张量到GPU上，而是返回了一个copy。所以你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p>\n</blockquote>\n<p>在多GPU上执行前向和反向传播是自然而然的事。 但是<strong>PyTorch默认将只使用一个GPU</strong>。</p>\n<p>使用<code>DataParallel</code>可以轻易的让模型并行运行在多个GPU上。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = nn.DataParallel(model)</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"导入模块和定义参数\">导入模块和定义参数<a href=\"pytorch-learning-note-5#导入模块和定义参数\"></a></h2><figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Parameters and DataLoaders</span></span><br><span class=\"line\">input_size = <span class=\"number\">5</span></span><br><span class=\"line\">output_size = <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">30</span></span><br><span class=\"line\">data_size = <span class=\"number\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Device</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">\"cuda:0\"</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">\"cpu\"</span>)</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"虚拟数据集\">虚拟数据集<a href=\"pytorch-learning-note-5#虚拟数据集\"></a></h2><p>制作一个虚拟（随机）数据集， 你只需实现 <code>__getitem__</code></p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RandomDataset</span><span class=\"params\">(Dataset)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, length)</span>:</span></span><br><span class=\"line\">        self.len = length</span><br><span class=\"line\">        self.data = torch.randn(length, size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.data[index]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.len</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"简单模型\">简单模型<a href=\"pytorch-learning-note-5#简单模型\"></a></h2><p>作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。 说明：<code>DataParallel</code>能在任何模型（CNN，RNN，Capsule Net等）上使用。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Model</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, input_size, output_size)</span>:</span></span><br><span class=\"line\">        super(Model, self).__init__()</span><br><span class=\"line\">        self.fc = nn.Linear(input_size, output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, input)</span>:</span></span><br><span class=\"line\">        output = self.fc(input)</span><br><span class=\"line\">        print(<span class=\"string\">\"\\t In Model: input size\"</span>, input.size(), <span class=\"string\">\"output size\"</span>, output.size())</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"创建一个模型和数据并行\">创建一个模型和数据并行<a href=\"pytorch-learning-note-5#创建一个模型和数据并行\"></a></h2><p>首先，我们需要创建一个模型实例和检测我们是否有多个GPU。 如果有多个GPU，使用<code>nn.DataParallel</code>来包装我们的模型。 然后通过<code>model.to(device)</code>把模型放到GPU上。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = Model(input_size, output_size)</span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.device_count() &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">    print(<span class=\"string\">\"Let's use\"</span>, torch.cuda.device_count(), <span class=\"string\">\"GPUs!\"</span>)</span><br><span class=\"line\">    <span class=\"comment\"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class=\"line\">    model = nn.DataParallel(model)</span><br><span class=\"line\"></span><br><span class=\"line\">model.to(device)</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"运行模型\">运行模型<a href=\"pytorch-learning-note-5#运行模型\"></a></h2><figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> rand_loader:</span><br><span class=\"line\">    input = data.to(device)</span><br><span class=\"line\">    output = model(input)</span><br><span class=\"line\">    print(<span class=\"string\">\"Outside: input size\"</span>, input.size(), <span class=\"string\">\"output_size\"</span>, output.size())</span><br></pre></td></tr></table></div></figure>\n\n<p>当没有或者只有一个GPU时，对30个输入和输出进行批处理，得到了期望的一样得到30个输入和输出，但是如果你有多个GPU，你得到如下的结果。</p>\n<p><strong>2 GPUs</strong></p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Let's use 2 GPUs!</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></div></figure>\n\n","prev":{"title":"[PyTorch学习笔记] 数据的加载和预处理","link":"pytorch-learning-note-6"},"next":{"title":"[PyTorch学习笔记] Training A Classifier","link":"pytorch-learning-note-4"},"plink":"https://yuxinzhao.net/pytorch-learning-note-5/","toc":[{"title":"导入模块和定义参数","id":"导入模块和定义参数","index":"1"},{"title":"虚拟数据集","id":"虚拟数据集","index":"2"},{"title":"简单模型","id":"简单模型","index":"3"},{"title":"创建一个模型和数据并行","id":"创建一个模型和数据并行","index":"4"},{"title":"运行模型","id":"运行模型","index":"5"}],"reward":true,"copyright":{"author":"Yuxin Zhao","link":"<a href=\"https://yuxinzhao.net/pytorch-learning-note-5/\" title=\"[PyTorch学习笔记] Data Parallelism\">https://yuxinzhao.net/pytorch-learning-note-5/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}