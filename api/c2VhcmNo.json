[{"title":"NP完全理论","date":"2022-01-03T14:20:41.000Z","updated":"2022-01-04T08:35:48.611Z","content":"本文是《近世计算理论导引：NP难度问题的背景、前景及其求解方法研究》的第三章《NP完全理论》的笔记。\n\n\n增长速度只考虑满足以下条件的数论函数 \nf\n\n\n\n\n \n\n:\n\n\\lim_{n\\to +\\infty} f(n)= +\\infty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n \n\n \n \n \n \n \n \n \n\n\n定义1：\nf(n) = O(g(n))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n，其中 \nf\n\n\n\n\n \n\n, \ng\n\n\n\n\n \n\n 为从 \n\\mathbb{N}\n\n\n\n\n \n\n 到 \n\\mathbb{N}\n\n\n\n\n \n\n 的函数，是指存在 \nC &gt; 0\n\n\n\n\n\n\n \n \n \n\n ，使得 \nf(n) \\le Cg(n)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n 对于充分大的 \nn\n\n\n\n\n \n\n 一致成立。\n\n形式化表示为 \n\\exists C \\in \\mathbb{R}^+, \\exists k &gt; 0, \\forall n &gt; k: f(n) \\le C(g(n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 。\n\n定理1：设 \nf\n\n\n\n\n \n\n, \ng\n\n\n\n\n \n\n 为 \n\\mathbb{N} \\mapsto \\mathbb{N}\n\n\n\n\n\n \n \n \n\n 的全函数，\n\\lim_{n\\to +\\infty} f(n) = + \\infty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n \n\n \n \n \n \n \n \n \n\n, \n\\lim_{n\\to +\\infty} g(n) = + \\infty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n \n\n \n \n \n \n \n \n \n\n，且 \n\\lim_{n\\to +\\infty} \\frac{f(n)}{g(n)}= \\beta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n \n\n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n \n\n，则\n\n若 \n\\beta \\in \\mathbb{R}^+\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，则 \nf\n\n\n\n\n \n\n，\ng\n\n\n\n\n \n\n 有相同的增长速率。\n若 \n\\beta = +\\infty\n\n\n\n\n\n\n\n \n \n \n \n\n，则 \nf\n\n\n\n\n \n\n 比 \ng\n\n\n\n\n \n\n 增长快。\n若 \n\\beta = 0\n\n\n\n\n\n\n \n \n \n\n，则 \ng\n\n\n\n\n \n\n 比 \nf\n\n\n\n\n \n\n 增长慢。\n\n记 \np(n)=a_r n^r+a_{r-1}n^{r-1}+\\cdots+a_0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n \n \n\n \n\n \n\n \n \n \n\n\n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n\n，其中 \na_r \\in \\mathbb{R}^+\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，\n\\forall i \\in (0,r-1): a_i \\in \\mathbb{R}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n\n，称 \nr\n\n\n\n\n \n\n 为多项式 \np\n\n\n\n\n \n\n 的次数。\n定理2：设 \np_1(n)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的次数为 \nr_1\n\n\n\n\n\n \n \n\n，\np_2(n)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的次数为 \nr_2\n\n\n\n\n\n \n \n\n，则\n\n若 \nr_1 = r_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则 \np_1\n\n\n\n\n\n \n \n\n 和 \np_2\n\n\n\n\n\n \n \n\n 增长速度相同。\n若 \nr_1 &gt; r_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则 \np_1\n\n\n\n\n\n \n \n\n 比 \np_2\n\n\n\n\n\n \n \n\n 增长速度快。\n若 \nr_1 &lt; r_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则 \np_2\n\n\n\n\n\n \n \n\n 比 \np_1\n\n\n\n\n\n \n \n\n 增长速度快。\n\n定理3：函数 \nk^n (k&gt;1)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 比任何多项式的增长速度快。\nP 和 NPP问题记 Turing机 \nT\n\n\n\n\n \n\n 接收输入 \nx\n\n\n\n\n \n\n 所历的机器运行步数为 \nstep(x)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n。 \n定义1：多项式时间可判定的语言 \nL\n\n\n\n\n \n\n\n\n满足以下条件\n\n存在一个字符表为 \nA\n\n\n\n\n \n\n 的Turing机 \nT\n\n\n\n\n \n\n 接受语言 \nL\n\n\n\n\n \n\n。\n\n存在一个多项式 \np(n)\n\n\n\n\n\n\n\n \n \n \n \n\n 使得\n\n\\forall x \\in L:step(x) \\le p(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中，\nstep(x)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 称为计算时间，\n|x|\n\n\n\n\n\n \n \n \n\n 表示 \nx\n\n\n\n\n \n\n 的字长。则称 \nL\n\n\n\n\n \n\n 是多项式时间可判定的。\n\n若存在机器 \nT\n\n\n\n\n \n\n，能够在多项式时间内解决 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n 的问题，则 \nL\n\n\n\n\n \n\n 是多项式时间可判定的。\n\n\n定义2：当字符表 \nA\n\n\n\n\n \n\n 确定后，我们用 \nP\n\n\n\n\n \n\n 表示一切“多项式时间可判定的语言”所构成的类。\n\n\nP=\\{L|\\exists T,\\exists p ,\\forall x\\in L: step(x) \\le p(|x|) \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n说 \nQ \\in P\n\n\n\n\n\n\n \n \n \n\n 指的是 \nx \\in ? Q\n\n\n\n\n\n\n\n \n \n \n \n\n 这个问题是多项式时间可判定的。\n\n\n定义3：多项式时间可计算的函数 \nf\n\n\n\n\n \n\n\n\n设 \nf\n\n\n\n\n \n\n 为 \nA^* \\mapsto A^*\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 的一个全函数，\nA\n\n\n\n\n \n\n 为字符表。\n\n存在一个多项式 \np(n)\n\n\n\n\n\n\n\n \n \n \n \n\n 使得\n\n\\forall x \\in A^*:step(x) \\le p(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中，\nstep(x)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 称为计算时间，\n|x|\n\n\n\n\n\n \n \n \n\n 表示 \nx\n\n\n\n\n \n\n 的字长。则称 \nf\n\n\n\n\n \n\n 是多项式时间可计算的。\n\n\n合并定义：\n\n说语言 \nL\n\n\n\n\n \n\n 是多项式时间可判定的或函数 \nf\n\n\n\n\n \n\n 是多项式时间可计算的只需\n存在一个Turing机完成计算任务的计算步数 \nstep(x)\\le p(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 对充分大的 \n|x|\n\n\n\n\n\n \n \n \n\n 一致地成立，其中 \n|x|\n\n\n\n\n\n \n \n \n\n 为输入的字。\n存在一个Turing机完成计算任务的计算步数 \nstep(x)=O(|x|^r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n, \nr \\in \\mathbb{N}^+\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n。\n\n\n多项式时间可判定的语言 \nL\n\n\n\n\n \n\n 和多项式时间可计算的函数 \nf\n\n\n\n\n \n\n 的定义区别在于需要Turing机完成的计算任务不同。\n多项式时间可判定的语言 \nL\n\n\n\n\n \n\n 的定义要求的计算任务是判断 \nx\\in?L\n\n\n\n\n\n\n\n \n \n \n \n\n，即要求对 \nx\\in L\n\n\n\n\n\n\n \n \n \n\n 满足。\n多项式时间可计算的函数 \nf\n\n\n\n\n \n\n 的定义要求的计算任务是对全体 \nx \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n 都满足。\n\n\n\nCook-Karp论题：任给字符表 \nA\n\n\n\n\n \n\n，对 \nA^*\n\n\n\n\n\n \n \n\n 上任一全函数 \nf:A^* \\mapsto A^*\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n，若由某种装置能在多项式时间内计算函数 \nf\n\n\n\n\n \n\n，则必存在一台 Turing 机能在多项式时间内计算函数 \nf\n\n\n\n\n \n\n。\n定理1：设 \nL \\in P\n\n\n\n\n\n\n \n \n \n\n, \nf\n\n\n\n\n \n\n 为从 \nA^*\n\n\n\n\n\n \n \n\n 到 \nA^*\n\n\n\n\n\n \n \n\n 多项式时间可计算函数， \nQ=\\{x\\in A^*|f(x) \\in L\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n\n，则 \nQ\\in P\n\n\n\n\n\n\n \n \n \n\n。\n\n\n\n此过程关键在于判断 \nx \\in? Q\n\n\n\n\n\n\n\n \n \n \n \n\n 这个问题是不是多项式时间可判定的。由于 \nf\n\n\n\n\n \n\n 是多项式时间可计算的，\nL\\in P\n\n\n\n\n\n\n \n \n \n\n，只需要将 \nx\n\n\n\n\n \n\n 在多项式时间内映射到 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n，再在多项式时间内判定 \nf(x) \\in ? L\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 即可。\n\n由于 \nf\n\n\n\n\n \n\n 是多项式时间可计算的，需要时间 \n\\le p_1(|x|)\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n\n；由于 \nL\\in P\n\n\n\n\n\n\n \n \n \n\n，因此 \nf(x) \\in ? L\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 这个问题是多项式时间可判定的，即判断所需要时间 \n\\le p_2(|f(x)|)\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n\n。\n\n联系Turing机的运行步骤，由于 \nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 最多运行 \np_1(|x|)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 步，最多移动 \np_1(|x|)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 格，因此 \n|f(x)| \\le |x| + p_1(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n。\n\n于是判定 \nx \\in ? Q\n\n\n\n\n\n\n\n \n \n \n \n\n 的运行时间为\n\np_1(|x|) + p_2(|f(x)|) \\le p_1(|x|) + p_2(|x| + p_1(|x|)) \\triangleq p_3(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n定理2：设 \nf\n\n\n\n\n \n\n, \ng\n\n\n\n\n \n\n 为多项式时间可计算的部分函数，其中 \ng\n\n\n\n\n \n\n 为全函数，而 \nf\n\n\n\n\n \n\n 可为真部分函数，即可在某些点上无定义，使得 \nh(x) \\triangleq f(g(x))\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n 为全函数，则 \nh\n\n\n\n\n \n\n 为多项式时间内可计算的全函数。\n\n\n\n计算需要时间为\n\np_g(|x|) +p_f(|g(x)|) \\le p_f(|x|) + p_f(|x|+p_g(|x|)) \\triangleq p_h(|x|)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n不确定型Turing机与NP问题补充定义：不确定型Turing机\n\n第一章中定义的Turing机要求Turing机的行为准则集（四元组）中前两个元素（当前内部状态和当前指针指向的值）在所有行为准则集中不允许重复。将该条件放宽为允许重复后该Turing机即为不确定型Turing机，而原先不允许重复的Turing机称为确定型Turing机。\n\n不确定型Turing机会遇到当前内部状态和当前指针指向的值有对应的多条行为准则，因此Turing机的下一步可以在其中选择一条执行。\n\n确定型Turing机是不确定型Turing机的子集。\n\n格局(瞬像)序列 \n\\gamma_1,\\cdots,\\gamma_m (m\\ge1)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n 是Turing机 \nT\n\n\n\n\n \n\n 接收字 \nx \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n 的一条接收路径，其中 \n\\gamma_1\n\n\n\n\n\n \n \n\n 是初始格局，\n\\gamma_m\n\n\n\n\n\n \n \n\n 是停机格局。\n\n不确定型Turing机所接收的语言的重新定义：设 \nA=\\{ s_1,\\cdots, s_n\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 是给定的字符表，\nx \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，Turing机 \nT\n\n\n\n\n \n\n 的字符表包含 \nA\n\n\n\n\n \n\n。若存在一条 \nT\n\n\n\n\n \n\n 接收 \nx\n\n\n\n\n \n\n 的接收路径，则称 \nT\n\n\n\n\n \n\n 接收 \nx\n\n\n\n\n \n\n。如果Turing机的字母表为 \nA\n\n\n\n\n \n\n，则 \nL=\\{x\\in A^* | T 接收 x\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n\n接\n\n\n收\n\n \n \n\n 称为 \nT\n\n\n\n\n \n\n 所接收的语言。\n\n\n定义4：设字符表 \nA\n\n\n\n\n \n\n , \nL \\subseteq A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，如果存在接收语言 \nL\n\n\n\n\n \n\n 的非确定型Turing机 \nT\n\n\n\n\n \n\n 和多项式 \np(n)\n\n\n\n\n\n\n\n \n \n \n \n\n，使得对于每一个 \nx\\in L\n\n\n\n\n\n\n \n \n \n\n，存在 \nT\n\n\n\n\n \n\n 关于 \nx\n\n\n\n\n \n\n 的接收路径 \n\\gamma_1, \\cdots, \\gamma_m\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n，其中 \nm \\le p(|x|)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n，则称语言 \nL\n\n\n\n\n \n\n 属于 \nNP\n\n\n\n\n\n \n \n\n 类。\n\n即存在一个非确定性Turing机 \nT\n\n\n\n\n \n\n 能够在多项式时间内解决 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n 问题，则 \nL \\in NP\n\n\n\n\n\n\n\n \n \n \n \n\n。\n\nCook-Karp论题的不确定计算型式：任给字母表 \nA\n\n\n\n\n \n\n, 语言 \nL \\subseteq A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，若存在着某种不确定计算的装置能够在多项式时间（只要求可能的最短时间）内解决问题 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n，则必有 \nL \\in NP\n\n\n\n\n\n\n\n \n \n \n \n\n，即必有一台不确定的Turing机 \nT\n\n\n\n\n \n\n 能够在多项式时间内解决问题 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n。\n定理3：\nP \\subseteq NP\n\n\n\n\n\n\n \n \n \n \n\n。\n\n若 \nL \\in P\n\n\n\n\n\n\n \n \n \n\n， 则存在一个确定型Turing机能够在多项式时间内解决 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n 问题，而确定型Turing机也是非确定型Turing机，即存在一个非确定型Turing机能够在多项式时间内解决 \nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n 问题，得到 \nL \\in NP\n\n\n\n\n\n\n\n \n \n \n \n\n。\n\\forall L \\in P:L \\in NP\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n，因此 \nP \\subseteq NP\n\n\n\n\n\n\n \n \n \n \n\n。\n\n定理4：若 \nL \\in NP\n\n\n\n\n\n\n\n \n \n \n \n\n，则 \nL\n\n\n\n\n \n\n 为可计算集。\n\n由 \nL \\in NP\n\n\n\n\n\n\n\n \n \n \n \n\n，对于 \nx \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，\nx \\in ? L\n\n\n\n\n\n\n\n \n \n \n \n\n 的问题可以由一个非确定型Turing机解决，因此 \nL\n\n\n\n\n \n\n 是可计算集。\n\n合取范式与SAT形如 \n(P_1 \\lor P_2 \\lor \\bar P_3 \\lor P_4) \\land (\\bar P_1 \\lor \\bar P_2 \\lor \\bar P_3 \\lor P_4) \\land (\\bar P_2 \\lor \\bar P_1 \\lor \\bar P_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n 的式子称为合取范式，其中 \nP_i\n\n\n\n\n\n \n \n\n 称为命题变元。若存在一组 \nP_i\n\n\n\n\n\n \n \n\n ，使得该合取范式为真，则称该合取范式为可满足的。\n我们首先介绍一种SAT问题的直接表示方法：\n令字符表 \nA= \\{(,),\\cdot,\\triangle, \\lor, \\land \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n，令 \nA^*\n\n\n\n\n\n \n \n\n 中的字 \n(\\cdot \\lor \\cdot \\cdot \\lor \\triangle \\triangle \\triangle \\lor \\cdot \\cdot \\cdot \\cdot) \\land (\\triangle \\triangle \\lor \\triangle \\lor \\triangle \\triangle \\triangle \\lor \\cdot \\cdot \\cdot)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 表示合取范式 \n(P_1 \\lor P_2 \\lor \\bar P_3 \\lor P_4) \\land (\\bar P_1 \\lor \\bar P_2 \\lor \\bar P_3 \\lor P_4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n。\n可将 \nA^*\n\n\n\n\n\n \n \n\n 中的字分为三类：不符合合取范式的字；代表不可满足的合取范式的字；代表可满足的合取范式的字。\nSAT(可满足性)问题的一种表示方法为：\n\nSAT = \\{ x \\in A^* | x 代表可满足的合取范式 \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n\n代\n\n\n表\n\n\n可\n\n\n满\n\n\n足\n\n\n的\n\n\n合\n\n\n取\n\n\n范\n\n\n式\n\n \n\n\nSAT问题即是问 \nx \\in ? SAT\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n。\n上述表示方法能够表示所有的SAT问题，以下考虑命题变元数量 \n\\le n\n\n\n\n\n\n \n \n\n 条件下SAT问题的另一种表示方法：\n令字符表 \nA_n =\\{ a_1, \\cdots, a_n, \\bar a_1, \\cdots, \\bar a_n, / \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n，字符表 \nA_n\n\n\n\n\n\n \n \n\n 上任一以 \n/\n\n\n\n\n \n\n 开头的行都代表一个合取范式，如 \n/a_1 a_2 \\bar a_3 a_4 /\\bar a_1 \\bar a_2 \\bar a_3 a_4 /\\bar a_2 \\bar a_1 \\bar a_3\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n 表示 \n(P_1 \\lor P_2 \\lor \\bar P_3 \\lor P_4) \\land (\\bar P_1 \\lor \\bar P_2 \\lor \\bar P_3 \\lor P_4) \\land (\\bar P_2 \\lor \\bar P_1 \\lor \\bar P_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n。\n命题变元数量 \n\\le n\n\n\n\n\n\n \n \n\n 条件下的SAT问题表示为：\n\nSAT_n = \\{ x \\in A_n^* |x 代表可满足的合取范式\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n \n\n \n \n\n代\n\n\n表\n\n\n可\n\n\n满\n\n\n足\n\n\n的\n\n\n合\n\n\n取\n\n\n范\n\n\n式\n\n \n\n\n由此完整的 \nSAT\n\n\n\n\n\n\n \n \n \n\n 可表示为\n\nSAT = \\lim_{n \\to \\infty} SAT_n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n\n \n \n\n\n\n定理5：\n\\forall n \\in \\mathbb{N}^+:SAT_n \\in NP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n。\n\n构造不确定型Turing机 \nT\n\n\n\n\n \n\n，使得若 \nx \\in SAT_n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n 则 \nT\n\n\n\n\n \n\n 在多项式时间内接收 \nx\n\n\n\n\n \n\n；若 \nx \\notin SAT_n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n 则 \nT\n\n\n\n\n \n\n 不接受 \nx\n\n\n\n\n \n\n。\n\n\nT\n\n\n\n\n \n\n 的字符表为 \n\\widetilde A_n=\\{ a_1, \\cdots, a_n, \\bar a_1, \\cdots, \\bar a_n, /, | \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n。\n\n\nT\n\n\n\n\n \n\n 的状态表为 \n\\{ q_1, m_j,r_i^+,r_i^-,v_i^+,v_i^-,w_i^+,w_i^-,p,\\bar p \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n。\n\n\nT\n\n\n\n\n \n\n 的行为准则集为：\n\n\\begin{array}[]{}\n  \n   \\left( \\begin{array}[]{l}\n  \\rule[-0.5em]{0em}{2em} q_1 &amp; B &amp; R &amp; q_1 \\\\\n  \\rule[-0.5em]{0em}{2em} q_1 &amp; / &amp; L &amp; m_1 \\\\\n  \\rule[-0.5em]{0em}{2em} q_1 &amp; S &amp; L &amp; q_1 \\\\\n  \\rule[-0.5em]{0em}{2em} m_i &amp; B &amp; R &amp; r_i^+ \\\\\n  \\rule[-0.5em]{0em}{2em} m_i &amp; B &amp; R &amp; r_i^- \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^\\pm &amp; | &amp; R&amp; v_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^\\pm &amp; / &amp; R&amp; v_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^+ &amp; S &amp; R &amp; r_i^+ \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^- &amp; S &amp; R &amp; r_i^- \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^+ &amp; a_i &amp; L &amp; w_i^+ \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^- &amp; \\bar a_i &amp; L &amp; w_i^- \\\\\n  \\rule[-0.5em]{0em}{2em} w_i^\\pm &amp; S &amp; L &amp; w_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} w_i^\\pm &amp; / &amp; | &amp; v_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} v_i^\\pm &amp; S &amp; R &amp; v_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} v_i^\\pm &amp; / &amp; R &amp; r_i^\\pm \\\\\n  \\rule[-0.5em]{0em}{2em} r_i^\\pm &amp; B &amp; L &amp; m_{i+1} \\\\\n  \\rule[-0.5em]{0em}{2em} v_i^\\pm &amp; B &amp; L &amp; m_{i+1} \\\\\n  \\rule[-0.5em]{0em}{2em} m_{i+1} &amp; S &amp; L &amp; m_{i+1} \\\\\n  \\rule[-0.5em]{0em}{2em} m_{n+1} &amp; B &amp; R &amp; p \\\\\n  \\rule[-0.5em]{0em}{2em} p &amp; S &amp; R &amp; p \\\\\n  \\rule[-0.5em]{0em}{2em} p &amp; / &amp; L &amp; \\bar p \\\\\n  \\rule[-0.5em]{0em}{2em} \\bar p &amp; S &amp; R &amp; p\n  \\end{array} \\right)\n  \n  \\begin{array}[]{l}\n  \n  \\left.\\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n \\end{array} \\right\\} 作查头之用 \\\\\n  \\left. \\begin{array}[]{l}  \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} \\end{array} \\right\\} 为 a_i选取真假值,i=1,\\cdots,n \\\\\n  \\begin{array}[]{l}\n  \\rule[-0.5em]{0em}{2em} 当前子句已满足，观察下一子句\\\\  \n  \\rule[-0.5em]{0em}{2em} 当前子句尚未满足，寻求满意文字\\\\\n  \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots, a_{i-1}, a_{i+1}, \\cdots, a_n,\\bar a_1, \\cdots, \\bar a_n；不满意则右移\\\\\n  \\rule[-0.5em]{0em}{2em} S 遍取 a_1, \\cdots, a_n,\\bar a_1, \\cdots, \\bar a_{i-1}, \\bar a_{i+1}, \\cdots, \\bar a_n；不满意则右移\n  \\end{array}\\\\ \n  \\left. \\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} \\end{array} \\right\\} 发现满意文字后，装新内态 w_i^\\pm\\\\\n  \\left.\\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n \\\\ \\rule[-0.5em]{0em}{2em} \\end{array} \\right\\} 标志被满足了的子句 \\\\\n  \\left.\\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n, | \\\\ \\rule[-0.5em]{0em}{2em} \\end{array} \\right\\} 寻找下一个未被满足的子句 \\\\\n  \\left. \\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} \\end{array} \\right\\} 准备为下一个命题变元选真假值\\\\\n  \\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n, /, |, 找输入字 x 的头 \\end{array}\\\\\n  \\left.\\begin{array}[]{l} \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n, | \\\\ \\rule[-0.5em]{0em}{2em} \\\\ \\rule[-0.5em]{0em}{2em} S 遍取 a_1,\\cdots,a_n,\\bar a_1, \\cdots, \\bar a_n, /, |, B  \\end{array} \\right\\} 最后检查是否还残存/符号，即是否满足了 \\\\\n  \\end{array}\n  \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n \n \n\n\n\n\n\n \n \n\n\n\n\n\n \n \n\n\n\n\n\n \n \n\n\n\n\n\n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n \n \n\n\n\n\n\n \n\n \n \n \n\n\n\n\n\n\n \n\n \n \n \n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n \n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n \n\n \n \n \n\n\n\n \n\n \n \n \n\n\n\n \n\n \n \n \n\n\n \n \n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n作\n\n\n查\n\n\n头\n\n\n之\n\n\n用\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n为\n\n\n \n \n\n\n选\n\n\n取\n\n\n真\n\n\n假\n\n\n值\n\n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n当\n\n前\n\n\n子\n\n\n句\n\n\n已\n\n\n满\n\n\n足\n\n\n，\n\n\n观\n\n\n察\n\n\n下\n\n\n一\n\n\n子\n\n\n句\n\n\n\n\n\n\n当\n\n前\n\n\n子\n\n\n句\n\n\n尚\n\n\n未\n\n\n满\n\n\n足\n\n\n，\n\n\n寻\n\n\n求\n\n\n满\n\n\n意\n\n\n文\n\n\n字\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n；\n\n\n不\n\n\n满\n\n\n意\n\n\n则\n\n\n右\n\n\n移\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n \n \n \n\n\n \n \n \n\n \n \n \n\n\n；\n\n\n不\n\n\n满\n\n\n意\n\n\n则\n\n\n右\n\n\n移\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n发\n\n\n现\n\n\n满\n\n\n意\n\n\n文\n\n\n字\n\n\n后\n\n\n，\n\n\n装\n\n\n新\n\n\n内\n\n\n态\n\n\n \n \n \n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n标\n\n\n志\n\n\n被\n\n\n满\n\n\n足\n\n\n了\n\n\n的\n\n\n子\n\n\n句\n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n寻\n\n\n找\n\n\n下\n\n\n一\n\n\n个\n\n\n未\n\n\n被\n\n\n满\n\n\n足\n\n\n的\n\n\n子\n\n\n句\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n准\n\n\n备\n\n\n为\n\n\n下\n\n\n一\n\n\n个\n\n\n命\n\n\n题\n\n\n变\n\n\n元\n\n\n选\n\n\n真\n\n\n假\n\n\n值\n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n找\n\n\n输\n\n\n入\n\n\n字\n\n \n\n的\n\n\n头\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n\n\n\n\n\n\n\n\n\n\n \n\n遍\n\n\n取\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n最\n\n\n后\n\n\n检\n\n\n查\n\n\n是\n\n\n否\n\n\n还\n\n\n残\n\n\n存\n\n \n\n符\n\n\n号\n\n\n，\n\n\n即\n\n\n是\n\n\n否\n\n\n满\n\n\n足\n\n\n了\n\n\n\n\n\n\n\n\n\n\n\n待续\n\nCook 定理\n待续\n\n另外几个NP完全问题\n待续\n\n参考文献[1] 黄文奇, 许如初. 近世计算理论导引: NP 难度问题的背景, 前景及其求解算法研究[M]. 科学出版社, 2004: 20-46.\n","plink":"https://yuxinzhao.net/np-complete-theory/"},{"title":"不可计算性(不可解性)","date":"2022-01-01T14:20:41.000Z","updated":"2022-01-04T08:35:48.655Z","content":"本文是《近世计算理论导引：NP难度问题的背景、前景及其求解方法研究》的第二章《不可计算性》的笔记。\n\n\n胜弈机的不存在性胜弈机和“上帝是万能的”问题类似，即存在一个计算机系统能够在国际象棋中击败任何对手。\n如果有两个胜弈机互相博弈，结局为一负一胜或者平局，出现至少一个胜弈机与胜弈机定义矛盾的情况，因此胜弈机不存在。\n不可计算函数的存在性自然数集合 \n\\mathbb{N}\n\n\n\n\n \n\n 为可数无穷集合，记有可数个元素 \nN_0\n\n\n\n\n\n \n \n\n，即 \n\\mathbb{N} = \\{0, N_0 - 1\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n\n, \nN_0 \\to +\\infty\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n。\n此处讨论的函数为映射 \nf: \\mathbb{N} \\mapsto \\mathbb{N}\n\n\n\n\n\n\n\n \n \n \n \n \n\n ，是一个全函数。\n定理1：不同程序的总个数不超过 \n\\mathbb{N}\n\n\n\n\n \n\n 的元素个数 \nN_o\n\n\n\n\n\n \n \n\n。\n\n程序是一个有穷字符串，将Turing机的字符表 \nA=\\{a_1,\\cdots,a_v\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 的 \nA^*\n\n\n\n\n\n \n \n\n 中的元素按照以下顺序排列：\n\n\\phi;a_1,\\cdots,a_v;a_1a_1,\\cdots,a_va_v;\\cdots\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n\n \n \n\n \n \n\n\n即可将 \nA^*\n\n\n\n\n\n \n \n\n 中的有穷字符串以自然数进行编号，建立起从程序到 \n\\mathbb{N}\n\n\n\n\n \n\n的一一映射，即穷字符串共有 \nN_0\n\n\n\n\n\n \n \n\n 个。其中只有部分字符串为合法的程序，因此程序的总个数 \n\\le N_0\n\n\n\n\n\n\n \n\n \n \n\n\n。\n\n\n定理2：数论函数有 \nN_0^{N_0}\n\n\n\n\n\n \n\n \n \n\n \n\n 个。\n\n对于 \nx \\in \\mathbb{N}\n\n\n\n\n\n\n \n \n \n\n，\nf(x)\n\n\n\n\n\n\n\n \n \n \n \n\n 有 \nN_0\n\n\n\n\n\n \n \n\n 个可能输出，而 \nx\n\n\n\n\n \n\n 共有 \nN_0\n\n\n\n\n\n \n \n\n 个，因此 \nf\n\n\n\n\n \n\n 总共有 \nN_0^{N_0}\n\n\n\n\n\n \n\n \n \n\n \n\n 个可能性。\n\n定理3：可计算函数总共恰好有 \nN_0\n\n\n\n\n\n \n \n\n 个互不相同。\n\n由定理1可知不同程序的个数最多为 \nN_0\n\n\n\n\n\n \n \n\n 个，而每个程序至多计算一个函数，因此有程序计算的函数至多有 \nN_0\n\n\n\n\n\n \n \n\n 个；假定 \nf\n\n\n\n\n \n\n 为可计算函数，则 \nc f\n\n\n\n\n\n \n \n\n, \nc \\in \\mathbb{N}\n\n\n\n\n\n\n \n \n \n\n 这 \nN_0\n\n\n\n\n\n \n \n\n 个函数也为可计算函数；综上可计算函数恰好有 \nN_0\n\n\n\n\n\n \n \n\n 个互不相同。\n\n定理4：\n2^{N_0} &gt; N_0\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n\n。\n定理5：存在着不可计算的函数。\n\n\nN_0^{N_0} \\ge 2^{N_0} \\gt N_0\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n 说明数论函数的数量 \nN_0^{N_0}\n\n\n\n\n\n \n\n \n \n\n \n\n 大于可计算的函数的数量 \nN_0\n\n\n\n\n\n \n \n\n，因此肯定有某些数论函数无法用程序计算。 \n\n停机问题的不可解性设 \nT_1, T_2, \\cdots\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n 为全体不同的合法程序。\n对任一程序 \nT_i\n\n\n\n\n\n \n \n\n，当给出输入之后启动，它有停机和不停机两种结局。停机后则会给出输出。\n对于程序的输入和输出我们都可以用自然数进行编码，同时记停机为 \n\\downarrow\n\n\n\n\n \n\n，不停机为 \n\\uparrow\n\n\n\n\n \n\n。\n任一程序 \nT_i\n\n\n\n\n\n \n \n\n 都相当于一个函数 \n\\varphi_i\n\n\n\n\n\n \n \n\n ，其定义域为 \n\\{0,1,2,\\cdots\\}\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n，值域为 \n\\{\\uparrow, 0, 1,2, \\cdots\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n。\n定义1：递归可枚举集(r.e. 集)\n\n说 \n\\mathbb{N}\n\n\n\n\n \n\n 的某个子集 \nA\n\n\n\n\n \n\n 为递归可枚举集是指它可以表示为某个 \n\\varphi_i\n\n\n\n\n\n \n \n\n 的定义域，即 \n(\\exists i)[A=\\{x|\\varphi_i (x) \\downarrow\\}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n 。\n\n定义2：可计算集\n\n说 \n\\mathbb{N}\n\n\n\n\n \n\n 的某个子集 \nA\n\n\n\n\n \n\n 为可计算集是指存在一个函数，对于任意给定的自然数 \nx\n\n\n\n\n \n\n， \nx \\in ? A\n\n\n\n\n\n\n\n \n \n \n \n\n 的问题是可以由该程序确切判断的。\n\n引理1：可计算集为 r.e. 集。\n\n设 \nA\n\n\n\n\n \n\n 为可计算集，定义可计算函数 \nf\n\n\n\n\n \n\n:\n\nf(x) = \\begin{cases} 1, &amp;x \\in A \\\\ \\uparrow, &amp;x \\notin A \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n于是 \nA=\\{x| f(x) \\downarrow \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n 。\n\n\n引理2：可计算集的余集也是可计算集。\n\n设 \nA\n\n\n\n\n \n\n 为可计算集，则可判断 \nx \\in ? A\n\n\n\n\n\n\n\n \n \n \n \n\n。若 \nx \\in A\n\n\n\n\n\n\n \n \n \n\n  则 \nx \\notin \\bar A\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n， 若 \nx \\notin A\n\n\n\n\n\n\n \n \n \n\n 则 \nx \\in \\bar A\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n。因此可判断 \nx \\in ? \\bar A\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，即 \n\\bar A\n\n\n\n\n\n \n \n\n 是可计算集。\n\n引理3：存在集 \n\\Theta\n\n\n\n\n \n\n 不是 r.e. 集。\n\n记 \nw_i = \\{x|\\varphi_i (x) \\downarrow\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n，\nw_i\n\n\n\n\n\n \n \n\n 是 r.e. 集。\n构造 \n\\Theta = \\{ i | i \\notin w_i \\} = \\{ i | \\varphi_i (i) \\uparrow \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n ，由于 \n\\forall i:\\Theta \\neq w_i\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n，\n\\Theta\n\n\n\n\n \n\n 不是 r.e. 集。\n由引理1知 \n\\Theta\n\n\n\n\n \n\n 不是可计算集。\n\n\\Theta\n\n\n\n\n \n\n 的余集记作 \nK\n\n\n\n\n \n\n，即 \nK=\\bar \\Theta=\\{i|\\varphi_i(i) \\downarrow \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n。\n\n定理1：\nK\n\n\n\n\n \n\n 不是可计算集。\n\n由引理3知 \n\\Theta\n\n\n\n\n \n\n 不是可计算集。\nK = \\bar \\Theta\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，于是由引理2可知 \nK\n\n\n\n\n \n\n 不是可计算集。\n\nTuring机停机问题之Turing机不可解性定理1：任给自然数 \nx\n\n\n\n\n \n\n，问 \n\\varphi_x(x)\n\n\n\n\n\n\n\n \n \n \n \n \n\n 是否收敛，这个问题是不可能由某个Turing机统一解决的。\n\n设该问题可以由某一个Turing机 \nT_{\\theta}\n\n\n\n\n\n \n \n\n 解决，即\n\n\\varphi_\\theta(x) = \\begin{cases} 1, &amp;\\varphi_x(x) \\downarrow \\\\ \\uparrow, &amp;\\varphi_x(x) \\uparrow \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n\n\n\n\n\n若该问题可解，则“任给自然数 \nx\n\n\n\n\n \n\n，问 \n\\varphi_x(x)\n\n\n\n\n\n\n\n \n \n \n \n \n\n 是否不收敛”这个问题同样可解，即存在一个Turing机 \nT_v\n\n\n\n\n\n \n \n\n 使得\n\n\\varphi_v(x) = \\begin{cases} \\uparrow, &amp;\\varphi_x(x) \\downarrow \\\\ 1, &amp;\\varphi_x(x) \\uparrow \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n\n\n\n\n\n因此 \n\\varphi_v(v) \\uparrow \\Leftrightarrow \\varphi_v(v) \\downarrow\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n \n\n，矛盾。\n\n\n参考文献[1] 黄文奇, 许如初. 近世计算理论导引: NP 难度问题的背景, 前景及其求解算法研究[M]. 科学出版社, 2004: 11-17.\n","plink":"https://yuxinzhao.net/unsolvability/"},{"title":"计算的数学模型——Turing机","date":"2021-12-22T13:00:00.000Z","updated":"2022-01-04T08:35:48.647Z","content":"本文是《近世计算理论导引：NP难度问题的背景、前景及其求解方法研究》的第一章《计算的数学模型——Turing机》的笔记。\n\n\nTuring机的定义Turing机由有穷个字符构成的字符表 \n\\{s_1, \\cdots, s_n \\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n、有穷个内部状态构成的内部状态表 \n\\{q_1,\\cdots,q_v\\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 和有穷条规则构成的行为准则集组成。\nTuring机的运行方式是对一个左右无穷的条带进行处理。该条带上有穷个方格被填入字符表 \n\\{s_1, \\cdots, s_n \\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 中的某一个字符，其余方格全部填入空白字符 \ns_0\n\n\n\n\n\n \n \n\n。内部状态 \nq_i\n\n\n\n\n\n \n \n\n 指向着条带上的某一个方格。\n初始时刻，将内部状态 \nq_1\n\n\n\n\n\n \n \n\n 指向条带最左非 \ns_0\n\n\n\n\n\n \n \n\n 方格的左边一个方格。\n\n\\begin{matrix}\n\\cdots &amp; s_0 &amp; s_\\lambda &amp; \\cdots &amp; s_\\mu &amp; s_0 &amp; \\cdots \\\\\n&amp; \\uparrow \\\\\n&amp; q_1\n\\end{matrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n \n\n\n \n \n\n\n\n\n \n \n\n\n \n\n\n\nTuring机后续的行为由有穷条规则构成的集合（行为准则集）指挥。其行为规则由以下四种类型，其外形为四元组。\n\n\\begin{matrix}\n\\begin{cases}\nq_i s_j s_k q_l \\\\\nq_i s_j L q_i \\\\\nq_i s_j R q_l\n\\end{cases} \\\\\n0 \\le j,k \\le n \\\\\n1 \\le i,l \\le v\n\\end{matrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n\n\n\n\n\n第一条规则 \nq_i s_j s_k q_l\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n \n \n\n\n 指出：当Turing机当前的内部状态为 \nq_i\n\n\n\n\n\n \n \n\n，且指向的方格的字符为 \ns_j\n\n\n\n\n\n \n \n\n 时，指针不移动，但将该方格的字符改为 \ns_k\n\n\n\n\n\n \n \n\n，并将内部状态改为 \nq_l\n\n\n\n\n\n \n \n\n。\n\n\n\\begin{matrix}\n\\begin{matrix}\n\\cdots &amp; s_j &amp; \\cdots \\\\\n&amp; \\uparrow \\\\\n&amp; q_i\n\\end{matrix}\n&amp;\\to&amp;\n\\begin{matrix}\n\\cdots &amp; s_k &amp; \\cdots \\\\\n&amp; \\uparrow \\\\\n&amp; q_l\n\\end{matrix}\n\\end{matrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n \n\n \n \n\n\n \n\n\n \n\n\n\n \n\n\n\n \n \n\n \n\n \n \n\n\n \n\n\n\n\n\n\n第二条规则 \nq_i s_j L q_i\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n 指出：当Turing机当前的内部状态为 \nq_i\n\n\n\n\n\n \n \n\n，且指向的方格的字符为 \ns_j\n\n\n\n\n\n \n \n\n 时，指针向左移动一个方格，并将内部状态改为 \nq_l\n\n\n\n\n\n \n \n\n。\n\n\n\\begin{matrix}\n\\begin{matrix}\n\\cdots &amp; s_\\lambda &amp; s_j &amp; s_\\mu &amp; \\cdots \\\\\n&amp;&amp; \\uparrow \\\\\n&amp;&amp; q_i\n\\end{matrix}\n&amp;\\to&amp;\n\\begin{matrix}\n\\cdots &amp; s_\\lambda &amp; s_j &amp; s_\\mu &amp; \\cdots \\\\\n&amp; \\uparrow \\\\\n&amp; q_l\n\\end{matrix}\n\\end{matrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n \n\n\n \n\n\n\n \n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n\n\n\n\n\n\n第三条规则 \nq_i s_j R q_i\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n 指出：当Turing机当前的内部状态为 \nq_i\n\n\n\n\n\n \n \n\n，且指向的方格的字符为 \ns_j\n\n\n\n\n\n \n \n\n 时，指针向右移动一个方格，并将内部状态改为 \nq_l\n\n\n\n\n\n \n \n\n。\n\n\n\\begin{matrix}\n\\begin{matrix}\n\\cdots &amp; s_\\lambda &amp; s_j &amp; s_\\mu &amp; \\cdots \\\\\n&amp;&amp; \\uparrow \\\\\n&amp;&amp; q_i\n\\end{matrix}\n&amp;\\to&amp;\n\\begin{matrix}\n\\cdots &amp; s_\\lambda &amp; s_j &amp; s_\\mu &amp; \\cdots \\\\\n&amp;&amp;&amp; \\uparrow \\\\\n&amp;&amp;&amp; q_l\n\\end{matrix}\n\\end{matrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n \n\n\n \n\n\n\n \n\n\n\n \n \n\n\n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n \n\n\n\n\n\n\n当没有任何一条规则匹配时，计算终止，称为停机。\n\nTuring机的一种写法是：\n\n\\begin{pmatrix}\n\\{ s_1, s_2\\}, &amp; \\{ q_1, q_2,q_3\\}, &amp;\n\\begin{Bmatrix}\nq_2 &amp; s_0 &amp; s_1 &amp; q_3 \\\\\nq_3 &amp; s_2 &amp; L &amp; q_2 \\\\\nq_1 &amp; s_1 &amp; s_0 &amp; q_2\n\\end{Bmatrix}\n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n在任一给定时刻，Turing机带上的每一个方格内是什么字符，指针指向哪个方格，这些信息称为格局/带格局。\nTuring机所计算的函数设有一个字符表 \nA= \\{ s_1, \\cdots, s_n \\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n，函数 \nf\n\n\n\n\n \n\n 是从 \nA^{*m}\n\n\n\n\n\n\n \n\n \n \n\n\n 到 \nA^*\n\n\n\n\n\n \n \n\n 的一个部分函数，函数 \nf\n\n\n\n\n \n\n 在某个子集上有定义，而在其余集上无定义。其中 \nA^* = \\bigcup_{k=1}^\\infty A^k\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n 的元素称为字，\nA^{*m}\n\n\n\n\n\n\n \n\n \n \n\n\n 的元素为 \nm\n\n\n\n\n \n\n 个字。\n设有一个Turing机 \nT\n\n\n\n\n \n\n 的字符表为 \n\\{s_1, \\cdots, s_N\\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n、内部状态表为 \n\\{ q_1, \\dots, q_v \\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n，Turing机 \nT\n\n\n\n\n \n\n 实现了对函数 \nf\n\n\n\n\n \n\n 的计算是指满足以下条件：\n\n若以 \n\\begin{array}[t]{} s_0 &amp; x_1 &amp; s_0 &amp; x_2 &amp; s_0 &amp; \\cdots &amp; s_0 &amp; x_m  \\\\ \\uparrow \\\\ q_1 \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n （其中 \nx_i \\in A^*\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n）为格局开始则\n最终 \nT\n\n\n\n\n \n\n 会停机当且仅当 \nf(x_1, \\cdots, x_m)\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 有定义。\n\nT\n\n\n\n\n \n\n 停机时的构形 \n\\begin{array}[t]{} s_0 &amp; \\times &amp; \\cdots &amp; \\times &amp; \\cdots &amp; \\times &amp; s_0 \\\\ &amp;&amp;&amp; \\uparrow \\\\ &amp;&amp;&amp; q_i \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n\n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n\n\n 中间忽略掉所有的 \ns_0, s_{n+1}, \\cdots, s_N\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n\n\n 后剩下的字符串即是字 \nf(x_1, \\cdots, x_m)\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n。\n\n\n\n满足上述条件的情况下, \nf\n\n\n\n\n \n\n 被称为Turing机 \nT\n\n\n\n\n \n\n 所计算的 \nm\n\n\n\n\n \n\n 元函数，或称 Turing 机 \nT\n\n\n\n\n \n\n 计算了 \nm\n\n\n\n\n \n\n 元函数 \nf\n\n\n\n\n \n\n。\n如果 \nT\n\n\n\n\n \n\n 计算了 \nf\n\n\n\n\n \n\n，且满足以下两个附加条件，则称 \nT\n\n\n\n\n \n\n 严格计算了 \nf\n\n\n\n\n \n\n：\n\n\nN=n\n\n\n\n\n\n\n \n \n \n\n，即 \nT\n\n\n\n\n \n\n 没有使用 \n{s_0} \\bigcup A\n\n\n\n\n\n\n\n \n \n \n \n\n 以外的字符；\n\nT\n\n\n\n\n \n\n 停机时，带上的构形如 \n\\begin{array}[t]{} \\cdots &amp; s_0 &amp; s_0 &amp; y &amp; s_0 &amp; s_0 &amp; \\cdots \\\\ &amp;&amp;&amp; \\uparrow \\\\ &amp;&amp;&amp; q_i \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n\n\n\n \n \n\n\n \n \n\n \n\n\n ，其中字符串 \ny\n\n\n\n\n \n\n 不包含空白 \ns_0\n\n\n\n\n\n \n \n\n, 即 \ny \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n。\n\n目前已经证明：任一给定的Turing机计算的函数，一定能被某一个 Turing 机严格计算。\nTuring机所接受的语言设Turing机 \nT\n\n\n\n\n \n\n 的内部状态表为 \n\\{ q_1, \\dots, q_v \\}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n， 字符表为 \nA = \\{s_1, \\cdots, s_n\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n。\n对于某个字 \nu \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n，如果以格局 \n\\begin{array}[t]{} s_0 &amp; u \\\\ \\uparrow \\\\ q_1 \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n\n\n 开始 \nT\n\n\n\n\n \n\n 会停机，则称 \nT\n\n\n\n\n \n\n 接受字 \nu\n\n\n\n\n \n\n。\n\nT\n\n\n\n\n \n\n 所接受的语言 \nL\n\n\n\n\n \n\n 是 \nT\n\n\n\n\n \n\n 所接受的 \nA^*\n\n\n\n\n\n \n \n\n 中的字的全体所构成的集合，\nL \\in A^*\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n。\n计算复杂度设函数 \nf\n\n\n\n\n \n\n 为 Turing机 \nT\n\n\n\n\n \n\n 所计算的 \nm\n\n\n\n\n \n\n 元函数，设 \nf(x_1, \\cdots, x_m)\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 有定义，于是当以格局 \n\\begin{array}[t]{} s_0 &amp; x_1 &amp; s_0 &amp; x_2 &amp; s_0 &amp; \\cdots &amp; s_0 &amp; x_m  \\\\ \\uparrow \\\\ q_1 \\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n 开始时 \nT\n\n\n\n\n \n\n 最终会停机。\n从开机到停机这一过程中所执行的四元组的次数称为计算的步数，也称为 \nT\n\n\n\n\n \n\n 计算函数值 \nf(x_1, \\cdots, x_m)\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 的时间复杂度。\n计算过程中带上最左一个曾经不为 \ns_0\n\n\n\n\n\n \n \n\n 的格子到最右一个不曾为 \ns_0\n\n\n\n\n\n \n \n\n 的格子之间是 \nf\n\n\n\n\n \n\n 的 “工作部分”，之间所包含的格子数称为 \nT\n\n\n\n\n \n\n 计算函数值 \nf(x_1, \\cdots, x_m)\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 的空间复杂度。\n时间复杂度和空间复杂度统称时空开销。\nChurch-Turing 论题设任意给定一个字符表 \nA = \\{ s_1, \\cdots, s_n\\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n，\nn \\ge 1\n\n\n\n\n\n\n \n \n \n\n，\nf\n\n\n\n\n \n\n 是从 \nA^{*m}\n\n\n\n\n\n\n \n\n \n \n\n\n 到 \nA^*\n\n\n\n\n\n \n \n\n 上的一个任给的全函数，\nm \\ge 1\n\n\n\n\n\n\n \n \n \n\n。\n论题给出如下断言：\n\n只要世上有一台机器能够实现对函数 \nf\n\n\n\n\n \n\n 的计算，则一定存在一台 Turing 机能实现对 \nf\n\n\n\n\n \n\n 的计算。\n\n该论题目前已被证明永远无法证实，但尚未保证永远不能被证伪。\nTuring机的编码给定一台Turing机：\n\n\\begin{pmatrix}\n\\{ s_1, s_2\\}, &amp; \\{ q_1, q_2,q_3\\}, &amp;\n\\begin{Bmatrix}\nq_2 &amp; s_0 &amp; s_1 &amp; q_3 \\\\\nq_3 &amp; s_2 &amp; L &amp; q_2 \\\\\nq_1 &amp; s_1 &amp; s_0 &amp; q_2\n\\end{Bmatrix}\n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n规定将自然数使用二进制书写，如 \ns_2\n\n\n\n\n\n \n \n\n 写成 \ns_{10}\n\n\n\n\n\n\n \n\n \n \n\n\n，然后将Turing机的所有部分自然拉长写成 \n\\{ s, q, 0, 1, R, L \\}^*\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n\n 上的一个字，如：\n\ns1s10q1q10q11q10s0s1q11q11s10Lq10q1s1s0q10\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n\n\n\n对 \n\\{s,q,0,1,R,L\\}*\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 上的所有字使用字典序进行编号，可以将每一个Turing机 \nT\n\n\n\n\n \n\n 对应上一个自然数，称为该Turing机的字码 \nM(T)\n\n\n\n\n\n\n\n \n \n \n \n\n。\n性质：\n\n给定一台 Turing机 \nT\n\n\n\n\n \n\n 可以算出字码 \nM(T) \\in \\mathbb{N}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n。\n若 \nT_1 \\neq T_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n， 则 \nM(T_1) \\neq M(T_2)\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n。\n\n\\forall v \\in \\mathbb{N}\n\n\n\n\n\n\n\n \n \n \n \n\n： 是否存在 \nT\n\n\n\n\n \n\n 使得 \nM(T) = v\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是可判定的。\n\n可以进一步改进字码为：当Turing机集合 \nU=\\{T_1, \\cdots, T_u \\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n 中所有Turing机实际上为相同的Turing机，只是规则的编码顺序不同而导致各Turing机计算出来的字码不同时，使用 \n\\min\\{ M(T) |T \\in U\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n 作为 \nU\n\n\n\n\n \n\n 中所有Turing机共同的字码。据此可以得到Turing机集合到自然数集合的一对一映射。\n参考文献[1] 黄文奇, 许如初. 近世计算理论导引: NP 难度问题的背景, 前景及其求解算法研究[M]. 科学出版社, 2004: 1-10.\n","plink":"https://yuxinzhao.net/turing-machine/"},{"title":"粒子群优化算法(Particle Swarm Optimization, PSO)","date":"2021-12-21T20:00:00.000Z","updated":"2022-01-04T08:35:48.611Z","content":"本文介绍粒子群算法及其原理和衍生算法。\n\n\n基本粒子群优化算法假设问题定义在一个 \nd\n\n\n\n\n \n\n 维连续域上。\n我们选择一个包含 \nN\n\n\n\n\n \n\n 个候选解的种群，记为 \n\\boldsymbol{x}_i \\in \\mathbb{R}^d\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n, \ni \\in [1,N]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n。我们称这个种群为粒子群，每一个 \n\\boldsymbol{x}_i\n\n\n\n\n\n \n \n\n 是一个粒子。\n假设每个粒子 \n\\boldsymbol{x}_i\n\n\n\n\n\n \n \n\n 以速度 \n\\boldsymbol{v}_i\n\n\n\n\n\n \n \n\n 在搜索空间中移动，其中 \n\\boldsymbol{v}_i \\in \\mathbb{R}^d\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n。\n算法伪代码\n其中 \n\\boldsymbol{b}_i\n\n\n\n\n\n \n \n\n 表示第 \ni\n\n\n\n\n \n\n 个粒子历史上的最好位置，\n\\boldsymbol{h}_i\n\n\n\n\n\n \n \n\n 表示第 \ni\n\n\n\n\n \n\n 个粒子的 \n\\sigma\n\n\n\n\n \n\n 个邻居中当代的最好位置。\n每一次更新速度时，粒子 \ni\n\n\n\n\n \n\n 都以一定概率向 \n\\boldsymbol{b}_i\n\n\n\n\n\n \n \n\n 和 \n\\boldsymbol{h}_i\n\n\n\n\n\n \n \n\n 两个最优位置偏移。\n由于更新公式 \n\\boldsymbol{x}_i \\leftarrow \\boldsymbol{x}_i + \\boldsymbol{v}_i\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n 可能导致 \n\\boldsymbol{x}_i\n\n\n\n\n\n \n \n\n 超出搜索空间，因此可以增加一些限制将移动出搜索空间的粒子拉回搜索空间内，如\n\n\\begin{align}\n\\boldsymbol{x}_i &amp;\\leftarrow \\min \\{ \\boldsymbol{x}_i, \\boldsymbol{x}_{max} \\} \\\\ \\boldsymbol{x}_i &amp;\\leftarrow \\max \\{ \\boldsymbol{x}_i, \\boldsymbol{x}_{min} \\}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n\n \n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n\n \n\n\n\n\n\n此处 \n[\\boldsymbol{x}_{min}, \\boldsymbol{x}_{max}\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n\n\n\n] 定义了搜索空间的界限。\n算法涉及的超参数\n学习率 \n\\phi_{1}\n\n\n\n\n\n \n \n\n 被称为认知学习率，\n\\phi_{2}\n\n\n\n\n\n \n \n\n 被称为社会学习率，它们是分布在 \n[0, \\phi_{1,max}]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n 和 \n[0, \\phi_{2,max}]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n 上的随机向量。一个经验法则是 \n\\phi_{1,max}\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n 和 \n\\phi_{2,max}\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n 通常设置在 2.05 左右。\n\nv_{max}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 是粒子允许的最大速度，当粒子速度超过限制的最大速度时，将速度归一化到最大速度。通常设置在搜索空间范围的10%到20%之间。\n\n粒子群的拓扑算法中涉及到了粒子的邻居，因此需要考虑粒子的邻域是怎么规定的，这里引入群的拓扑(topology)的概念。\n拓扑的多种选择，此处给出几种：\n\n动态拓扑（dynamic topology）, 又称 lbest 拓扑\n每个粒子的邻域在每一代都会变化，比如最近邻。\n\n\n静态拓扑 （static topology)\n在算法运行前粒子的邻域就已经确定，不随迭代变化。\n全拓扑 (all topology)，又称 gbest拓扑：邻域包含所有粒子。\n环形拓扑 (wheel topology)：每个粒子与另外两个粒子连接构成邻域。\n\n\n\n\nPython实现此处给出一个邻域为全拓扑的粒子群优化算法的实现。\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import numpy as npimport osimport matplotlib.pyplot as pltclass PSO:    def __init__(self, num_generation, population_size, num_variable, lower_bound, upper_bound):        self.num_generation = num_generation        self.population_size = population_size        self.num_variable = num_variable        self.lower_bound = lower_bound        self.upper_bound = upper_bound        self.population_position = np.random.rand(self.population_size, self.num_variable) * (self.upper_bound -self.lower_bound).reshape(1, -1) + self.lower_bound.reshape(1, -1)        self.population_velocity = np.random.randn(self.population_size, self.num_variable)        self.history_best_position = np.zeros((self.population_size, self.num_variable))  # history best of each particle        self.history_best_fitness = np.zeros((self.population_size,))        self.global_best_position = np.zeros((self.num_variable,))  # global best of current population        self.global_best_fitness = None        # initialize        for i in range(self.population_size):            self.history_best_position[i] = self.population_position[i].copy()            self.history_best_fitness[i] = self.fitness(self.population_position[i])            if self.global_best_fitness is None or self.global_best_fitness &lt; self.history_best_fitness[i]:                self.global_best_position = self.population_position[i].copy()                self.global_best_fitness = self.history_best_fitness[i]    def update(self):        c1 = 2  # learning factor        c2 = 2        w = 0.4  # self weight factor        max_velocity = 10.0        new_global_best_position = None        new_gloabl_best_fitness = None        for i in range(self.population_size):            self.population_velocity[i] = w * self.population_velocity[i] + c1 * np.random.rand(self.num_variable) * (self.history_best_position[i] - self.population_position[i]) \\                                           + c2 * np.random.rand(self.num_variable) * (self.global_best_position - self.population_position[i])            velocity = np.linalg.norm(self.population_velocity[i])            if velocity &gt; max_velocity:                self.population_velocity[i] *= max_velocity / velocity            self.population_position[i] += self.population_velocity[i]            self.population_position[i] = np.where(self.population_position[i] &lt; self.lower_bound, self.lower_bound, np.where(self.population_position[i] &gt; self.upper_bound, self.upper_bound, self.population_position[i]))            fitness = self.fitness(self.population_position[i])            if fitness &gt; self.history_best_fitness[i]:                self.history_best_position[i] = self.population_position[i]                self.history_best_fitness[i] = fitness            if new_gloabl_best_fitness is None or new_gloabl_best_fitness &lt; fitness:                new_global_best_position = self.population_position[i]                new_gloabl_best_fitness = fitness        self.global_best_position = new_global_best_position.copy()        self.global_best_fitness = new_gloabl_best_fitness    def fitness(self, position):        return -(position[0] - 50) ** 2 - (position[1] - 50) ** 2    def run(self):        best_fitness_of_each_generation = [self.global_best_fitness]        for generation in range(self.num_generation):            print(\"generation &#123;&#125;:\".format(generation), end=' ')            self.update()            best_fitness_of_each_generation.append(self.global_best_fitness)            print(\"best fitness: &#123;&#125;  best position: &#123;&#125;\".format(self.global_best_fitness, self.global_best_position))        print(\"searching end\")        plt.figure()        plt.title(\"best fitness\")        plt.xlabel(\"generation\")        plt.ylabel(\"fitness\")        plt.plot(list(range(self.num_generation + 1)), best_fitness_of_each_generation, color='b', linewidth=2)        os.makedirs(\"output\", exist_ok=True)        plt.savefig(\"output/simple_pso_best_fitness.png\", dpi=240)if __name__ == '__main__':    pso = PSO(50, 5, 2, np.array([0., 0.]), np.array([100., 100.]))    pso.run()\n\n速度限制考虑 \n\\phi_2=0\n\n\n\n\n\n\n\n \n \n \n \n\n 的简化版PSO算法，粒子位置和速度的更新如下：\n\n\\begin{align}\n\\boldsymbol{v}_i(t+1) &amp;= \\boldsymbol{v}_i(t) + \\phi_1(\\boldsymbol{b}_i - \\boldsymbol{x}_i(t)) \\\\\n\\boldsymbol{x}_i(t+1) &amp;= \\boldsymbol{x}_i(t) + \\boldsymbol{v}_i(t+1)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n\n\n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n\n上式可写成\n\n\\begin{bmatrix}\n\\boldsymbol{x}_i(t+1) \\\\\n\\boldsymbol{v}_i(t+1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 - \\phi_1 &amp; 1 \\\\\n-\\phi_1 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{x}_i(t) \\\\\n\\boldsymbol{v}_i(t)\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\phi_1 \\\\\n\\phi_2\n\\end{bmatrix}\n\\boldsymbol{b}_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n\n\n \n \n\n \n\n\n\n \n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n \n\n\n \n\n\n \n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n \n\n\n \n \n\n\n\n右边矩阵的特征值为\n\n\\lambda = \\frac{2 - \\phi_1 \\pm \\sqrt{\\phi_1^2 - 4 \\phi_1}}2\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n\n \n \n\n \n\n \n\n\n \n \n \n \n \n\n \n \n\n\n\n\n \n\n\n\n\n特征值 \n\\lambda\n\n\n\n\n \n\n 支配着系统的稳定性（解释见附录）。\n\n当 \n\\phi_1 \\in [0, 4]\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n 时，\n|\\lambda| = 1\n\n\n\n\n\n\n\n \n \n \n \n \n\n，系统处于临界稳定，对某些初始情况下的\n\\boldsymbol{x}_i(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \n\\boldsymbol{v}_i(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 会无限增大。\n当 \n\\phi_1 &gt; 4\n\n\n\n\n\n\n\n \n \n \n \n\n 时，存在 \n|\\lambda_2| &gt; 1\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n，系统不稳定，\n\\boldsymbol{x}_i(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \n\\boldsymbol{v}_i(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 会无限增大，无法收敛。\n\n因此需要限制速度。\n惯性权重可以通过给速度 \n\\boldsymbol{v}_i(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 添加一个惯性权重 \nw\n\n\n\n\n \n\n 使得粒子在优化过程中减小惯性以获得更好的性能。\n\n\\boldsymbol{v}_i \\leftarrow w \\boldsymbol{v}_i + \\phi_1(\\boldsymbol{b}_i - \\boldsymbol{x}_i) + \\phi_2 (\\boldsymbol{h}_i - \\boldsymbol{x}_i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n\n惯性权重 \nw\n\n\n\n\n \n\n 经常取从第一代的 0.9 减少到最后一代的 0.4 左右。\n随着迭代进行，惯性权重有助于降低粒子的速度从而改善收敛性。\n压缩系数惯性权重常常使用压缩系数来实现而不是惯性权重。\n\n\\boldsymbol{v}_i \\leftarrow K[ \\boldsymbol{v}_i + \\phi_1(\\boldsymbol{b}_i - \\boldsymbol{x}_i) + \\phi_2 (\\boldsymbol{h}_i - \\boldsymbol{x}_i) ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n其中，\nK\n\n\n\n\n \n\n 被称为压缩系数。\n可以一般化的写成：\n\n\\begin{align}\n\\boldsymbol{v}_i(t + 1) &amp;= K[\\boldsymbol{v}_i(t) + \\phi_T (\\boldsymbol{p}_i(t) - \\boldsymbol{x}_i(t))] \\\\\n\\phi_T &amp;= \\phi_{1} + \\phi_{2} \\\\\n\\boldsymbol{p}_i(t) &amp;= \\frac{\\phi_1 \\boldsymbol{b}_i(t) + \\phi_2 \\boldsymbol{h}_i(t)}{\\phi_1+\\phi_2}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n \n\n \n \n\n\n\n\n\n\n\n\n\n定理： 如果速度更新中 \n\\boldsymbol{b}_i\n\n\n\n\n\n \n \n\n 和 \n\\boldsymbol{h}_i\n\n\n\n\n\n \n \n\n 为常数，且 \n\\phi_T &gt; 4\n\n\n\n\n\n\n\n \n \n \n \n\n，则对于\n\nK &lt; \\frac{2}{\\phi_T - 2}\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n\n \n \n \n \n\n\n\n\n\n粒子群优化是稳定的。\n因此可以取\n\n\\begin{align}\nK &amp;=\\frac{2\\alpha}{\\phi_{T,max} -2} \\\\\n\\phi_{T,max} &amp;= \\phi_{1,max} + \\phi_{2,max}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n\n \n\n\n\n\n \n \n\n\n \n\n \n \n \n \n \n\n \n \n\n\n\n\n\n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n\n\n\n其中，\n\\alpha \\in (0, 1)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 指示在粒子群优化算法变得不稳定之前，压缩系数 \nK\n\n\n\n\n \n\n 与其理论上限的接近程度，较大的 \n\\alpha\n\n\n\n\n \n\n 允许更多的探索，较小的 \n\\alpha\n\n\n\n\n \n\n 强调对局部的开发。\n通过使迭代过程中 \n\\alpha\n\n\n\n\n \n\n 由大变小能够使算法在前期注重探索，在后期注重局部寻优，从而取得更好的效果。\n全局速度更新\n\\boldsymbol{v}_i \\leftarrow K[ \\boldsymbol{v}_i + \\phi_1(\\boldsymbol{b}_i - \\boldsymbol{x}_i) + \\phi_2 (\\boldsymbol{h}_i - \\boldsymbol{x}_i) + \\phi_3(\\boldsymbol{g} - \\boldsymbol{x}_i)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n\n其中， \n\\boldsymbol{g}\n\n\n\n\n \n\n 为历史中所有粒子的最好位置。\n可以一般化的写成：\n\n\\begin{align}\n\\boldsymbol{v}_i(t + 1) &amp;= K[\\boldsymbol{v}_i(t) + \\phi_T (\\boldsymbol{p}_i(t) - \\boldsymbol{x}_i(t))] \\\\\n\\phi_T &amp;= \\phi_{1} + \\phi_{2} + \\phi_{3} \\\\\n\\boldsymbol{p}_i(t) &amp;= \\frac{\\phi_1 \\boldsymbol{b}_i(t) + \\phi_2 \\boldsymbol{h}_i(t) + \\phi_3 \\boldsymbol{g}(t)}{\\phi_1+\\phi_2+\\phi_3}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n\n \n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n\n\n\n\n\n\n\n完全知情的粒子群让每个粒子的速度都根据当代所有粒子的位置来更新，而不是依据少数几个当代粒子或者历史上的最优粒子。\n可以一般化的写成：\n\n\\begin{align}\n\\boldsymbol{v}_i(t + 1) &amp;= K[\\boldsymbol{v}_i(t) + \\phi_T (\\boldsymbol{p}_i(t) - \\boldsymbol{x}_i(t))] \\\\\n\\phi_T &amp;= \\sum_{j=1}^M \\phi_{j} \\\\\n\\boldsymbol{p}_i(t) &amp;= \\frac{ \\sum_{j=1}^M w_{ij}\\phi_j \\boldsymbol{b}_i(t)}{\\sum_{j=1}^M w_{ij} \\phi_j}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n \n\n\n\n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n\n \n \n\n\n\n\n\n\n\n\n\n其中 \n\\boldsymbol{b}_j(t)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 为第 \nj\n\n\n\n\n \n\n 个粒子目前为止找到的最好的解：\n\n\\boldsymbol{b}_j(t) = \\arg \\min_x f(\\boldsymbol{x}): \\boldsymbol{x} \\in \\{ \\boldsymbol{x}_j(0), \\boldsymbol{x}_j(1), \\cdots, \\boldsymbol{x}_j(t)\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n \n\n\n \n \n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n常取：\n\n\\begin{align}\n\\phi_{j,max} &amp;\\approx 2 \\\\\nK &amp;= \\frac{2 \\alpha}{M \\phi_{T,max} - 2}, \\alpha \\in (0,1) \\\\\n\\phi_{T,max} &amp;= \\sum_{j=1}^M \\phi_{j,max}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n\n \n \n \n \n \n \n \n \n\n\n \n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n \n \n\n\n\n\n\n\n\n负强化的粒子群优化算法原本的PSO算法强调粒子应该向好的粒子靠近，而负强化的粒子群优化算法(Negative inreforcement particle swarm optmization, NPSO)除了要求向好的粒子靠近，还要求远离不好的粒子。\n\n\\begin{align}\n\\boldsymbol{v}_i \\leftarrow &amp; K[ \\boldsymbol{v}_i + \\phi_1(\\boldsymbol{b}_i - \\boldsymbol{x}_i) + \\phi_2 (\\boldsymbol{h}_i - \\boldsymbol{x}_i) + \\phi_3(\\boldsymbol{g} - \\boldsymbol{x}_i) \\\\ &amp;- \\phi_4(\\boldsymbol{\\bar b}_i - \\boldsymbol{x}_i) - \\phi_5 (\\boldsymbol{\\bar h}_i - \\boldsymbol{x}_i) - \\phi_6(\\boldsymbol{\\bar g} - \\boldsymbol{x}_i)]\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n\n其中，\n\\boldsymbol{\\bar b}_i\n\n\n\n\n\n\n \n \n \n\n 是第 \ni\n\n\n\n\n \n\n 个粒子以前最差的位置，\n\\boldsymbol{\\bar h}_i\n\n\n\n\n\n\n \n \n \n\n 是第 \ni\n\n\n\n\n \n\n 个粒子的领域当代最差的位置，\n\\boldsymbol{\\bar g}\n\n\n\n\n\n \n \n\n 是整个种群以前最差的位置。\n附录： 离散线性定常系统的稳定性离散线性定常系统可以描述为\n\n\\boldsymbol{x}(t+1)=\\boldsymbol{A} \\boldsymbol{x}(t)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n当 \nt \\rightarrow \\infty\n\n\n\n\n\n\n \n \n \n\n 时，\n\\boldsymbol{x}(t) \\rightarrow \\boldsymbol{0}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 则称该系统是稳定的。\n我们可以使用 \n\\boldsymbol{A}\n\n\n\n\n \n\n 的特征值和特征向量来描述该系统的稳定性。\n\n\\boldsymbol{A}\n\n\n\n\n \n\n 是一个用于将 \n\\boldsymbol{x}(t)\n\n\n\n\n\n\n\n \n \n \n \n\n 线性变换到另一个空间的变换。而 \n\\boldsymbol{A}\n\n\n\n\n \n\n 的特征向量的含义是这一变换中不变的基，可以理解为坐标轴。\n\\boldsymbol{A}\n\n\n\n\n \n\n 的特征值则代表每一个维度上的缩放比例。\n一个直观的理解是想要让空间中的一个点在经过无数次空间变换后最终收敛到 \n\\boldsymbol{0}\n\n\n\n\n \n\n，那么每一次空间变换都应该将空间缩小，因此 \n\\boldsymbol{A}\n\n\n\n\n \n\n 的特征值的模应该要小于1。\n参考文献[1] Simon D. Evolutionary optimization algorithms[M]. John Wiley &amp; Sons, 2013: 265-289.\n","plink":"https://yuxinzhao.net/particle-swarm-optimization/"},{"title":"使用TensorRT加速PyTorch模型","date":"2021-04-16T13:55:20.000Z","updated":"2022-01-04T08:35:48.647Z","content":"本文将介绍使用TensorRT推理引擎对PyTorch模型进行加速，便于算法的部署。\n\n\n\n\nPyTorch模型转换成TensorRT引擎的路径目前，PyTorch模型转TensorRT需要借助ONNX作为中间的交换格式，先将PyTorch模型转换成ONNX模型，再将ONNX模型转换成TensorRT引擎。\n环境准备系统：Ubuntu18.04\n\n从TensorRT官网下载对应版本的TensorRT安装包。本例中下载到的是TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.1.tar.gz。\n\n将压缩包解压到/opt目录下。\n1tar xzvf TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.1.tar.gz -C /opt\n\n设置环境变量\n将以下两条放入~/.bashrc中。\n12export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/TensorRT-7.2.3.4/lib:/usr/local/cuda/lib64export PATH=$PATH:/opt/TensorRT-7.2.3.4/bin\n\n\n\n\n   将其生效。\n   1source ~/.bashrc\n\n\n安装TensorRT的python包及其他辅助python包\n1234pip install /opt/TensorRT-7.2.3.4/python/tensorrt-7.2.3.4-cp38-none-linux_x86_64.whlpip install /opt/TensorRT-7.2.3.4/uff/uff-0.6.9-py2.py3-none-any.whlpip install /opt/TensorRT-7.2.3.4/graphsurgeon/graphsurgeon-0.4.5-py2.py3-none-any.whlpip install /opt/TensorRT-7.2.3.4/onnx_graphsurgeon/onnx_graphsurgeon-0.2.6-py2.py3-none-any.whl\n\n安装其他必要的库\n123456pip install -i https://mirrors.aliyun.com/pypi/simple pycuda onnx-simplifiersed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list \\&amp;&amp; apt-get update -y \\&amp;&amp; apt-get install -y -f libglib2.0-0 libsm6 libxrender1 libxext6 libgl1-mesa-glx \\&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple opencv-python\n\n\n\n\n\nPyTorch转换成ONNXONNX格式是本身是由Facebook公司参与开发的，受到PyTorch的原生支持，可用torch.onnx模块进行ONNX的导出。\n准备待转换的PyTorch模型首先构建好模型并加载上权重，还可以将标准化、阈值化等预处理、后处理步骤也加入到模型中，将尽可能多的步骤放到ONNX中，后续可以一并转换成TensorRT引擎，避免因预处理、后处理步骤放在了CPU上执行或使用PyTorch实现而造成的数据在GPU和CPU上频繁传输导致算法速度降低。\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.onnximport torch.nn as nnimport ...class OnnxModel(nn.Module):    def __init__(self, cfg):        super().__init__()        self.model = build_model(cfg)        self.model.load_state_dict(torch.load(cfg.MODEL.WEIGHTS))        self.num_classes = 2        assert self.num_classes == 2, \"the OnnxModel implementation is only for num_classes == 2\"    def forward(self, images, images_wh, pixel_mean_std, score_thresholds):        images = (images - pixel_mean_std[:, :1].view(-1, 1, 1, 1)) / pixel_mean_std[:, 1:].view(-1, 1, 1, 1)        bboxes, classification = self.model(images)        batch_size, num_boxes = [int(i) for i in bboxes.shape[:2]]        h, w = [int(i) for i in images.shape[-2:]]        standard_wh = torch.from_numpy(np.array([w, h]).reshape(1, 2)).cuda()        wh_scale = images_wh.max(1, keepdims=True)[0] / (images_wh * standard_wh)        bboxes *= wh_scale[:, None, :].repeat(1, 1, 2)        bboxes = torch.clamp(bboxes, min=0, max=1)        classification_class0 = classification[:, :, 0]        classification_class1 = classification[:, :, 1]        is_valid_boxes = (bboxes[:, :, 0] &lt; bboxes[:, :, 2]) &amp; (bboxes[:, :, 1] &lt; bboxes[:, :, 3])        is_class0_over_threshold = classification_class0 &gt; score_thresholds[0]        is_class1_over_threshold = classification_class1 &gt; score_thresholds[1]        is_0_classes = classification_class0 &gt; classification_class1        is_class0_score_save = is_class0_over_threshold &amp; is_0_classes &amp; is_valid_boxes        is_class1_score_save = is_class1_over_threshold &amp; ~is_0_classes &amp; is_valid_boxes        index_score_save = torch.cat([is_class0_score_save.float()[:, :, None], is_class1_score_save.float()[:, :, None]], dim=2)        classification *= index_score_save        images_wh_helper = images_wh[:, None, :].repeat(1, 1, 2)        return bboxes, classification, images_wh_helper\n\n受TensorRT的限制，某些操作不能在PyTorch中使用，需要用其他的形式进行代替：\n\n不能够对Boolean类型张量进行索引或切片。TensorRT不允许对Boolean类型的张量进行索引或切片，因此一个替代的方案是在需要进行索引或切片前，先将Boolean类型张量转换成int类型张量或float类型张量，再进行索引或切片。\n\n12345temp = bboxes[:, :, 0:2] &lt; bboxes[:, :, 2:4]is_valid_boxes = temp[:, :, 0] &amp; temp[:, :, 1]  # 该操作对布尔类型的张量进行了索引，可以正常转换成ONNX，但ONNX转换成TensorRT将会失败，因尽可能避免# 可改成is_valid_boxes = (bboxes[:, :, 0] &lt; bboxes[:, :, 2]) &amp; (bboxes[:, :, 1] &lt; bboxes[:, :, 3])\n\n\n不能够给张量的切片赋值。\n\n12345bboxes[:, :, 0::2] /= w  # 此操作单独改变了切片中的值，无法转换成ONNXbboxes[:, :, 1::2] /= h# 可改成bboxes /= torch.cat([w, h]).repeat(1, 1, 2)\n\n转换PyTorch到ONNX主要使用torch.onnx.export接口将PyTorch模型转换成ONNX。\n将待转换的模型准备好，放到gpu设备上，并准备好一个输入的样本用于运行生成静态图。\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849device = \"cuda:0\"model = OnnxModel(cfg)model.to(device)model.eval()batch_size = 2image_shape = (batch_size, cfg.MODEL.INPUT.IN_CHANNELS, *cfg.MODEL.INPUT.SIZE)images = torch.randn(*image_shape).to(device) * 255pixel_mean_std = torch.Tensor(batch_size * [0.0, 1.0]).view(batch_size, 2).to(device)score_thresholds = torch.Tensor([0.2, 0.2]).to(device)images_wh = torch.Tensor(batch_size * [640, 480]).view(batch_size, 2).to(device)model(images, images_wh, pixel_mean_std, score_thresholds)torch.onnx.export(    model,    (images, images_wh, pixel_mean_std, score_thresholds),    out_onnx_path,    export_params=True,    input_names=['images', 'images_width_height', 'pixel_mean_std', 'score_thresholds'],    output_names=['bboxes', 'classification', 'images_width_height_helper'],    do_constant_folding=True,    verbose=True,    opset_version=12,    dynamic_axes=&#123;        \"images\": &#123;            0: 'batch_size',        &#125;,        \"images_width_height\": &#123;            0: 'batch_size',        &#125;,        \"pixel_mean_std\": &#123;            0: 'batch_size',        &#125;,        \"bboxes\": &#123;            0: 'batch_size',        &#125;,        \"classification\": &#123;            0: 'batch_size',        &#125;,        \"images_width_height_helper\": &#123;            0: 'batch_size',        &#125;    &#125;,)\n\n简化静态图由于PyTorch底层实现上产生的ONNX静态图中还会存在着很多可以合并优化的步骤，其中有些步骤会在ONNX转换成TensorRT的步骤中出错，因此需要将得到的ONNX模型进行简化。简化的工具可使用python包onnx-simplifier。\n1234567891011121314import onnxsimmodel_opt, check_ok = onnxsim.simplify(    out_onnx_path,    dynamic_input_shape=True,    input_shapes=&#123;        'images': image_shape,        'images_width_height': (batch_size, 2),        'pixel_mean_std': (batch_size, 2),    &#125;)if check_ok:    onnx.save(model_opt, out_onnx_path)else:    raise Exception(\"Check failed\")\n\n注意到此时我们生成的静态图仍然是动态batchsize的。\nONNX转换成TensorRT上一步中我们得到的onnx模型是动态大小的，直接转换成TensorRT模型需要使用TensorRT的dynamic shape特性，但是该特性下会有很多问题，部分模块会无法使用，因此先介绍如何转换成固定张量维度的TensorRT引擎。\n添加非极大值抑制插件由于非极大值抑制操作无法直接从PyTorch转换到ONNX，因此需要手动添加非极大值抑制，可以通过TensorRT提供的插件机制，实现无法转换的部分。由于TensorRT官方提供了各种插件，其中包括了非极大值抑制，因此可以很方便地使用非极大值抑制插件而不必自己编写。\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def add_nms(onnx_model, nms_iou_threshold, nms_top_k, nms_keep_top_k):    import onnx_graphsurgeon as gs    graph = gs.import_onnx(onnx_model)    bboxes_tensor, classification_tensor, images_width_height_helper_tensor = graph.outputs    batch_size = bboxes_tensor.shape[0]    # add reshape    bboxes_new_shape = gs.Constant(        \"bboxes_new_shape\",        values=np.array([bboxes_tensor.shape[0], bboxes_tensor.shape[1], 1, bboxes_tensor.shape[2]], dtype=np.int64),    )    bboxes_new_tensor = gs.Variable(        \"bboxes_new\",        shape=tuple(int(i) for i in bboxes_new_shape.values),        dtype=bboxes_tensor.dtype,    )    bboxes_reshape_node = gs.Node(        op=\"Reshape\",        inputs=[bboxes_tensor, bboxes_new_shape],        outputs=[bboxes_new_tensor],    )    graph.nodes.append(bboxes_reshape_node)    # add nms    nms_attrs = &#123;        \"shareLocation\": True,        \"backgroundLabelId\": -1,        \"numClasses\": 2,        \"topK\": nms_top_k,        \"keepTopK\": nms_keep_top_k,        \"scoreThreshold\": 0.01,        \"iouThreshold\": nms_iou_threshold,        \"isNormalized\": True,        \"clipBoxes\": True,        \"plugin_version\": \"1\",        \"plugin_namespace\": \"\",    &#125;    num_detections_tensor = gs.Variable(        \"num_detections\",        shape=(batch_size, 1),        dtype=np.int32,    )    nmsed_boxes_norm_tensor = gs.Variable(        \"nmsed_boxes_norm\",        shape=(batch_size, nms_attrs['keepTopK'], 4),        dtype=np.float32,    )    nmsed_scores_tensor = gs.Variable(        \"nmsed_scores\",        shape=(batch_size, nms_attrs['keepTopK']),        dtype=np.float32,    )    nmsed_classes_tensor = gs.Variable(        \"nmsed_classes\",        shape=(batch_size, nms_attrs['keepTopK']),        dtype=np.float32,    )    nms_node = gs.Node(        op=\"BatchedNMS_TRT\",        name='NMS',        attrs=nms_attrs,        inputs=[bboxes_new_tensor, classification_tensor],        outputs=[num_detections_tensor, nmsed_boxes_norm_tensor, nmsed_scores_tensor, nmsed_classes_tensor],    )    graph.nodes.append(nms_node)    #rescale box xyxy from (1, 1) to (w, h)    nmsed_boxes_tensor = gs.Variable(        \"nmsed_boxes\",        shape=(batch_size, nms_attrs['keepTopK'], 4),        dtype=np.float32,    )    scale_node = gs.Node(        op=\"Mul\",        name=\"scale_width_height\",        inputs=[nmsed_boxes_norm_tensor, images_width_height_helper_tensor],        outputs=[nmsed_boxes_tensor],    )    graph.nodes.append(scale_node)    #nmsed_boxes_tensor = nmsed_boxes_norm_tensor    # redefine output    graph.outputs = [num_detections_tensor, nmsed_boxes_tensor, nmsed_scores_tensor, nmsed_classes_tensor]    graph.cleanup().toposort()    onnx_model = gs.export_onnx(graph)    return onnx_model\n\n将ONNX从动态形状转换成固定形状1234567891011121314151617import onnxsimmodel_opt, check_ok = onnxsim.simplify(    base_onnx_file_path,    dynamic_input_shape=False,    input_shapes=&#123;        'images': image_shape,        'images_width_height': (image_shape[0], 2),        'pixel_mean_std': (image_shape[0], 2),    &#125;)if check_ok:    print(\"success batchlize\")    model_opt = add_nms(model_opt, nms_iou_threshold, nms_top_k, nms_keep_top_k)    print(\"success add nms\")    onnx.save(model_opt, out_onnx_file_path)else:    raise Exception(\"Check failed\")\n\n转换ONNX到TensorRT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657TRT_LOGGER = trt.Logger()trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")explicit_batch = 1 &lt;&lt; (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)def build_engine(max_batch_size, onnx_file_path, engine_file_path, fp16_mode=False):    \"\"\" Take an ONNX file and creates a TensorRT engine to run inference with\"\"\"    with trt.Builder(TRT_LOGGER) as builder, \\            builder.create_network(explicit_batch) as network, \\            trt.OnnxParser(network, TRT_LOGGER) as parser:        builder.max_workspace_size = 1 &lt;&lt; 30  # your workspace size 1GiB        builder.max_batch_size = max_batch_size        builder.fp16_mode = fp16_mode        builder.int8_mode = False        # Parse model file        if not os.path.exists(onnx_file_path):            quit(\"ONNX file &#123;&#125; not found\".format(onnx_file_path))        print(\"Loading ONNX file from path &#123;&#125; ...\".format(onnx_file_path))        with open(onnx_file_path, 'rb') as model:            print(\"Beginning ONNX file parsing\")            parser.parse(model.read())        print(\"Completed parsing of ONNX file\")        print(\"Building an engine from file &#123;&#125;; this may take a while...\".format(onnx_file_path))        engine = builder.build_cuda_engine(network)        print(\"Completed creating Engine\")        with open(engine_file_path, 'wb') as f:            f.write(engine.serialize())def generate_engine_from_onnx(batch_size, nms_iou_threshold, nms_top_k, nms_keep_top_k):    cfg = get_cfg('config.yaml')    base_onnx_file_path = cfg.MODEL.BASE_ONNX_WEIGHTS(cfg)    if not os.path.exists(base_onnx_file_path):        raise FileNotFoundError(\"please generate base onnx file firstly.\")    cfg.MODEL.BATCH_SIZE = batch_size    image_shape = (batch_size, cfg.MODEL.INPUT.IN_CHANNELS, *cfg.MODEL.INPUT.SIZE)    if nms_iou_threshold is None:        nms_iou_threshold = cfg.MODEL.NMS_THRESHOLD    else:        cfg.MODEL.NMS_THRESHOLD = nms_iou_threshold    out_onnx_file_path = cfg.MODEL.ONNX_WEIGHTS(cfg)    out_engine_file_path = cfg.MODEL.TENSORRT_WEIGHTS(cfg)    if not os.path.exists(out_onnx_file_path):        from .modify_onnx import build_onnx_file        build_onnx_file(base_onnx_file_path, out_onnx_file_path, image_shape, nms_iou_threshold, nms_top_k, nms_keep_top_k)    if not os.path.exists(out_engine_file_path):        build_engine(batch_size, out_onnx_file_path, out_engine_file_path, fp16_mode=False)\n\n使用TensorRT引擎进行推理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121import pycuda.driver as cudaimport tensorrt as trtimport numpy as npTRT_LOGGER = trt.Logger()trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")explicit_batch = 1 &lt;&lt; (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)class HostDeviceMem(object):    def __init__(self, host_mem, device_men):        \"\"\" Within this context, host_men meas the cpu memory, device_mem means the gpu memory\"\"\"        self.host = host_mem        self.device = device_men    def __str__(self):        return \"Host:\\n&#123;&#125;\\nDevice:&#123;&#125;\\n\".format(self.host, self.device)    def __repr__(self):        return self.__str__()def allocate_buffers(engine):    inputs = []    outputs = []    bindings = []    stream = cuda.Stream()    for binding in engine:        size = trt.volume(engine.get_binding_shape(binding))        dtype = trt.nptype(engine.get_binding_dtype(binding))        # Allocate host and device buffers        host_mem = cuda.pagelocked_empty(size, dtype)        device_mem = cuda.mem_alloc(host_mem.nbytes)        # Append the device buffer to device bindings        bindings.append(int(device_mem))        if engine.binding_is_input(binding):            inputs.append(HostDeviceMem(host_mem, device_mem))        else:            outputs.append(HostDeviceMem(host_mem, device_mem))    return inputs, outputs, bindings, streamdef get_engine(engine_file_path):    with open(engine_file_path, 'rb') as f, trt.Runtime(TRT_LOGGER) as runtime:        return runtime.deserialize_cuda_engine(f.read())def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):    # Transfer data from CPU to the GPU.    [cuda.memcpy_htod_async(input.device, input.host, stream) for input in inputs]    # Run inference    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)    # Transfer predictions back from the GPU.    [cuda.memcpy_dtoh_async(output.host, output.device, stream) for output in outputs]    # Synchronize the stream    stream.synchronize()    # Return only the host outputs.    return [output.host for output in outputs]class Predictor:    def __init__(self, cfg):        self.cfg = cfg.copy()        self.input_size = cfg.MODEL.INPUT.SIZE        self.cuda_device = cuda.Device(cfg.MODEL.DEVICE)        self.cuda_context = self.cuda_device.make_context()        self.engine = get_engine(cfg.MODEL.TENSORRT_WEIGHTS(cfg))        self.context = self.engine.create_execution_context()        self.inputs, self.outputs, self.bindings, self.stream = allocate_buffers(self.engine)        self.max_batch_size = cfg.MODEL.BATCH_SIZE        self.thresholds = np.array([cfg.MODEL.PEOPLE_THRESHOLD, cfg.MODEL.CAR_THRESHOLD], dtype=np.float32)        self.nms_threshold = cfg.MODEL.NMS_THRESHOLD    def __del__(self):        self.cuda_context.pop()        del self.cuda_context    def __call__(self, images, pixel_mean_std_list):        self.cuda_context.push()        batch_size = len(images)        if batch_size &gt; self.max_batch_size:            raise Exception(\"the max_batch_size is &#123;&#125;\".format(self.max_batch_size))                images_width_height = np.array([[image.shape[1], image.shape[0]] for image in images], dtype=np.float32)        pixel_mean_std = np.stack(pixel_mean_std_list, axis=0)        images, resize_params_list = zip(*[resize_padding(images[i], size=self.input_size, fill_pixel=pixel_mean_std[i, 0]) for i in range(len(images))])        images = [np.expand_dims(image, -1) if len(image.shape) == 2 else image for image in images]        images = np.stack(images, axis=0).transpose(0, 3, 1, 2)        self.inputs[0].host = images.reshape(-1).astype('float32')        self.inputs[1].host = images_width_height        self.inputs[2].host = pixel_mean_std        self.inputs[3].host = self.thresholds        trt_outputs = do_inference(self.context, self.bindings, self.inputs, self.outputs, self.stream)        num_detections = trt_outputs[0][:batch_size]        nmsed_boxes = trt_outputs[3].reshape(self.engine.max_batch_size, -1, 4)[:batch_size]        nmsed_scores = trt_outputs[1].reshape(self.engine.max_batch_size, -1)[:batch_size]        nmsed_classes = trt_outputs[2].reshape(self.engine.max_batch_size, -1)[:batch_size]        prediction_list = [            &#123;                'rois': nmsed_boxes[i, :num_detections[i]],                'class_ids': nmsed_classes[i, :num_detections[i]],                'scores': nmsed_scores[i, :num_detections[i]],            &#125;            for i in range(batch_size)        ]        self.cuda_context.pop()        return prediction_list\n\n12345678910from pycuda.driver as cudacuda.init()cfg = ...predictor = Predictor(cfg)images = ...pixel_mean_std_list = ...prediction_list = predictor(images, pixel_mean_std_list)\n\n注意：\n\n推理的输出trt_output不一定会按照PyTorch输出时的顺序或修改后ONNX模型中的输出顺序，需要先确定根据输出输出大小、类型、数值确定。\nTensorRT引擎的输入和输出都是一维向量，输入前需要展开成一维向量，输出后将一维向量reshape成预定的大小。\n在进行推理前，对于每一个使用TensorRT的进程，需要先导入pycuda.driver，并使用cuda.init()进行初始化。\n使用TensorRT进行一次推理前需要先cuda.context.push()，并在推理完成后cuda.context.pop()。\n\n","plink":"https://yuxinzhao.net/speed-up-pytorch-model-using-tensorrt/"},{"title":"强连通分量","date":"2021-01-05T11:52:50.000Z","updated":"2022-01-04T08:35:48.647Z","content":"本文描述强连通分量的概念及使用深度优先搜索将有向图分解为强连通分量的方法。\n\n\n什么是强连通分量有向图 \nG=(V,E)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的强连通分量(Strongly Connected Component)是一个最大结点子集 \nC \\subseteq V\n\n\n\n\n\n\n \n \n \n\n，对于该集合中的任意一对结点 \nu\n\n\n\n\n \n\n 和 \nv\n\n\n\n\n \n\n 来说，路径 \nu \\leadsto v\n\n\n\n\n\n\n \n \n \n\n 和路径 \nv \\leadsto u\n\n\n\n\n\n\n \n \n \n\n 同时存在，即结点 \nu\n\n\n\n\n \n\n 和 \nv\n\n\n\n\n \n\n 可以互相到达。\n\n上图(有向图 \nG\n\n\n\n\n \n\n)是一个强连通分量的例子，其中每个阴影区域都是一个强连通分量。\n\n值得注意的是有向图 \nG\n\n\n\n\n \n\n 的转置图 \nG^T=(V, E^T)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n\n (调转所有有向边的方向)的强连通分量与 \nG\n\n\n\n\n \n\n 相同。\n\n找到强连通分量后可以将强连通分量合并成一个点，将原本的有向图变成有向无环图，即分量图。\n怎么计算强连通分量算法步骤深度优先搜索算法(DFS)在每个结点上盖上一个时间戳。每个结点 \nv\n\n\n\n\n \n\n 有两个时间戳：\n\n第一个时间戳 \nv.d\n\n\n\n\n\n\n \n \n \n\n 记录结点 \nv\n\n\n\n\n \n\n 第一次被发现的时间\n第二个时间戳 \nv.f\n\n\n\n\n\n\n \n \n \n\n 记录搜索完成对结点 \nv\n\n\n\n\n \n\n 的邻接链表扫描的时间\n\n算法伪代码：\n\n调用 \nDFS(G)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 计算每个结点 \nu\n\n\n\n\n \n\n 的完成扫描时间 \nu.f\n\n\n\n\n\n\n \n \n \n\n \n计算 \nG^T\n\n\n\n\n\n \n \n\n\n调用 \nDFS(G^T)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n，但在DFS的主循环中，以 \nu.f\n\n\n\n\n\n\n \n \n \n\n 的降序选择下一个进行搜索的根节点\n输出 \nDFS(G^T)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n 得到的森林，森林中每棵树都是一个强连通分量\n\n为什么第二次DFS得到的每棵树都是强连通分量假定图 \nG\n\n\n\n\n \n\n 有强连通分量 \nC_1\n\n\n\n\n\n \n \n\n, \nC_2\n\n\n\n\n\n \n \n\n, \n\\cdots\n\n\n\n\n \n\n, \nC_k\n\n\n\n\n\n \n \n\n。\n首先由于分量图是一个有向无环图，可以得到引理1。\n引理1：设 \nC\n\n\n\n\n \n\n 和 \nC'\n\n\n\n\n\n \n \n\n 为有向图 \nG=(V,E)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的两个不同的强连通分量，设结点 \nu,v \\in C\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，结点 \nu', v' \\in C'\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n，假定图 \nG\n\n\n\n\n \n\n 包含一条从结点 \nu\n\n\n\n\n \n\n 到结点 \nu'\n\n\n\n\n\n \n \n\n 的路径 \nu \\leadsto u'\n\n\n\n\n\n\n \n \n\n \n \n\n\n。那么图 \nG\n\n\n\n\n \n\n 不可能包含一条从结点 \nv'\n\n\n\n\n\n \n \n\n 到结点 \nv\n\n\n\n\n \n\n 的路径 \nv' \\leadsto v\n\n\n\n\n\n\n \n \n \n \n\n。\n将结点的发现时间 \nv.d\n\n\n\n\n\n\n \n \n \n\n 和完成时间 \nv.f\n\n\n\n\n\n\n \n \n \n\n 的概念推广到结点集合上：如果结点集合 \nU \\subseteq V\n\n\n\n\n\n\n \n \n \n\n，则定义 \nd(U)=\\min_{u \\in U} \\{u.d\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n\n 和 \nf(U) = \\max_{u \\in U} \\{u.f\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n\n。即 \nd(U)\n\n\n\n\n\n\n\n \n \n \n \n\n 和 \nf(U)\n\n\n\n\n\n\n\n \n \n \n \n\n 分别是结点集合 \nU\n\n\n\n\n \n\n 中所有结点里最早发现时间和最晚的完成时间。\n引理2：设 \nC\n\n\n\n\n \n\n 和 \nC'\n\n\n\n\n\n \n \n\n 为有向图 \nG=(V,E)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的两个不同的强连通分量。假设存在一条边 \n(u, v) \\in E\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n，这里 \nu \\in C\n\n\n\n\n\n\n \n \n \n\n, \nu' \\in C'\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则 \nf(C) &gt; f(C')\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n。\n推论1：设 \nC\n\n\n\n\n \n\n 和 \nC'\n\n\n\n\n\n \n \n\n 为有向图 \nG=(V,E)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的两个不同的强连通分量。假设存在一条边 \n(u, v) \\in E^T\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n\n，这里 \nu \\in C\n\n\n\n\n\n\n \n \n \n\n, \nu' \\in C'\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，则 \nf(C) &lt; f(C')\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n。\n第二次DFS运行在 \nG^T\n\n\n\n\n\n \n \n\n 上。从完成时间最晚的强连通分量 \nC\n\n\n\n\n \n\n 开始。DFS从 \nC\n\n\n\n\n \n\n 中的某个结点 \nx\n\n\n\n\n \n\n 开始访问 \nC\n\n\n\n\n \n\n 中所有的结点。根据推论1，\nG^T\n\n\n\n\n\n \n \n\n 不可能包含从 \nC\n\n\n\n\n \n\n 到任何其他强连通量的边，因此从结点 \nx\n\n\n\n\n \n\n 开始的搜索不会访问任何其他分量中的结点。因此以 \nx\n\n\n\n\n \n\n 为根结点的树仅包含了 \nC\n\n\n\n\n \n\n 中所有结点。在完成对 \nC\n\n\n\n\n \n\n 中所有结点的访问后，从另一个强连通分量 \nC'\n\n\n\n\n\n \n \n\n 中选择一个结点作为根结点再进行DFS，这里 \nf(C')\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n 的取值再除 \nC\n\n\n\n\n \n\n 以外的所有强连通分量中最大。与在 \nC\n\n\n\n\n \n\n 上搜索的情况相同，得到的树也是一个强连通分量，以此类推，每棵深度优先树恰好是一个强连通分量。\n","thumbnail":"strongly-connected-component/logo.png","plink":"https://yuxinzhao.net/strongly-connected-component/"},{"title":"MPI基础","date":"2020-05-17T14:09:54.000Z","updated":"2022-01-04T08:35:48.603Z","content":"MPI(Message Passing Interface)是消息传递函数库的标准规范，有着多种实现，如MPICH、OPEN-MPI等。本文以MPICH为例介绍MPI的基本使用。\n\n\n安装1sudo apt-get install mpich libmpich-dev\n\n第一个MPI程序helloworld.c123456789#include &lt;stdio.h&gt;#include \"mpi.h\"int main(int argc, char *argv[]) &#123;    MPI_Init(&amp;argc, &amp;argv); /* MPI的初始化函数 */    printf(\"Hello, world!\\n\");    MPI_Finalize(); /* MPI的结束函数 */    return 0;&#125;\n\n\nMPI_Init() 和 MPI_Finalize() 将被并行化。\n\n编译：\n1mpicc helloworld.c -o helloworld\n\n运行：\n1mpirun -n 4 ./helloworld\n\n\n其中-n 4 指定进程数为4。\n\n输出：\n1234Hello, world!Hello, world!Hello, world!Hello, world!\n\n我是谁？世界有多大？在写MPI程序时，我们常需要知道\n\n任务由多少个进程进行并行计算?\n我是哪一个进程？\n\nMPI提供了两个函数分别用于解决以上两个问题：\n\nint MPI_Comm_size(MPI_Comm comm, int *size)\n获取指定通信域的进程数\n\n\nint MPI_Comm_rank(MPI_Comm comm, int*rank)\n获取指定进程的编号\n\n\n\n123456789101112#include &lt;stdio.h&gt;#include \"mpi.h\"int main(int argc, char *argv[]) &#123;    int id, numProcs;    MPI_Init(&amp;argc, &amp;argv); /* MPI的初始化函数 */    MPI_Comm_size(MPI_COMM_WORLD, &amp;numProcs); /* 获取进程数 */    MPI_Comm_rank(MPI_COMM_WORLD, &amp;id); /* 获取进程号 */    printf(\"Hello, I am %d of %d!\\n\", id, numProcs);    MPI_Finalize(); /* MPI的结束函数 */    return 0;&#125;\n\n输出：\n1234Hello, I am 0 of 4!Hello, I am 1 of 4!Hello, I am 2 of 4!Hello, I am 3 of 4!\n\nMPI数据类型MPI的数据类型分为两种：预定义类型和派生数据类型。\n\nMPI通过预定义数据类型来解决异构计算中的互操作性问题\nMPI派生数据类型用于定义由于数据类型不同且地址空间不连续的数据项组成的消息。\n\nMPI预定义数据类型\n\n\nMPI(C语言绑定)\nC\n\n\n\nMPI_BYTE\n\n\n\nMPI_CHAR\nsigned char\n\n\nMPI_DOUBLE\ndouble\n\n\nMPI_FLOAT\nfloat\n\n\nMPI_INT\nint\n\n\nMPI_LONG\nlong\n\n\nMPI_LONG_DOUBLE\nlong double\n\n\nMPI_PACKED\n\n\n\nMPI_SHORT\nshort\n\n\nMPI_UNSIGNED_CHAR\nunsigned char\n\n\nMPI_UNSIGNED\nunsigned int\n\n\nMPI_UNSIGNED_LONG\nunsigned long\n\n\nMPI_UNSIGNED_SHORT\nunsigned short\n\n\n\nMPI_BYTE 表示一个字节\nMPI_PACKED 预定义数据类型被用来实现传输地址空间不连续的数据项\n\nMPI常用常量\n\n\n常量名\n含义\n\n\n\nMPI_MAX_PROCESSOR_NAME\nMPI_Get_processor_name返回的处理器名称最大长度\n\n\nMPI_PROC_NULL\n空进程，与空进程进行通信相当于空操作\n\n\nMPI_COMM_WORLD\n缺省通信域\n\n\nMPI_COMM_NULL\n空通信域\n\n\nMPI_UNDEFINED_RANK\n未定义的进程号\n\n\nMPI_ANY_SOURCE\n接收操作中用于表示从任何源地址接受\n\n\nMPI_ANY_TAG\n接收操作中用于表示接收任何标签的消息\n\n\n消息传递MPI_Send1int MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)\n\n\nbuf：所发送消息的首地址\ncount: 将发送的数据的个数\ndatatype: 发送数据的MPI数据类型\ndest: 接收消息的进程的标识号，取值范围为0～进程数-1 或 MPI_PROC_NULL\ntag: 消息标签，取值为0～MPI_TAG_UB\ncomm: 通信域\n\nMPI_Recv1int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)\n\n\nbuf: 接收消息数据的首地址\ncount: 接收数据的最大个数\ndatatype: 接收数据的MPI数据类型\nsource: 发送消息的进程的标识号\ntag: 消息标签，取值为0～MPI_TAG_UB\ncomm: 通信域\nstatus: 返回状态\n\n123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include \"mpi.h\"const int MAX_STRING = 100;int main(int argc, char *argv[]) &#123;    char greeting[MAX_STRING];    int  comm_size;    int  my_rank;    MPI_Init(&amp;argc, &amp;argv);    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_size);    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);    if (my_rank != 0) &#123;        sprintf(greeting, \"Hello, world! I am process %d of %d!\", my_rank, comm_size);        MPI_Send(greeting, strlen(greeting) + 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);    &#125; else &#123;        printf(\"Greetings from process %d of %d!\\n\", my_rank, comm_size);        for (int q = 1; q &lt; comm_size; q++) &#123;            MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);            printf(\"%s\\n\", greeting);        &#125;    &#125;    MPI_Finalize();    return 0;&#125;\n\n输出：\n1234Greetings from process 0 of 4!Hello, world! I am process 1 of 4!Hello, world! I am process 2 of 4!Hello, world! I am process 3 of 4!\n\n通信域MPI提供了丰富的函数用于管理通信域\n\n\n\n函数名\n含义\n\n\n\nMPI_Comm_size\n获取指定通信域中进程的个数\n\n\nMPI_Comm_rank\n获取当前进程在指定通信域中的编号\n\n\nMPI_Comm_compare\n对给定的两个通信域进行比较\n\n\nMPI_Comm_dup\n复制一个已有的通信域生成一个新的通信域，两者除了通信上下文不同，其他都一样\n\n\nMPI_Comm_create\n根据给定的进程组创建一个新的通信域\n\n\nMPI_Comm_split\n从一个指定通信域分裂出多个子通信域，每个子通信域中的进程都是原通信域中的进程\n\n\nMPI_Comm_free\n释放一个通信域\n\n\n消息状态MPI_Recv中的status存放接收消息的状态\n\nMPI_Status 是MPI定义的一个数据类型，使用之前需要用户为其分配空间\n包含：\nstatus.MPI_SOURCE：发送数据进程的标识\nstatus.MPI_TAG：发送数据使用的tag\nstatus.MPI_ERROR：本接收操作返回的错误代码\n\n\n\n还可以通过status获取实际接收到的消息的长度\n1int MPI_Get_count(MPI_Status status, MPI_Datatype datatype, int *count)\n\n\nstatus: 接收操作的返回值\ndatatype: 接收缓冲区中元素的数据类型\ncount: 接收消息中的元素个数\n\n点对点通信通信模式MPI提供了以下四种通信模式\n\n标准(standard)模式\n发送的结束等于消息已从发送方发出，而不是滞留在发送方的系统缓冲区中\n对应 MPI_Send\n\n\n缓冲(buffered)模式\n用户需要事先申请一块缓冲区，通过MPI_Buffer_attch将缓冲区绑定一个进程后，发送过程中写入此缓冲区，不依赖于接收方的接收操作。\n发送结束仅表示消息进入用户指定的缓冲区中。\n对应 MPI_Bsend\n用 MPI_Buffer_detach 可以回收申请的缓冲区。\n\n\n同步(synchronous)模式\n双方握手后才进行消息的发送，不需要附加的缓冲区\n对应 MPI_Ssend\n\n\n就绪(ready)模式\n接收方必须先做出接收操作，处于就绪状态，发送方才能成功发送消息。\n对应 MPI_Rsend\n\n\n\n通信机制阻塞通信MPI_Send和MPI_Recv都是阻塞型的\n阻塞通信返回的条件：\n\n通信操作已完成\n调用的缓冲区可用\n\n非阻塞通信MPI_Send和MPI_Recv都有对应的非阻塞版本：MPI_Isend和MPI_Irecv。\n12int MPI_Isend(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)int MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request *request)\n\n\n与其阻塞版本比较多了一个参数 request: 非阻塞通信完成对象(句柄)\n\n其他通信模式也有其非阻塞版本: MPI_Ibsend, MPI_Issend, MPI_Irsend。\n通信检测1int MPI_Wait(MPI_Request *request, MPI_Status *status)\n\n\n等待指定通信请求完成才返回\n成功返回时，status包含关于完成的通信的消息，相应的request被置为MPI_REQUEST_NULL。\n\n1int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)\n\n\n无论通信是否完成都立刻返回，flag 为1表示通信完成。\n\n其他通信检测函数：\n123456int MPI_Waitany(int count, MPI_Request *array_of_requests, int *index, MPI_Status *status)int MPI_Waitall(int count, MPI_Request *array_of_requests, int *index, MPI_Status *array_of_statuses)int MPI_Waitsome(int incount, MPI_Request *array_of_requests, int *outcount, int *array_of_indices, MPI_Status *array_of_statuses)int MPI_Testany(int count, MPI_Request *array_of_requests, int *index, int *flag, MPI_Status *status)int MPI_Testall(int count, MPI_Request *array_of_requests, int *index, int *flag, MPI_Status *array_of_statuses)int MPI_Testsome(int incount, MPI_Request *array_of_requests, int *outcount, int *array_of_indices, MPI_Status *array_of_statuses)\n\n请求释放1int MPI_Request_free(MPI_Request request)\n\n\n释放指定的通信请求及其所占用的内存资源\n若该通信尚未完成，则等待通信完成后，不会影响该通信\n成功返回后request被重置为MPI_REQUEST_NULL\n\n请求撤销1int MPI_Cancel(MPI_Request request)\n\n\n非阻塞型，用于取消一个尚未完成的通信，但不能保证一定会取消，如果通信已经开始，则无法取消。\n调用MPI_Cancel后仍需要MPI_Wait, MPI_Test, MPI_Request_free来释放该通信请求。\n\n1int MPI_Test_cancelled(MPI_Status status, int *flag)\n\n\n检测是否取消成功\n\n消息探测MPI_Probe和MPI_Iprobe函数探测接收消息的内容。用户根据探测到的消息内容决定如何接收这些消息，如根据消息大小分配缓冲区等。前者为阻塞版本，后者为非阻塞版本。\n12int MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status *status)int MPI_Iprobe(int source, int tag, MPI_Comm comm, int *flag, MPI_Status *status)\n\n\nsource: 数据源的rank，可以是 MPI_ANY_SOURCE\ntag: 数据标签，可以是 MPI_ANY_TAG\ncomm: 通信域\nflag: 布尔值，探测到与否\nstatus: 探测到的消息的内容\n\nMPI_Sendrecv12int MPI_Sendrecv(void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag,                 void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status *status)\n\n\n通过轮转的方式实现一个语句同时向其他进程发送数据和从其他进程接收数据的操作\n系统会优化通信次序，从而有效避免不合理的通信次序，最大程度避免死锁\n\n123456int a, b;// ...MPI_Status status;int dest = (rank + 1) % p;  // p 为进程个数int source = (rank + p - 1) % p;MPI_Sendrecv(&amp;a, 1, MPI_INT, dest, 99, &amp;b, 1, MPI_INT, source, 99, MPI_COMM_WORLD, &amp;status);\n\n点对点通信应用示例任务：计算 Z=R(Q(P(W)))​。\n流水线型\n12345while (Not_Done) &#123;    MPI_Irecv(NextX, ... );    MPI_Isend(PreviousY, ... );    CurrentY = Q(CurrentX);&#125;\n\n双缓冲流水线型\n123456789while (Not_Done) &#123;    if (X == Xbuf0) &#123;X=Xbuf1; Y=Ybuf1; Xin=Xbuf0; Yout=Ybuf0;&#125;    else &#123;X=Xbuf0; Y=Ybuf0; Y=Ybuf0; Xin=Xbuf1; Yout=Ybuf1;&#125;    MPI_Irecv(Xin, ..., recv_handle);    MPI_Isend(Yout, ..., send_handle);    Y = Q(X); // 重叠计算    MPI_Wait(recv_handle, recv_status);    MPI_Wait(send_handle, send_status);&#125;\n\n聚合通信特点：\n\n通信空间中所有进程都参与通信操作\n每个进程都需要调用该操作函数\n\n功能：\n\n通信：完成组内数据的传输\n聚集：在通信的基础上对给定的数据完成一定的操作\n同步：实现组内所有进程在执行进度上取得一致\n\n按通信方向分为：\n\n一对多通信：一个进程向其他所有的进程发送消息，这个负责发送消息的进程称为Root进程\n多对一通信：一个进程负责从其他所有的进程接收消息，这个负责接收消息的进程也称为Root进程\n多对多通信：每个进程都向其他所有的进程发送或接收消息\n\n\n    \n        类型\n        函数名\n        含义\n    \n    \n        通信\n        MPI_Bcast\n        一对多广播同样的消息\n    \n    \n        MPI_Gather\n        多对一收集各个进程的消息\n    \n    \n        MPI_Gatherv\n        MPI_Gather的一般化\n    \n    \n        MPI_Allgather\n        全局收集\n    \n    \n        MPI_Allgatherv\n        MPI_Allgather的一般化\n    \n    \n        MPI_Scatter\n        一对多散播不同的消息\n    \n    \n        MPI_Scatterv\n        MPI_Scatter的一般化\n    \n    \n        MPI_Alltoall\n        多对多全局交换消息\n    \n    \n        MPI_Alltoallv\n        MPI_Alltoall的一般化\n    \n    \n        聚集\n        MPI_Reduce\n        多对一归约\n    \n    \n        MPI_Allreduce\n        MPI_Reduce的一般化\n    \n    \n        MPI_Reduce_scatter\n        MPI_Reduce的一般化\n    \n    \n        MPI_Scan\n        前缀和\n    \n    \n        同步\n        MPI_Barrier\n        路障同步\n    \n\n\nMPI_Bcast广播，一对多\n1int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)\n\n\nbuffer: 发送/接收缓冲区\ncount: 元素个数\ndatatype: MPI数据类型\nroot: root进程号\ncomm: 通信域\n\n123456789int p, myrank;float buf;MPI_Comm comm;MPI_Init(&amp;argc, &amp;argv);MPI_Comm_rank(comm, &amp;my_rank);MPI_Comm_size(comm, &amp;p);if (myrank == 0) buf = 1.0;MPI_Bcast(&amp;buf, 1, MPI_FLOAT, 0, comm);MPI_Finalize()\n\nMPI_Gather多对一，数据收集\n12int MPI_Gather(void *sendbuf, int sendcnt, MPI_Datatype sendtype,               void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)\n\n12345678910int p, myrank;float data[10];float *buf;MPI_Comm comm;MPI_Init(&amp;argc, &amp;argv);MPI_Comm_rank(comm, &amp;myrank);MPI_Comm_size(comm, &amp;p);if (myrank == 0)    buf = (float*)malloc(p*10*sizeof(float));MPI_Gather(data, 10, MPI_FLOAT, buf, 0, MPI_FLOAT, 0, comm);\n\nMPI_Allgather多对多，数据收集\n12int MPI_Allgather(void *sendbuf, int sendcount, MPI_Datatype sendtype,                  void *recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm)\n\n123456789int p, myrank;float data[10];float *buf;MPI_Comm comm;MPI_Init(&amp;argc, &amp;argv);MPI_Comm_rank(comm, &amp;myrank);MPI_Comm_size(comm, &amp;p);buf = (float*)malloc(p*10*sizeof(float));MPI_Gather(data, 10, MPI_FLOAT, buf, 0, MPI_FLOAT, comm);\n\nMPI_Scatter一对多，数据散发，root进程将一个大的数据块分成小块分散发给各个进程（包括root进程自己），是数据收集的逆操作。\n12int MPI_Scatter(void *sendbuf, int sendcnt, MPI_Datatype sendtype,                void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)\n\n12345678910int p, myrank;float data[10];float *buf;MPI_Comm comm;MPI_Init(&amp;argc, &amp;argv);MPI_Comm_rank(comm, &amp;myrank);MPI_Comm_size(comm, &amp;p);if (myrank == 0)    buf = (float*)malloc(p*10*sizeof(float));MPI_Scatter(data, 10, MPI_FLOAT, buf, 0, MPI_FLOAT, 0, comm);\n\nMPI_Alltoall多对多，数据分发\n\n每个进程发送一个消息给n个进程，包括它自己，这n个消息的发送缓冲区中以标号的顺序有序地存放。\n一次全局交换中共有 \nn^2\n\n\n\n\n\n \n \n\n 个消息进行通信。\n\n\n12int MPI_Alltoall(void *sendbuf, int sendcount, MPI_Datatype sendtype,                 void *recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm)\n\nMPI_Reduce简单归约，将通信域内每个进程输入缓冲区中的数据按给定的操作进行简单运算，并将结果返回到root进程的输出缓冲区中。\n\n归约操作可以使用MPI预定义的运算操作，也可以使用用户自定义的运算操作，但必须满足结合律。\n创建自定义归约运算操作：MPI_Op_create\n释放自定义的归约操作: MPI_Op_free\n\n\n\n1int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)\n\n\n参数 recvbuf 只对根进程有意义\n所有进程所提供的数据长度相同、类型相同\n\n\nMPI归约预定义操作：\n\n\n\n操作\n含义\n\n\n\nMPI_MAX\n最大值\n\n\nMPI_MIN\n最小值\n\n\nMPI_SUM\n求和\n\n\nMPI_PROD\n求积\n\n\nMPI_LAND\n逻辑与\n\n\nMPI_BAND\n按位与\n\n\nMPI_LOR\n逻辑或\n\n\nMPI_BOR\n按位或\n\n\nMPI_LXOR\n逻辑异或\n\n\nMPI_BXOR\n按位异或\n\n\nMPI_MAXLOC\n最大值及相应位置\n\n\nMPI_MINLOC\n最小值及相应位置\n\n\nMPI_Allreduce1int MPI_Allreduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)\n\n\n所有进程的recvbuf将同时获得归约运算的结果\n相当于MPI_Reduce后再将结果进行一次广播\n\n\nMPI_Scan前缀和计算\n\n每一个进程都对排在它前面的进程进行归约操作，操作结束后，第i个进程中的recvbuf中将包含前i个进程的归约结果。\n0号进程接收缓冲区中的数据就是其发送缓冲区的数据\n\n1int MPI_Scan(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)\n\n\nMPI_BarrierMPI唯一的同步函数\n\n当通信域内所有进程都执行该函数时才返回，否则将一直等待\n\n1int MPI_Barrier(MPI_Comm comm)\n\nMPI派生数据类型派生数据类型可有效减少消息传递次数，增大通信粒度，同时可以避免或减少消息传递时数据在内存中的拷贝。\n主要思想：当一个发送数据的函数知道数据项集合的一些信息时，可以在发送前收集好这些数据，而不是在发送时临时抱佛脚；同理，接收数据的函数在接收数据时可以根据这些信息将数据项分发到正确的目标内存地址。\n\n\n\n函数名\n含义\n\n\n\nMPI_Type_contiguous\n定义由相同数据类型的元素组成的类型\n\n\nMPI_Type_vector\n定义由成块的元素组成的类型，块之间具有相同间隔\n\n\nMPI_Type_indexed\n定义由成块的元素组成的类型，块长度和偏移由参数指定\n\n\nMPI_Type_create_struct\n定义由不同数据类型的元素组成的类型\n\n\nMPI_Type_commit\n提交一个派生数据类型\n\n\nMPI_Type_free\n释放一个派生数据类型\n\n\nMPI_Get_address\n返回内存地址\n\n\nMPI_Type_contiguous将一个已有的数据类型按顺序依次连续进行复制后的新类型。\n1int MPI_Type_contiguous(int count, MPI_Datatype oldtype, MPI_Datatype *newtype)\n\nMPI_Type_vector复制一个数据类型到含有相等大小块的空间，每个块通过连接相同数量的就数据类型的拷贝来获得块与块之间的空间是旧数据类型的extent的倍数。\n1int MPI_Type_vector(int count, int blocklength, int stride, MPI_Datatype oldtype, MPI_Datatype *newtype)\n\n\ncount: 块的数量\nblocklength: 块中所包含的元素个数\nstride: 各块第一个元素之间相隔的元素个数\n\nMPI_Type_indexed复制一个旧数据类型到块序列中，每个块可以包含不同的拷贝数量和具有不同的偏移，所有的块偏移都是旧数据类型extent的倍数\n1int MPI_Type_indexed(int count, int *array_of_blocklengths, int *array_of_displacements, MPI_Datatype oldtype, MPI_Datatype *newtype)\n\n\ncount: 块的数量\narray_of_blocklengths: 每个块所含的元素个数\narray_of_displacements: 各块偏移值\n\nMPI_Type_create_struct最通用的类型生成器\n1int MPI_Type_create_struct(int count, int *array_of_blocklengths, MPI_Aint *array_of_displacements, MPI_Datatype *array_of_types, MPI_Datatype *newtype)\n\n\ncount: 块的数量\narray_of_blocklengths: 每个块中所含的元素个数\narray_of_displacements: 每个块偏移字节数,MPI_Aint是一个足够存储系统地址的大整数类型\narray_of_types: 每个块中元素的类型\n\nMPI_Type_commit类型提交，新类型在通信前必须提交\n1int MPI_Type_commit(MPI_Datatype *datatype)\n\nMPI_Type_free释放类型\n1int MPI_Type_free(MPI_Datatype *datatype)\n\nMPI_Get_address返回内存地址\n1int MPI_Get_address(void *location, MPI_Aint *address)\n\n","thumbnail":"mpi/mpi_logo.jpg","plink":"https://yuxinzhao.net/mpi/"},{"title":"大数据量下均值和方差的增量计算","date":"2020-05-13T22:25:28.000Z","updated":"2022-01-04T08:35:48.539Z","content":"本文介绍如何在大数据量下增量计算均值和方差。\n\n\n假设我们有两组数据，一组历史数据：\nh_1, h_2, \\dots, h_M\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n，一组增量数据：\na_1, a_2, \\dots, a_N\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n。\n历史数据均值：\n\n\\bar{H} = \\frac{1}{M} \\sum_{i=1}^M h_i\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n历史数据方差：\n\n\\sigma_H^2 = \\frac{1}{M} \\sum_{i=1}^M (h_i -\\bar{H})^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n增量数据均值：\n\n\\bar{A} = \\frac{1}{N} \\sum_{j=1}^N a_j\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n\n增量数据方差：\n\n\\sigma_A^2 = \\frac{1}{N} \\sum_{j=1}^N (a_j -\\bar{A})^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n我们希望求得历史数据和增量数据的总均值 \n\\bar{X}\n\n\n\n\n\n \n \n\n 和方差 \n\\sigma^2\n\n\n\n\n\n \n \n\n。\n\n\\bar{X} = \\frac{1}{M+N} \\left( \\sum_{i=1}^M h_i + \\sum_{j=1}^N a_j \\right) = \\frac{M \\bar{H} + N \\bar{A}}{M + N}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n \n\n\n\n\n \n\n \n \n\n \n \n\n \n \n\n\n\n \n \n \n\n\n\n\n\n\n\\begin{align}\n\\sigma^2 &amp;= \\frac{1}{M+N} \\left[ \\sum_{i=1}^M (h_i -\\bar{X})^2 + \\sum_{j=1}^N (a_j - \\bar{X})^2 \\right] \\\\\n&amp;= \\frac{1}{M+N} \\left[ \\sum_{i=1}^M \\left( (h_i - \\bar{H}) -(\\bar{X} - \\bar{H}) \\right)^2 + \\sum_{j=1}^N \\left( (a_j - \\bar{A}) - (\\bar{X} -\\bar{A}) \\right)^2 \\right] \\\\\n&amp;= \\frac{1}{M+N} \\left[ \\sum_{i=1}^M \\left( (h_i - \\bar{H})^2 -2(h_i - \\bar{H})(\\bar{X} - \\bar{H}) + (\\bar{X} - \\bar{H})^2 \\right) \\\\ + \\sum_{j=1}^N \\left( (a_j - \\bar{A})^2 - 2(a_j - \\bar{A})(\\bar{X} -\\bar{A})  + (\\bar{X} -\\bar{A})^2 \\right) \\right] \\\\\n&amp;= \\frac{1}{M+N} \\left[ M \\sigma_H^2 + M(\\bar{X}-\\bar{H})^2-2(\\bar{X} - \\bar{H})(\\sum_{i=1}^M h_i - M \\bar{H}) \\\\ + N \\sigma_A^2 + N(\\bar{X} - \\bar{A})^2 - 2(\\bar{X} - \\bar{A})(\\sum_{j=1}^N a_j - N \\bar{A}) \\right] \\\\\n&amp;= \\frac{1}{M+N} \\left[ M \\sigma_H^2 + M(\\bar{X} - \\bar{H})^2 + N \\sigma_A^2 +N(\\bar{X} - \\bar{A})^2 \\right] \\\\\n&amp;= \\frac{M \\left[ \\sigma_H^2 + (\\bar{X} - \\bar{H})^2 \\right]  + N \\left[ \\sigma_A^2 + (\\bar{X} - \\bar{A})^2 \\right]}{M+N}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n\n \n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n\n\n \n\n\n\n\n \n\n \n\n \n \n \n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n\n\n \n \n \n\n\n\n\n\n\n\n\n","plink":"https://yuxinzhao.net/big-data-mean-std/"},{"title":"计算机视觉-几何变换","date":"2020-04-08T13:07:17.000Z","updated":"2022-01-04T08:35:48.563Z","content":"本文介绍几何变换(Geometric Transformation)的相关知识，包括：\n\n齐次坐标\n线性变换\n欧几里得变换\n仿射变换\n射影变换\n\n\n坐标系统\n\n\n\n齐次坐标 Homogeneous Coordinate在介绍几何变换的知识之前，有必要先对坐标进行定义。通常情况下，对于一个点 \nP\n\n\n\n\n \n\n ，假设它是一个一维的点，我们通常会用坐标 \nx\n\n\n\n\n \n\n 来区分它，而对于二维的点，我们会用 \n(x, y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 来说明这个点的位置。但是在计算机视觉中，常用齐次坐标来表示坐标的位置，这种表示方法对于 \nn\n\n\n\n\n \n\n 维的点需要 \nn+1\n\n\n\n\n\n\n \n \n \n\n 维来表示，通常是在坐标后加上一个1，如对于二维坐标 \n(3,4)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，用齐次坐标表示为 \n(3, 4, 1)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n。\n为什么要这么表示？我们可以类比一下我们的相机，相机拍出来的画面是 \n2D\n\n\n\n\n\n \n \n\n 的，但是它拍摄的东西却是 \n3D\n\n\n\n\n\n \n \n\n 的，拍摄到的 \n2D\n\n\n\n\n\n \n \n\n 画面中的每一个点都是从物体上某一点射到照片对应像素上的一条光线。齐次坐标与此类似，但特别的是光线的起点都在坐标原点。\n\n对于一个一条射线 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，当它打在 \ny=1\n\n\n\n\n\n\n \n \n \n\n 这一超平面上，得到的点的坐标是 \n(\\frac{x}{y}, 1)\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n \n \n \n\n，这就是一维点 \n\\frac{x}{y}\n\n\n\n\n\n\n\n \n \n\n\n 的齐次坐标表示。类似地，射线 \n(x, y, z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 打在 \nz=1\n\n\n\n\n\n\n \n \n \n\n 这一超平面上得到的点是 \n(\\frac{x}{z}, \\frac{y}{z}, 1)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n \n\n\n\n \n \n\n\n \n \n \n\n，这就是二维点 \n(\\frac{x}{z}, \\frac{y}{z})\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n \n\n\n\n \n \n\n\n \n\n 的齐次坐标。值得注意的是，齐次坐标并没有约束必须在 \nz=1\n\n\n\n\n\n\n \n \n \n\n 这一超平面上表示，因此，\n(x,y,z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 也可以说是二维点 \n(\\frac{x}{z}, \\frac{y}{z})\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n\n \n\n\n\n \n \n\n\n \n\n 的齐次坐标，因为它们描述的是同一条射线，是等价的。\n特别地，对于形如 \n(x,y,0)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的齐次坐标，即超平面为 0，我们用它来表示 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 这一个方向的向量。\n这一表示方式对于几何变换的运算更方便，之后的讨论中，所有点的坐标将以齐次坐标表示。\n对于一个点 \nP = (x,y,z,1)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n 或一个向量 \nu = (x, y, z, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n，，常写成列向量的形式:\n\nP = \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} \\qquad u = \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 0 \\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n线性变换 Linear Transformation对于满足以下方程的变换 \n\\mathcal{L}\n\n\n\n\n \n\n 都称为线性变换：\n\n\\mathcal{L} (a P + b Q) = a \\mathcal{L} (P) + b \\mathcal{L} (Q)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中, \na\n\n\n\n\n \n\n, \nb\n\n\n\n\n \n\n 是标量，\nP\n\n\n\n\n \n\n, \nQ\n\n\n\n\n \n\n 是点。\n线性变换不会改变曲线的度(degree of curve)。\n线性变换可以被表示为一个 \n(n + 1) \\times (n + 1)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n 维的矩阵，通过左乘点的坐标即可对点进行线性变换。\n常见的线性变换有三种:\n\n欧几里得变换(Euclidean transformation)：保持长度和角度不变。又称刚体变换(rigid body transformation)。只能平移和旋转。\n仿射变换(affine transformation)：保持长度和角度的比例不变。除了平移和旋转，还能缩放、剪切。\n射影变换(projective transformation)：能够平行线相交，即透视变换。\n\n其中，欧几里得变换是仿射变换的子集，仿射变换又是射影变换的子集。\n欧几里得变换和仿射变换平移 Translation\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} x + t_x \\\\ y + t_y \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} 1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} \n= \\mathcal{T}(t_x, t_y) P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n \n \n \n\n\n \n \n \n\n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n\n\n\n \n\n \n\n \n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n平移矩阵用 \n\\mathcal{T}\n\n\n\n\n \n\n 表示。\n\n\\mathcal{T}^{-1}(t_x, t_y) = \\mathcal{T}(- t_x, -t_y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n\n对于三维的平移:\n\n\\mathcal{T}(t_x, t_y, t_z)\n= \\begin{pmatrix}\n1 &amp; 0 &amp; 0 &amp; t_x \\\\\n0 &amp; 1 &amp; 0 &amp; t_y \\\\\n0 &amp; 0 &amp; 1 &amp; t_z \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n\n\n旋转 Rotation\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} x \\cos(\\theta) - y \\sin(\\theta) \\\\ x \\sin(\\theta) + y \\cos(\\theta) \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} \n\\cos(\\theta) &amp; -\\sin(\\theta) &amp; 0 \\\\ \n\\sin(\\theta) &amp; \\cos(\\theta) &amp; 0 \\\\ \n0 &amp; 0 &amp; 1 \n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} \n= \\mathcal{R}(\\theta) P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n \n\n\n\n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n\n \n\n\n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n\n\n\n \n\n \n\n \n\n\n \n \n \n \n \n \n\n\n旋转矩阵用 \n\\mathcal{R}\n\n\n\n\n \n\n 表示。\n\n\\mathcal{R}^{-1}(\\theta) = \\mathcal{R}(-\\theta) = \\mathcal{R}^T(\\theta)\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n对于三维中绕某一坐标轴的旋转：\n\n\\mathcal{R}_x(\\theta)\n= \\begin{pmatrix}\n1 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; \\cos(\\theta) &amp; -\\sin(\\theta) &amp; 0 \\\\ \n0 &amp; \\sin(\\theta) &amp; \\cos(\\theta) &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n \n\n\n \n\n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n\n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n\n\\mathcal{R}_y(\\theta)\n= \\begin{pmatrix}\n\\cos(\\theta) &amp; 0 &amp; -\\sin(\\theta) &amp; 0 \\\\ \n0 &amp; 1 &amp; 0 &amp; 0 \\\\\n\\sin(\\theta) &amp; 0 &amp; \\cos(\\theta) &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n\n\n \n \n \n \n\n\n\n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n \n \n \n\n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n\n\\mathcal{R}_z(\\theta)\n= \\begin{pmatrix}\n\\cos(\\theta) &amp; -\\sin(\\theta) &amp; 0 &amp; 0 \\\\ \n\\sin(\\theta) &amp; \\cos(\\theta) &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n \n \n\n\n\n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n\n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n缩放 Scaling\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} s_x x \\\\ s_y y \\\\ s_z z \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix}\ns_x &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; s_y &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; s_z &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} \n= \\mathcal{S}(s_x, s_y, s_z) P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n \n \n \n\n\n \n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n缩放矩阵用 \n\\mathcal{S}\n\n\n\n\n \n\n 表示。\n\n\\mathcal{S}(s_x, s_y, s_z)^{-1} = \\mathcal{S}(\\frac{1}{s_x}, \\frac{1}{s_y}, \\frac{1}{s_z})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n \n\n\n\n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n\n剪切 Shear剪切的概念是一个点的某一坐标轴保持不变，其他坐标轴与这一坐标轴呈正比变化。\n\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} x + ay \\\\ y \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} 1 &amp; a &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix} \n= \\mathcal{H}_y(a) P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n \n \n \n\n\n \n \n \n\n\n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n\n\n\n \n\n \n\n \n\n\n \n\n \n \n\n \n \n \n \n\n\n\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix} x +az \\\\ y+bz \\\\ z \\\\ 1 \\end{pmatrix} \n= \\begin{pmatrix}\n1 &amp; 0 &amp; a &amp; 0 \\\\\n0 &amp; 1 &amp; b &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} \n= \\mathcal{H}_z(a,b) P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n\n\n \n \n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n \n\n \n \n\n \n \n \n \n \n \n\n\n剪切矩阵用 \n\\mathcal{H}\n\n\n\n\n \n\n 表示。\n\n\\mathcal{H}_z (a,b)^{-1} = \\mathcal{H}_z (-a,-b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n \n\n\n射影变换射影变换将点 \nP = (x, y, z, w)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n 变换到 \nP' = (x', y', z', w')\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n 的变换 \n\\mathcal{P}\n\n\n\n\n \n\n 可以表示为:\n\nP' \n= \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ w' \\end{pmatrix} \n= \\begin{pmatrix}\np_{11} &amp; p_{12} &amp; p_{13} &amp; p_{14} \\\\\np_{21} &amp; p_{22} &amp; p_{23} &amp; p_{24} \\\\\np_{31} &amp; p_{32} &amp; p_{33} &amp; p_{34} \\\\\np_{41} &amp; p_{42} &amp; p_{43} &amp; p_{44} \n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ w \\end{pmatrix} \n= \\mathcal{P} P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n \n \n \n\n\n射影变换中原本确定位置的点可以移到无穷远，平行的线可以变得不平行，但曲线的度不会变，这意味着，直线射影变换后还是直线，度还是1，圆变换后可能是椭圆等，但度还是2，不能变成度为3的曲线。\n变换的叠加 Concatenation of Transformation\nP' = \\mathcal{T} P \\\\\nP'' = \\mathcal{R} P' = \\mathcal{R} \\mathcal{T} P\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n\n\n\n由此可见，变换的叠加可以由变换矩阵左乘得到。\n变换过程是可逆的:\n\nP = \\mathcal{T}^{-1} \\mathcal{R}^{-1} P''\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n复杂变换对于一个物体复杂的变换的流程如下：\n\n将坐标通过一些简单变换的叠加 \n\\mathcal{F}\n\n\n\n\n \n\n 变换到方便操作的空间中。\n运用一个基本的仿射变换 \n\\mathcal{B}\n\n\n\n\n \n\n。\n通过 \n\\mathcal{F}^{-1}\n\n\n\n\n\n\n \n\n \n \n\n\n 变换回原空间。\n\n以上几步可以叠加成一个操作: \n\\mathcal{F}^{-1} \\mathcal{B} \\mathcal{F}\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n\n。\n例：假设我们想将一个中心点在 \n(x,y,z)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n ，方向为 \n(a,b,c)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 的圆柱体以其中心为锚点将圆柱体的高拉伸为s倍。其过程如下：\n\n通过简单变换的叠加将圆柱体的坐标轴移到原点，且方向沿向 \nz\n\n\n\n\n \n\n 轴方向。\n\n\n\\mathcal{F} = \\mathcal{R_y(\\beta)} \\mathcal{R_x}(\\alpha) \\mathcal{T}(-x, -y, -z)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n要对物体做到基本变换: \n\n\\mathcal{B} = \\mathcal{S}(1, 1, s)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n将物体变换回原空间。\n\n\\mathcal{F}^{-1} =\\mathcal{T}(x, y, z) \\mathcal{R_x}(-\\alpha) \\mathcal{R_y(-\\beta)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n \n \n \n \n\n\n\n\n\n总的变换可以写成:\n\n\\mathcal{F}^{-1} \\mathcal{B} \\mathcal{F} = \\mathcal{T}(x, y, z) \\mathcal{R_x}(-\\alpha) \\mathcal{R_y(-\\beta)} \\mathcal{S}(1, 1, s) \\mathcal{R_y(\\beta)} \\mathcal{R_x}(\\alpha) \\mathcal{T}(-x, -y, -z)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n坐标系统 Coordinate System一个 \nn\n\n\n\n\n \n\n 维的坐标系统由原点 \nR\n\n\n\n\n \n\n 和坐标轴各方向的单位向量 \nu_1, u_2, \\dots, u_n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n 组成。\n在一个3维坐标系统中，记单位向量 \nu_i\n\n\n\n\n\n \n \n\n 的齐次坐标为 \n(u_{ix}, u_{iy}, u_{iz}, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n，原点为 \n(R_x, R_y, R_z, 1)\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n。\n点 \nP\n\n\n\n\n \n\n 的坐标可以表示为:\n\nP = \\begin{pmatrix} u_1 &amp; u_2 &amp; u_3 &amp; R \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix}\nu_{1x} &amp; u_{2x} &amp; u_{3x} &amp; R_x \\\\\nu_{1y} &amp; u_{2y} &amp; u_{3y} &amp; R_y \\\\\nu_{1z} &amp; u_{2z} &amp; u_{3z} &amp; R_z \\\\\n0 &amp; 0 &amp; 0 &amp; 1\n\\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ 1 \\end{pmatrix}\n= M_u C_u\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n\n\n \n \n\n\n\n\n \n \n\n\n\n\n \n \n\n\n \n\n \n\n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n\n \n\n\n \n\n \n \n\n\n \n \n\n\n\n其中，\nM_u\n\n\n\n\n\n \n \n\n 定义了坐标系统，\nC_u\n\n\n\n\n\n \n \n\n 是点 \nP\n\n\n\n\n \n\n 在坐标系统 \nM_u\n\n\n\n\n\n \n \n\n 中的坐标。\n坐标系统的变换假设有两个坐标系统 \nM_v\n\n\n\n\n\n \n \n\n 和 \nM_u\n\n\n\n\n\n \n \n\n，那么点 \nP\n\n\n\n\n \n\n 在这两个坐标系统下的表示为:\n\nP = M_v C_v = M_u C_u\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n由此可得坐标系统间转换的方法为:\n\nC_v = M_v^{-1} M_u C_u\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n\n \n\n\n \n \n\n\n \n \n\n\n\n全局坐标系统与局部坐标系统全局坐标系统和局部系统对应于转换矩阵与点坐标的左乘和右乘。\n\n左乘：全局坐标系统，坐标系统不变，变换点的坐标。\n右乘：局部坐标系统，点的坐标不变，变换坐标系统。\n\n自由度 Degrees of Freedom自由度定义为一个变换中可以改变的参数个数。自由度 \n\\le\n\n\n\n\n \n\n 变换矩阵中能改变的元素值。\n对于二维的变换：\n\n欧几里得变换： \n\\mathcal{T}(t_x, t_y)\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n 和 \n\\mathcal{R}(\\theta)\n\n\n\n\n\n\n\n \n \n \n \n\n，能够改变的参数为 \nt_x\n\n\n\n\n\n \n \n\n, \nt_y\n\n\n\n\n\n \n \n\n, \n\\theta\n\n\n\n\n \n\n，因此自由度为3。\n仿射变换：\n\\mathcal{T}(t_x, t_y)\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n, \n\\mathcal{R}(\\theta)\n\n\n\n\n\n\n\n \n \n \n \n\n, \n\\mathcal{S}(s_x, s_y)\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n, \nH_x(a)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n, \n\\mathcal{H}_y(b)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n ，加起来总共7个参数。但是自由度应该小于等于变换矩阵中能改变的元素值，即应该小于等于6。进一步的探究可以发现旋转可以由缩放和剪切组成，因此旋转是不必要的。自由度为6。\n射影变换：自由度为8。\n\n参考资料\nMajumder A, Gopi M. Introduction to Visual Computing: Core Concepts in Computer Vision, Graphics, and Image Processing[M]. CRC Press, 2018.\n\n","thumbnail":"computer-vision-geometric-transformation/geometric.png","plink":"https://yuxinzhao.net/computer-vision-geometric-transformation/"},{"title":"OpenMP基础","date":"2020-04-08T10:04:28.000Z","updated":"2022-01-04T08:35:48.611Z","content":"本文介绍 OpenMP C++版本的基本使用方法。\n\n\n安装无需安装，gcc自带。只需在编译时启动 -fopenmp 选项。\n第一个OpenMP程序12345678910#include &lt;iostream&gt;int main()&#123;    #pragma omp parallel    &#123;        std::cout &lt;&lt; \"Hello world!\\n\";    &#125;    return 0;&#125;\n\n在使用g++编译时应使用 -fopenmp 选项。\n123456$ g++ test1.cpp -fopenmp$ ./a.outHello world!Hello world!Hello world!Hello world!\n\n程序输出了4个 Hello world!， 这是因为 #pragma omp parallel 会根据硬件和操作系统配置在运行时生成代码，创建尽可能多的线程，每个线程的起始例程为代码块中位于指令之后的代码。\n控制线程数量使用编译命令的 num_threads 参数控制线程数量12345678910#include &lt;iostream&gt;int main()&#123;    #pragma omp parallel num_threads(6)    &#123;        std::cout &lt;&lt; \"Hello World!\\n\";    &#125;    return 0;&#125;\n\n上述程序控制用6个线程执行代码块，因此输出了6个Hello World!。\n使用 omp_set_num_threads 控制线程数量使用 OpenMP 的 API 需要包含头文件 omp.h 。\n123456789101112#include &lt;omp.h&gt;#include &lt;iostream&gt;int main()&#123;    omp_set_num_threads(6);    #pragma omp parallel    &#123;        std::cout &lt;&lt; \"Hello World!\\n\";    &#125;    return 0;&#125;\n\n通过环境变量控制线程数量12345678910#include &lt;iostream&gt;int main()&#123;    #pragma omp parallel    &#123;        std::cout &lt;&lt; \"Hello world!\\n\";    &#125;    return 0;&#125;\n\n在外部环境中通过环境变量 OMP_NUM_THREADS 控制线程数量。仅当没有在程序中显式设置线程数量时，该环境变量才会生效。\n1234$ export OMP_NUM_THREADS=2$ ./a.outHello world!Hello world!\n\n并行化 for 循环12345678910111213141516171819202122#include &lt;omp.h&gt;#include &lt;math.h&gt;#include &lt;time.h&gt;#include &lt;iostream&gt;int main() &#123;    clock_t clock_timer;    double wall_timer;    double c[100000];    for (int nthreads = 1; nthreads &lt;= 8; nthreads++) &#123;        clock_timer = clock();        wall_timer = omp_get_wtime();        int i;        #pragma omp parallel for private(i) num_threads(nthreads)        for (i = 0; i &lt; 100000; i++)            c[i] = sqrt(i * 4 + i * 2 + i);                std::cout &lt;&lt; \"threads: \" &lt;&lt; nthreads &lt;&lt; \" time on clock(): \" &lt;&lt; (double)(clock() - clock_timer) / CLOCKS_PER_SEC &lt;&lt; \" time on wall: \" &lt;&lt; omp_get_wtime() - wall_timer &lt;&lt; std::endl;    &#125;    return 0;&#125;\n\n\nomp_get_wtime 任选一个点返回已用去的时间，单位为秒。\nparallel for private(i) 意味着循环变量 i 将作为一个线程进行处理，每个线程有一个该变量的副本，且未进行初始化。\n\n12345678threads: 1 time on clock(): 0.002979 time on wall: 0.00297725threads: 2 time on clock(): 0.000751 time on wall: 0.000606505threads: 3 time on clock(): 0.000426 time on wall: 0.000428914threads: 4 time on clock(): 0.006254 time on wall: 0.00260468threads: 5 time on clock(): 0.002958 time on wall: 0.00133061threads: 6 time on clock(): 0.001262 time on wall: 0.000695385threads: 7 time on clock(): 0.001098 time on wall: 0.000518768threads: 8 time on clock(): 0.001211 time on wall: 0.000538669\n\n当不加 private(i) ，即使用直接使用 parallel for 时，将不再按照一个变量一个线程来并行，而是按照平均分配循环次数。如线程数为3，循环范围为0-8时，线程0负责0-2的范围，线程1负责3-5，线程2负责6-8。\n可以通过 schedule(type, chunksize) 来调整循环的分配方式：\n\ntype 为调度方式，有以下5种\nstatic\n在循环开始之前就先设置好迭代的轮数。\n将循环以 chunksize 为大小，分成 total_iterations / chunksize 份chunk，每个线程按顺序一份份执行。\n当省略 chunksize 时，chunksize = totoal_iterations / thread_count。\n\n\ndynamic\n在循环执行过程中，动态决定哪个线程执行哪个chunk。\nchunksize 的设置同 static, 但是在运行时哪个线程空闲chunk就会被分配给哪个线程。\n当省略 chunksize 时， chunksize = 1。\n\n\nguided\n在循环执行过程中，动态决定哪个线程执行哪个chunk。\nchunk的大小不固定为chunksize, 而是在运行过程中递减，直至为每个chunk为 chunksize 大小。\n当省略 chunksize 时， chunksize = 1。\n\n\nauto\n运行时根据系统自动选择合适的调度方式。\n\n\nruntime\n运行时通过环境变量 OMP_SCHEDULE 决定使用哪种调度方式，如 export OMP_SCHEDULE=&quot;static, 1&quot;。\n\n\n\n\n\n123#pragma omp parallel for num_threads(thread_count) schedule(static, 2)for (i = 0; i &lt; n; i++)    result[i] = f(i);\n\n规约操作(reduction operators)规约操作符是一个二元操作符，比如 + 或 *。规约操作指讲一系列操作数用同一个规约操作符重复地进行计算，中间结果都存储到同一变量(规约变量)的过程。\n语法：\n1reduction(&lt;operator&gt;: &lt;variable list&gt;)\n\n其中的操作符可以是 +, *, -, &amp;, |, ^, &amp;&amp;, ||。\n1234global_result = 0.0;#pragma omp parallel for num_threads(4) reduction(+: global_result)for (i = 1; i &lt; n; i++)    global_result += i\n\n临界区(critical section)编译指示OpenMP中处理一些互斥的区域时，需要显式指定临界区。\n语法:\n1234#pragma omp critical (optional section name)&#123;    // no 2 threads can execute this code blcok concurrently&#125;\n\n代码块中的代码同一时间只会有一个线程会执行。\noptional section name 是一个全局标识符，即临界区的名字，同一时刻不能同时运行两个相同名字的临界区。\n123456789#pragma omp critical (section1)&#123;    myhashtable.insert(\"key1\", \"value1\");&#125;#pragma omp critical (section2)&#123;    myhashtable.insert(\"key2\", \"value2\");&#125;\n\n互斥锁除了编译指示，OpenMP还提供了互斥锁 omp_lock_t。主要为以下5个API：\n\nomp_init_lock：必须时第一个访问omp_lock_t的API，使用它来初始化锁。初始化后锁处于未设置状态。\nomp_destroy_lock：销毁锁，调用时锁必须处于未设置状态。\nomp_set_lock：设置锁，获得互斥。如果一个线程无法设置锁，将会阻塞直到成功设置锁。 \nomp_test_lock：尝试设置锁，不会阻塞，成功返回1，失败返回0。\nomp_unset_lock：释放锁。\n\n例: 构建一个线程安全的队列。\n12345678910111213141516171819202122232425262728293031323334#include &lt;omp.h&gt;template&lt;typename T&gt;class ThreadSafeQueue : public ThreadUnsafeQueue&lt;T&gt; &#123;public:    ThreadSafeQueue(maxsize = 10) : ThreadUnsafeQueue(maxsize) &#123;        omp_init_lock(&amp;lock);    &#125;    ~ThreadSafeQueue() &#123;        omp_destroy_lock(&amp;lock);    &#125;    bool push(const T&amp; data) &#123;        omp_set_lock(&amp;lock);        bool result = ThreadUnsafeQueue&lt;T&gt;::push(data);        omp_unset_lock(&amp;lock);        return result;    &#125;    bool trypush(const T&amp; data) &#123;        bool result = omp_test_lock(&amp;lock);        if (result) &#123;            result = ThreadUnsafeQueue&lt;T&gt;::push(data);            omp_unset_lock(&amp;lock);        &#125;        return result;    &#125;    // likewise for pop    private:    omp_lock_t lock;&#125;;\n\n嵌套锁OpenMP提供了 omp_nest_lock_t 锁。它与 omp_lock_t 类似，但有一个优点: 当已经持有锁的线程通过 omp_set_nest_lock 重新获得锁时，内部计数器会加一，当多个 omp_unset_nest_lock 使计数器置为0时，才会释放锁。\nAPI与omp_lock_t 类似:\n\nomp_init_nest_lock：必须时第一个访问omp_lock_t的API，使用它来初始化锁。初始化后锁处于未设置状态，计数器为0。\nomp_destroy_nest_lock：销毁锁，销毁计数器不为0的锁将产生未定义的行为。\nomp_set_nest_lock：设置锁，获得互斥。若线程已持有该锁，则计数器+1，仍然能获得锁，若不持有该锁，将会阻塞直到成功设置锁。 \nomp_test_nest_lock：尝试设置锁，不会阻塞，成功返回1，失败返回0。\nomp_unset_nest_lock：计数器-1，当计数器为0时释放锁。\n\n控制任务执行的细粒度通过 sections 可以将没有相关性的程序用 #pragma omp section 分割区块，各区块分得一个线程并行运行。\n123456789101112131415161718192021222324252627282930#include &lt;omp.h&gt;#include &lt;iostream&gt;int main()&#123;    #pragma omp parallel    &#123;        std::cout &lt;&lt; \"All threads run this\\n\";                #pragma omp sections        &#123;            #pragma omp section            &#123;                std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[A] This executes in parallel\\n\";            &#125;            #pragma omp section            &#123;                std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[B] This executes in parallel\\n\";                std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[C] This executes after B\\n\";            &#125;            #pragma omp section            &#123;                std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[D] This also executes in parallel\\n\";            &#125;        &#125;    &#125;    return 0;&#125;\n\n\n每个section独立运行，互不干扰。\nOpenMP会使用当前空闲的线程运行 section，如果不加 #pragma omp parallel，空闲的线程就一直只有主线程一个，所有的section还是会在单线程上串行运行。\n\n12345678All threads run this0[A] This executes in parallel0[B] This executes in parallelAll threads run this1[D] This also executes in parallel0[C] This executes after BAll threads run thisAll threads run this\n\n也可以将 parallel 和 sections 合并:\n123456789101112131415161718192021222324252627#include &lt;omp.h&gt;#include &lt;iostream&gt;int main()&#123;        #pragma omp parallel sections    &#123;        #pragma omp section        &#123;            std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[A] This executes in parallel\\n\";        &#125;        #pragma omp section        &#123;            std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[B] This executes in parallel\\n\";            std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[C] This executes after B\\n\";        &#125;        #pragma omp section        &#123;            std::cout &lt;&lt; omp_get_thread_num() &lt;&lt; \"[D] This also executes in parallel\\n\";        &#125;    &#125;        return 0;&#125;\n\n12340[A] This executes in parallel0[D] This also executes in parallel1[B] This executes in parallel1[C] This executes after B\n\n参考资料\n通过GCC学习OpenMP框架 | IBM\nOpenMP 文档\nOpenMP Reference Sheet for C/C++ | W3Cschool\n\n","thumbnail":"openmp-basic/openmp.png","plink":"https://yuxinzhao.net/openmp-basic/"},{"title":"[论文笔记] Focal Loss for Region Proposal Network","date":"2020-04-04T15:37:13.000Z","updated":"2022-01-04T08:35:48.591Z","content":"\n类别不均衡的问题不止存在于one-stage检测器中，two-stage检测器面临着同样的问题，鉴于focal loss在one-stage上的成功应用，作者将focal loss应用于RPN，检验 focal loss 在two-stage检测器中的作用。\n\n\n标准RPN标准的RPN使用一个卷积层卷积特征，并在之后接上两个卷积层分别用于回归box和判断该位置的anchor是否存在目标。\n对于用于判断该位置的anchor是否存在目标的分类网络，即需要判别前景和背景，其损失函数采用交叉熵损失：\n\n\\begin{align}\nCE(p,y) &amp;= \\frac{1}{N_{cls}} \\sum_i CE(p_i, y_i) \\\\\n&amp;= \\frac{1}{N_{cls}} \\sum_i y_i \\log(p_i) + (1-y_i) \\log (1-p_i)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n\n \n\n\n\n\n\n对于生成box的回归网络，采用平滑 \nL_1\n\n\n\n\n\n \n \n\n 损失:\n\nsmooth_{L_1}(x) = \n\\begin{cases}\n0.5 x^2  &amp; \\text{if} \\space \\space |x| \\le 1 \\\\\n|x| - 0.5 &amp; \\text{otherwise}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n\n\n \n \n \n \n\n \n\n\n\n \n \n \n\n \n \n\n\n\n \n \n \n \n\n \n \n \n\n\n\n\n\n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n其中 \nx\n\n\n\n\n \n\n 是候选框与grouth-truth bounding box的差。\n总的损失函数为:\n\nL_{RPN} = \\frac{1}{N_{cls}} \\sum_i CE(p_i, p_i^t) + \\frac{1}{N_{reg}} \\sum_i I(t_i^t) L_{reg} (t_i, t_i^t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n\n其中, \nN_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 和 \nN_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 是归一化系数， \nI(t_i^t)\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n\n 是指示函数，只计算anchor分配到grouth-truth上的box的回归损失。\n标准RPN同样面临着背景比前景多很多而导致的样本不均衡问题，但RPN采用了采样的方法，从前景和背景中按1:1比例采样正样本和负样本，使用少量平衡的样本计算损失，以此克服样本不平衡的问题，但这也导致了部分前景/背景没有计算损失而得不到优化的情况。\nFocal Loss 改造的 RPN使用Focal Loss就不再需要RPN进行正负样本的平衡采样，能够对所有anchor计算损失。因此，首先去除RPN的采样。\n损失函数如下：\n\nL_{RPN-FL} = \\frac{\\lambda_{fl}}{N_{cls}'} \\sum_i FL(p_i^t) + \\frac{1}{N_{reg}'} \\sum_i I(t_i^t) L_{reg} (t_i, t_i^t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n\n \n\n\n\n\n \n\n \n \n\n\n\n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n\n\n \n\n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n\n其中，\n\\lambda_{fl}\n\n\n\n\n\n\n \n\n \n \n\n\n 用于平衡损失，作者取 \n\\lambda_{fl} = 0.1\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n \n\n\n。\nN_{cls}' = |p_i^t \\in object|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n ，\nN_{reg}' = 2 *|p_i^t \\in object|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n。Focal Loss中的超参数取 \n\\alpha=0.25\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n，\n\\gamma=2\n\n\n\n\n\n\n \n \n \n\n。\n对于 faster RCNN 框架，总损失如下：\n\nL = L_{RPN-FL} + L_{RCNN}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n\n\n\n\n实验结果\n\n鉴于RPN中分类网络可能采用sigmoid或者softmax进行前景与背景的二分类，作者实验了两种形式下focal loss的表现，实验结果表明focal loss能稍微提升检测的准确率，且softmax形式要优于sigmoid形式。\n作者还发现应用focal loss后RPN的分类损失下降平稳，但回归损失不稳定，容易巨幅变化，作者认为这是因为回归网络和分类网络共用了前面所有的特征层进行特征提取，两个网络的优化目标有冲突导致的，并认为RetinaNet中分类网络和回归网络独立拥有各自的一部分卷积层进行特征提取，解决了这个冲突，但作者并没有继续探究这个问题。\n参考文献\nChen C, Song X, Jiang S. Focal Loss for Region Proposal Network[C]//Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer, Cham, 2018: 368-380.\n\n","plink":"https://yuxinzhao.net/focal-loss-for-region-proposal-network/"},{"title":"[论文笔记] Focal Loss for Dense Object Detection","date":"2020-04-03T22:36:07.000Z","updated":"2022-01-04T08:35:48.587Z","content":"作者提出了 Focal Loss 损失函数，在交叉熵损失的基础上加入因子 \n(1 - p_t)^\\gamma\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n\n\n，用于处理one-stage检测器样本不均衡的问题。同时，作者提出了 RetinaNet one-stage检测器。\n\n\n\n\n问题目前的检测器主要分为one-stage检测器和two-stage检测器，two-stage检测器的准确率高但速度慢，one-stage检测器的速度快但准确率略低，作者发问是否能让one-stage检测器达到与two-stage检测器相似的准确率。\n\n\n\n检测器\n准确率\n速度\n\n\n\none-stage\n高\n慢\n\n\ntwo-stage\n低\n快\n\n\n作者发现导致one-stage检测器的原因是类别不均衡(class imbalance)。one-stage检测器会生成大量的候选目标位置，需要检测器判别这些候选目标位置哪些是前景、哪些是背景，但其中只有一小部分是包含目标的前景，这y种类别不均衡导致了one-stage的准确率较低。\n损失函数交叉熵损失 Cross Entropy Loss原本one-stage的类别分类采用的CE(cross-entropy)交叉熵损失函数:\n\n\\text{CE}(p,y) = \n\\begin{cases}\n- \\log (p) &amp; \\text{if} \\space \\space y = 1 \\\\\n- \\log (1 - p) &amp; \\text{otherwise}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n\n\n \n\n \n \n \n\n \n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n其中，\ny \\in \\{-1, +1\\}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n，表示ground-truth类别，\np \\in [0, 1]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 表示模型估算其类别为 \ny=1\n\n\n\n\n\n\n \n \n \n\n 的概率。\n为了简化表示方法，记\n\np_t = \n\\begin{cases}\np &amp; \\text{if} \\space \\space y=1 \\\\\n1 - p &amp; \\text{otherwise}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n\n \n\n \n \n \n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n由此，可以简写交叉熵损失函数为\n\n\\text{CE}(p,y) = \\text{CE}(p_t) = - \\log (p_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n\n \n\n\n平衡交叉熵损失 Balanced Cross Entropy一个常用的处理类别不均衡的方法是引入一个权值 \n\\alpha \\in [0,1]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n，用来控制类别 \ny=1\n\n\n\n\n\n\n \n \n \n\n 的权重，而用 \n(1-\\alpha)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 控制类别 \ny=-1\n\n\n\n\n\n\n\n \n \n \n \n\n 的权重。\n同样的记：\n\n\\alpha_t = \n\\begin{cases}\n\\alpha &amp; \\text{if} \\space \\space y=1 \\\\\n1 - \\alpha &amp; \\text{otherwise}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n\n \n\n \n \n \n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n由此，\n\\alpha\n\n\n\n\n \n\n平衡交叉熵损失函数为:\n\n\\text{CE}(p_t) = - \\alpha_t \\log(p_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n \n\n\nFocal Loss\n\n\\alpha\n\n\n\n\n \n\n平衡交叉熵损失函数平衡了正负样本的重要程度，达到了正负样本间的平衡，但它并没有区分出易样本和难样本(easy/hard examples)。当一个样本被模型正确判别类型时，这个样本我们称之为易样本(easy example)，如果类型判别错误，就称为难样本(hard example)。\n作者提出 Focal Loss 通过降低易样本的权重，让模型作用于训练难样本。\nFocal Loss 定义为\n\n\\text{FL}(p_t) = - (1-p_t)^\\gamma \\log(p_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n \n \n\n\n \n \n \n\n \n\n \n \n\n \n\n\n其中，\n\\gamma \\ge 0\n\n\n\n\n\n\n \n \n \n\n 是一个可以人为调整的超参数。\n对于TP和TN这种被正确分类的样本，Focal Loss 损失值下降被巨幅下降，而对FP和FN这种错误分类的样本，Focal Loss 损失值只会轻微降低，由此达到了易样本与难样本的平衡。\n当 \n\\gamma=0\n\n\n\n\n\n\n \n \n \n\n 时，focal loss 退化成普通的交叉熵损失函数，\n\\gamma\n\n\n\n\n \n\n 值越大对易样本的损失降低就越大，作者找到的最佳值是 \n\\gamma = 2\n\n\n\n\n\n\n \n \n \n\n。\n平衡 Focal Loss普通的 Focal Loss 只是关注于难易样本的平衡问题，与正负样本的平衡并不冲突，因此也可以为 focal loss 加上 \n\\alpha\n\n\n\n\n \n\n 平衡。\n\n\\text{FL}(p_t) = - \\alpha_t (1-p_t)^\\gamma \\log(p_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n\n\n \n \n \n\n \n\n \n \n\n \n\n\n采用这种形式对于准确率有轻微的提升。\n\n\\gamma\n\n\n\n\n \n\n 和 \n\\alpha\n\n\n\n\n \n\n 两个参数相互影响，作者发现，当 \n\\alpha\n\n\n\n\n \n\n 取值越小时，应该选取更大的 \n\\gamma\n\n\n\n\n \n\n，以达到更好的效果。作者查到的最佳取值为 \n\\gamma=2\n\n\n\n\n\n\n \n \n \n\n, \n\\alpha=0.25\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n。\n正负样本平衡和难易样本平衡的叠加使得负样本中的易样本，即经常很容易被判别为背景的样本，对损失的贡献很小，正样本中的难样本是模型重点的优化对象，损失值最大。\n各类样本对损失的影响：\n\n\n\n\n易样本(正确分类)\n难样本(错误分类)\n\n\n\n正样本\n损失下降较多\n损失下降较少(对损失的贡献最大)\n\n\n负样本\n损失巨幅下降(对损失的贡献很小)\n损失下降较多\n\n\nRetinaNet\n\nbackbone\n\nRetinaNet 采用 ResNet50 + FPN 作为backone。\n取ResNet50的 p3 到 p7 层，即原输入图像的 \n\\frac{1}{2^3}\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n 到 \n\\frac{1}{2^7}\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n 大小的特征图作为FPN的输入。\n由于ResNet50最多到 p5 层，作者在没有像FPN论文中一样下采样 p5 层获得 p6 层，而是让 p5 层通过一个 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n 卷积得到 p6 层，再让 p6 层 relu 激活后 \n3\\times 3\n\n\n\n\n\n \n \n \n\n 卷积得到 p7 层。\n\n\nclass subnet\n\n分类网络预测每个位置对应的A个anchor和K个类别的概率。\n以FPN每一级得到的 \nC\n\n\n\n\n \n\n 通道数的特征图作为输入，首先通过四个 \n3\\times 3\n\n\n\n\n\n \n \n \n\n 的卷积，每个卷积后用 ReLU 激活，最后使用一个 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n、输出通道数为 \nKA\n\n\n\n\n\n \n \n\n 的卷积生成对应每个位置每个anchor各类别的概率。 \n\n\nbox subnet\n\nbox回归网络回归左上、右下两个点距anchor左上、右下两个点的偏差。\n以FPN每一级得到的 \nC\n\n\n\n\n \n\n 通道数的特征图作为输入，首先通过四个 \n3\\times 3\n\n\n\n\n\n \n \n \n\n 的卷积，每个卷积后用 ReLU 激活，最后使用一个 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n、输出通道数为 \n4A\n\n\n\n\n\n \n \n\n 的卷积生成对应每个位置每个anchor的顶点坐标。 \n\n\nfocal loss\n\n总的focal loss计算为所有anchor的focal loss的和，并用被分配给ground-truth box的anchor数量归一化(normalized by the number of anchors assigned to a ground-truth box)。\n最佳的超参数取值是 \n\\gamma=2\n\n\n\n\n\n\n \n \n \n\n，\n\\alpha=0.25\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n。\n\n\n总损失\n\n总的损失为总的focal loss加上box回归网络的平滑 \nL_1\n\n\n\n\n\n \n \n\n 损失(smooth \nL_1\n\n\n\n\n\n \n \n\n loss)。\n\n\n\n\n\n参考文献\nLin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988.\n\n","plink":"https://yuxinzhao.net/focal-loss-for-dense-object-detection/"},{"title":"[论文笔记] EfficientDet: Scalable and Efficient Object Detection","date":"2020-03-29T15:15:48.000Z","updated":"2022-01-04T08:35:48.567Z","content":"作者提出了 bi-directional feature pyramid network (BiFPN)，实现了更简单高效的多尺度特征融合(multi-scale feature fusion)；通过复合缩放方法，在EfficientNet的基础上提出了EfficientDet目标检测模型。\n\n\n构建one-stage检测器的两大挑战\n高效的多尺度特征融合\nFPN已广泛用于多尺度特征融合，但作者发现由于特征的输入来源于不同尺度的特征，它们融合后对输出的融合特征的贡献并不相等。因此作者提出了weighted BiFPN，通过可学习的权重学习不同输入特征的重要程度，并反复进行自顶向下和自底向上的多尺度特征融合。\n\n\n模型缩放\n在放大模型时应该同时放大特征提取网络和box/class预测网络，并权衡准确率和效率。\n\n\n\nBiFPN\n主要思想： 高效的双向跨尺度连接和加权特征融合。\n多尺度特征融合问题定义给定一系列多尺度特征 \n\\vec P^{in} = \\left( P_{l_1}^{in}, P_{l_2}^{in}, \\dots \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n\n \n\n \n \n\n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n\n \n \n \n\n\n，其中 \nP_{l_i}^{in}\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n\n\n 表示 \nl_i\n\n\n\n\n\n \n \n\n 层的特征。目标是找到一个转换 \nf\n\n\n\n\n \n\n 能够高效的聚合不同特征，并输出一系列新特征 \n\\vec P^{out} = f(\\vec P^{in})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n \n\n\n \n\n。\n假定输入的特征为 \n\\vec P^{in} = \\left( P_3^{in}, \\dots, P_7^{in} \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n\n\n ，一个经典的FPN的融合操作为:\n\n\\begin{align}\nP_7^{out} &amp;= Conv(P_7^{in}) \\\\\nP_6^{out} &amp;= Conv(P_6^{in} + Resize(P_7^{out})) \\\\\n\\dots \\\\\nP_3^{out} &amp;= Conv(P_3^{in} + Resize(P_4^{out}))\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n\n\n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\n\n\n其中 \nResize(\\cdot)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n 通常为上采样或下采样操作。\n跨尺度连接经典的自顶向下FPN受单向信息流的影响，还不能很好地融合特征。NAS-FPN和PANet对FPN做了改进。\n\nNAS-FPN\n通过neural architecture search搜索得到，使用了上千块GPU，代价高昂，且难以解释和进行修改。\n\n\nPANet (Pyramid attention network)\nPANet增加了一条自底向上的通路，更好地融合了特征，准确率也比FPN和NAS-FPN高。\n\n\n\n作者又对PANet做了以下改进得到了BiFPN：\n\n移除只有单个入边的节点。因为只有单个入边的节点对特征融合没有作用。\n为同级的输入结点和输出节点增加一个连接。\n将自顶向下和自底向上的通路当成一个BiFPN模块，堆叠多个BiFPN来提取特征。\n\n加权特征融合作者对BiFPN中的每条边都赋予了一个可学习的权重，用于学习不同尺度特征的重要程度。\n快速归一化融合:\n\nO = \\sum_i \\frac{w_i}{\\epsilon +  \\sum_j w_j} \\cdot I_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n\n\n\n其中，权重 \nw_i \\ge 0\n\n\n\n\n\n\n\n \n \n \n \n\n 可以通过 ReLU 实现。\n\\epsilon = 0.0001\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n \n \n\n\n 用于避免数值错误。\n以BiFPN中的 \nP_6\n\n\n\n\n\n \n \n\n 层为例：\n\n\\begin{align}\nP_6^{td} &amp;= Conv \\left( \\frac{w_1 \\cdot P_6^{in} + w_2 \\cdot Resize(P_7^{in})}{w_1 + w_2 + \\epsilon} \\right) \\\\\nP_6^{out} &amp;= Conv \\left( \\frac{w_1' \\cdot P_6^{in} + w_2' \\cdot P_6^{td} + w_3' \\cdot Resize(P_5^{out})}{w_1' + w_2' + w_3' + \\epsilon} \\right) \n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n \n\n \n\n\n\n\n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n\n\n \n \n \n\n \n \n\n \n \n\n\n\n \n\n\n\n \n \n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n\n\n\n \n\n\n\n\n\n\n其中，\nP_6^{td}\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n 是第6层自顶向下通路的中间特征，\nP_6^{out}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n 是第6层的自底向上通路的输出特征。\nEfficientDet\n\nbackbone network\n\n直接采用EfficientNet-B0 到 B6。\n\n\nBiFPN\n\n放大模型时，BiFPN的宽度由复合缩放方法决定，深度线性增长。\n\n\\begin{align}\n     W_{bifpn} &amp;= 64 \\cdot (1.35^\\phi) \\\\\n     D_{bifpn} &amp;= 3 + \\phi\n     \\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n\n\n\n\n\n \n\n \n \n\n \n \n\n \n \n \n \n \n\n \n\n\n \n \n \n \n\n\n\n\n\n\n\n\nbox/class prediction network\n\n放大模型时，宽度由 \nW_{bifpn}\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n\n 决定，深度线性增长。\n\n\\begin{align}\n     W_{box} &amp;= W_{class} = W_{bifpn} \\\\\n     D_{box} &amp;= D_{class} =3+ \\lfloor \\phi / 3 \\rfloor\n     \\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n\n \n\n \n \n \n\n\n\n\n\n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\nInput image resolution\n\n由于BiFPN特征的输入使用了 3-7 层的特征，输入分辨率应该保证可以被 \n2^7\n\n\n\n\n\n \n \n\n 整除。放大模型输入分辨率时也应该注意这一点。\n\nR_{input} = 512 + \\phi \\cdot128\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n\n\n\n\n\n","thumbnail":"efficientdet/bifpn.png","plink":"https://yuxinzhao.net/efficientdet/"},{"title":"[论文笔记] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks","date":"2020-03-29T13:37:10.000Z","updated":"2022-01-04T08:35:48.575Z","content":"作者提出了一个统一缩放CNN模型深度(depth)/宽度(width)/分辨率(resolution)三个维度的复合缩放方法(compound scaling method)，通过将小模型放大，提升模型的性能。在此基础上，作者又通过神经架构搜索(neural architecture search)设计了一组模型，EfficientNet，使用了更少的参数，达到了更快的速度，更高的准确率。\n\n\n统一缩放通过将CNN模型放大能够使模型达到更高的准确率，然而放大CNN的过程一直没有被系统地研究过，之前的工作中针对模型的深度、宽度、分辨率(图像大小)三个维度单独进行过单独的实验，但没有得出一个缩放模型的主要方法来提高模型的准确率和性能。作者发现平衡CNN模型的三个维度 深度/宽度/分辨率 对于获得更好的模型而不造成太大的速度损失很重要，并提出了一种简单有效的复合缩放方法，即使用一组固定的系数来缩放模型三个维度的缩放系数。\n问题定义一个卷积层 \ni\n\n\n\n\n \n\n 可以被定义为一个函数：\n\nY_i = \\mathcal{F}_i(X_i)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n其中，\n\\mathcal{F}_i\n\n\n\n\n\n \n \n\n 是操作，\nX_i\n\n\n\n\n\n \n \n\n 是输入的张量，大小为 \n\\langle H_i, W_i, C_i \\rangle\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n， \nY_i\n\n\n\n\n\n \n \n\n 是输出的张量。\n一个卷积神经网络 \n\\mathcal{N}\n\n\n\n\n \n\n 通常可以表示为一系列卷积层的复合函数：\n\n\\mathcal{N} = \\mathcal{F}_k \\odot \\dots \\odot \\mathcal{F}_2 \\odot \\mathcal{F}_1(X_1) = \\bigodot_{j=1 \\dots k} \\mathcal{F}_j (X_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n \n \n \n\n\n\n \n \n\n \n\n \n \n\n \n\n\n然而，一个卷积神经网络通常分成了很多个段(stage)，而在每一个段中，卷积层基本是相同的，比如ResNet就有着五个段。由此，可以进一步简化CNN的表示。\n\n\\mathcal{N} = \\bigodot_{i=1\\dots s} \\mathcal{F}^{L_i}(X_{\\langle H_i, W_i, C_i \\rangle})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n\n \n\n\n模型缩放(model scaling)要做的就是放大网络的深度(\nL_i\n\n\n\n\n\n \n \n\n)、宽度(\nC_i\n\n\n\n\n\n \n \n\n)和分辨率(\nH_i, W_i\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n)而不改变\n\\mathcal{F_i}\n\n\n\n\n\n \n \n\n，使得模型在给定的资源限制下最大化准确率。\n\n\\max_{d,w,r} Accuracy(\\mathcal{N}(d,w,r)) \\\\\n\\begin{align}\ns.t. \\quad\n&amp; \\mathcal{N} = \\bigodot_{i=1\\dots s} \\mathcal{\\hat F}^{d \\cdot L_i}(X_{\\langle r \\cdot \\hat H_i, r \\cdot \\hat W_i, w \\cdot \\hat C_i \\rangle}) \\\\\n&amp; \\text{Memory} (\\mathcal{N}) \\le \\text{target memory} \\\\\n&amp; \\text{FLOPS} (\\mathcal{N}) \\le \\text{target flops}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n\n\n\n\n \n \n\n \n\n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n\n\n\n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n\n\n \n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n缩放的维度\n深度(Depth, \nd\n\n\n\n\n \n\n)\n缩放网络的深度通常是最通用的方法。\n但随着网络深度增大，梯度爆炸问题使得网络更难训练，为此产生了很多应对的技术，如跳连接(skip connection)和批归一化(batch normalization)。\n但对于非常深的网络继续增大网络深度获得的准确率提升已经很小了。\n\n\n宽度(Width, \nw\n\n\n\n\n \n\n)\n缩放网络的宽度广泛应用于小型模型中。\n更宽的网络能捕捉更多容易处理的特征，也更容易训练。但过宽的宽度也会难以捕获高层的特征。\n当网络越来越宽，准确率的提升也会变得越来越缓慢。\n\n\n分辨率(Resolution, \nr\n\n\n\n\n \n\n)\n当输入更高分辨率的图像，卷积网络能够捕捉到更多的模式。\n早期的卷积网络常用 \n224 \\times 224\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n\n 的模型输入，更为现代化的模型偏向于使用 \n299 \\times 299\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n\n 或 \n331 \\times 331\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n\n 的模型输入以获得更高的准确率。\n非常高的分辨率下再提高分辨率也很难带来准确率的提升。\n\n\n\n复合缩放方法 Compound Scaling\n作者提出的复合缩放方法使用一个复合系数 \n\\phi\n\n\n\n\n \n\n 来统一缩放模型的深度、宽度和分辨率。\n\n\\begin{align}\n\\text{depth}: \\space &amp;d = \\alpha^\\phi \\\\\n\\text{width}: \\space &amp;w = \\beta^\\phi \\\\\n\\text{resolution}: \\space &amp;r = \\gamma^\\phi \\\\\ns.t. \\space &amp; \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\\\\n&amp; \\alpha \\ge 1, \\beta \\ge1, \\gamma \\ge 1\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n\n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n通过一个简单的网格搜索(grid search)，可以确定 \n\\alpha\n\n\n\n\n \n\n, \n\\beta\n\n\n\n\n \n\n, \n\\gamma\n\n\n\n\n \n\n 三个常数。用户只需要调整 \n\\phi\n\n\n\n\n \n\n 即可控制使用的资源量。\n一个常规卷积层的 FLOPS 正比于 \nd\n\n\n\n\n \n\n, \nw^2\n\n\n\n\n\n \n \n\n, \nr^2\n\n\n\n\n\n \n \n\n，通过 \n\\phi\n\n\n\n\n \n\n 缩放网络，模型的总 FLOPS 会被缩放 \n(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n 倍。通过限制 \n\\alpha \\cdot \\beta^2 \\cdot \\gamma^2\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n\n 约等于2，模型的总 FLOPS 缩放约为 \n2^\\phi\n\n\n\n\n\n \n \n\n 倍。\nEfficientNet作者首先通过nerual architecture search设计了EfficientNet-B0。\n\n其中 \n\\text{MBConv}\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 层为mobile inverted bottleneck MBConv。\n接下来基于EfficientNet-B0，通过复合缩放方法将模型放大：\n步骤：\n\n固定 \n\\phi = 1\n\n\n\n\n\n\n \n \n \n\n，即假设允许使用两倍的资源，使用网格搜索确定 \n\\alpha\n\n\n\n\n \n\n, \n\\beta\n\n\n\n\n \n\n, \n\\gamma\n\n\n\n\n \n\n。得到EfficientNet-B0的最佳值为 \n\\alpha=1.2\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n, \n\\beta=1.1\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n, \n\\gamma=1.15\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n\n。\n固定 \n\\alpha\n\n\n\n\n \n\n, \n\\beta\n\n\n\n\n \n\n, \n\\gamma\n\n\n\n\n \n\n，使用不同的 \n\\phi\n\n\n\n\n \n\n 放大模型得到 EfficientNet-B1 到 B7。\n\n\n\n\n","plink":"https://yuxinzhao.net/efficientnet/"},{"title":"计算机视觉-特征检测","date":"2020-03-27T15:43:40.000Z","updated":"2022-01-04T08:35:48.551Z","content":"本文介绍计算机视觉特征检测的基础知识。包含：\n\n边缘检测\n特征检测\n\n\n\n图像的特征被定义为一张图片与众不同的地方。\n边缘检测 Edge Detection边缘检测通常分为两步：\n\n找到组成边缘的所有像素点，也称为边缘子(edgel)。\n将这些边缘子聚合更长的边，有时候会使用更为紧凑的参数来表示这些边(compact parametric representation)。\n\n边缘子检测器 Edgel Detector基于梯度的检测器 Gradient Based Detector边缘通常是由像素点间急剧的亮度变化构成的，由此会产生阶梯形(step edge)、坡形(ramp edge)、屋顶形(roof edge)的边缘。我们可以将一张图片看成一个函数，函数值急剧的变化会产生边缘。如何去衡量这个“急剧的变化”呢，可以使用一阶导数(或称梯度)，导数可以描述函数陡峭的程度，一阶导数值越大，这个地方变化就越剧烈，就越可能是边缘。\n\n我们将一个图像用一个二维函数 \nf\n\n\n\n\n \n\n 来表示。沿着 \nx\n\n\n\n\n \n\n 轴和 \ny\n\n\n\n\n \n\n 轴两个方向分别求偏导，就是这个函数的梯度:\n\n\\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}) = (g_x, g_y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n边缘的大小(magnitude)和方向(direction)分别表示为:\n\n\\begin{align}\n\\| \\nabla f \\| = \\sqrt{g_x^2 +g_y^2} \\\\\n\\theta = \\tan^{-1} \\frac{g_y}{g_x}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n \n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\n\n由于二维图像是离散的，我们可以通过有限差分法(finite differences)得到导数：\n\n\\begin{align}\ng_x = f(x+1, y) - f(x, y) \\\\\ng_y = f(x, y+1) - f(x, y) \n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n这个有限差分用卷积来表示\n\n有了梯度，我们可以对这个梯度进行阈值化(thresholding)，即设定一个阈值，当梯度大小 \n\\| \\nabla f \\|\n\n\n\n\n\n\n \n \n \n \n\n 大于该阈值时，我们认为该点是边缘子。\n除了像上图中使用一个 \n1 \\times 2\n\n\n\n\n\n\n \n \n \n\n 和 \n2 \\times 1\n\n\n\n\n\n\n \n \n \n\n 的卷积核来对图像求梯度，一个更好的方式是使用 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n 的Sobel算子(Sobel operator)，使用这个算子卷积对噪声更robust。Sobel算子如下图：\n\n对于噪声的处理，一个常见的作法是在求梯度前先对图像用低通滤波器滤波，常用高斯滤波器 \nh\n\n\n\n\n \n\n 进行滤波。\n\n\\frac{\\partial}{\\partial x}(h * f) = \\frac{\\partial h}{\\partial x} * f\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n \n \n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n由上式，该过程也等价于先对高斯滤波器 \nh\n\n\n\n\n \n\n 求偏导后再对原图像滤波。\n基于曲率的检测器 Curvature Based Detector基于梯度的检测器已经表现得很好了，但它还存在着两个问题：\n\n对于阶梯形和坡形的边缘，它的定位不是很准确。\n对于不同边缘的响应(response)各异，及识别到的边缘有的很粗、有的很细、有的边缘甚至细到中途消失。\n\n基于曲率的检测器的基本思想是找到一阶导数的极大值或极小值，这意味着基于曲率的检测器只会找到边缘变化程度最大即最陡峭的地方作为边缘子，而这个极大值和极小值则可以通过二阶导数的零点(且是横跨正负半轴的零点，zero-crossing)找到。\n基于曲率的检测器也称为 Marr-Hildreth 边缘检测器，检测的流程分为两步：\n\n求图像的二阶导数。\n对图像中的每个像素点，检查其四周各点对称方向是否为一正一负，如像素点左一像素点的二阶导数小于0，右一像素点的二阶导数大于0，则可认为中央像素点就是零点，将其标记为边缘子。\n\n对于一个二维图像 \nf\n\n\n\n\n \n\n，其二阶导数可以描述为：\n\n\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n \n\n\n \n\n \n \n\n\n\n\n \n\n\n\n\n \n \n \n\n\n \n\n \n \n\n\n\n\n\n\n也可以将其描述为一阶导数的一阶导数，由此，这个二阶导数的可以用有限差分法表示为:\n\n\\begin{align}\ng_x - g_{x-1} &amp;= (f(x+1, y) - f(x, y)) - (f(x, y) - f(x-1,y)) \\\\\n&amp;= f(x+1, y) -2 f(x, y) + f(x-1, y) \\\\\ng_y - g_{y-1} &amp;= (f(x, y+1) - f(x, y)) - (f(x, y) - f(x,y-1)) \\\\\n&amp;= f(x, y+1) -2 f(x, y) + f(x, y-1)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n将其组合成 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n 的卷积核，我们可以得到 Laplacian 算子:\n\n上图分别为4连通邻域(4-connected neighbors)和8连通邻域(8-connected neighbors)的Laplacian算子。\n同样地，我们可以使用低通滤波器对图像先滤波再求导，以降低噪声干扰。这相当于先对低通滤波器求二阶导再与图像卷积。高斯滤波器求二阶导得到的卷积核称作LoG算子(Laplacian of Gaussian)。使用一个高斯滤波器减去一个delta函数，即核中央的值减去1，通常可以得到和LoG相似的效果，这是因为这样得到的卷积核和LoG滤波器非常接近。\nCanny边缘检测器 Canny Edge DetectorCanny检测器由Canny这一科学家发明，他提出了一个好的边缘检测器应该具备以下特点：\n\n检测率高(good detection): 检测到假边(spurious edge)数量少。\n定位准确(good localization): 检测到的边接近真实的边缘。\n最小响应(minimal response): 只有一个单点响应，即边缘的宽度为1个像素点。\n\nCanny检测器的四个步骤：\n\n使用低通滤波器消除噪声。\n\n常用高斯滤波器。\n\n\n计算图像的梯度大小和方向。\n\n使用Sobel算子计算梯度。\n\n\n对梯度大小图像进行非极大值抑制(non-maxima suppression)。\n\n如果在某个标记的像素点处，梯度的大小没有到达极大值，那么该点就被抑制掉(梯度置为0)。\n\n\n运用迟滞和连通分析(hysteresis and connectivity analysis)检测边缘。\n\n迟滞分析(hysteresis)是避免边缘断裂的一种方法，它设定了两个阈值 \nL\n\n\n\n\n \n\n 和 \nH\n\n\n\n\n \n\n。对于每个像素点:\n\n\\text{像素点} =\n     \\begin{cases}\n     \\text{强边缘像素}, &amp; \\text{梯度大小} &gt; H \\\\\n     \\text{弱边缘像素}, &amp; L \\le \\text{梯度大小} \\le H \\\\\n     \\text{非边缘像素}, &amp; \\text{梯度大小} &lt; L \n     \\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n像\n\n素\n\n\n点\n\n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n强\n\n边\n\n\n缘\n\n\n像\n\n\n素\n\n \n\n\n弱\n\n边\n\n\n缘\n\n\n像\n\n\n素\n\n \n\n\n非\n\n边\n\n\n缘\n\n\n像\n\n\n素\n\n \n\n\n\n\n梯\n\n度\n\n\n大\n\n\n小\n\n \n \n\n\n \n \n\n梯\n\n度\n\n\n大\n\n\n小\n\n\n \n \n\n\n梯\n\n度\n\n\n大\n\n\n小\n\n \n \n\n\n\n\n\n\n\n弱边缘像素(weak edge)需要进行连通分析(connectivity analysis)，当一个弱边缘像素连接着一个强边缘像素(strong edge)时，这个弱边缘像素被认为是这个强边缘像素的延续因此也标记为强边缘。\n\n\n\n\n多分辨率边缘检测 Multi-Resolution Edge Detection低分辨率的边缘通常比高分辨率的边缘更重要，但低分辨率的边缘也难以检测，除非先用低通滤波器将高频部分消除。\n随着图像越来越模糊，高分辨率的中检测到的一些边缘会逐渐消失，而低分辨率的边缘会得以保留。\n\n聚合边缘子 Aggregating Edgels上文中，我们介绍了边缘子的检测方法，但边缘子并非完美的边缘，下一步就是将这些边缘子聚合起来，得到更长的边缘。\n聚合边缘子有两种方法：\n\n通过局部聚合进行路径跟踪\n通过霍夫变换进行全局聚合\n\n通过局部聚合进行路径跟踪 Path Tracing Via Local Aggregation步骤：\n\n从某任意一个点开始计算它的近邻中梯度大小和方向相近的点，该点被认为是同一边缘的一个像素点。基于近邻中拥有相似属性的边缘子很有可能在同一条边缘上的思想。\n如果没有找到合适的近邻边缘子，则这个边缘子就是边缘的端点。处理结束。\n另外找一个没有被标记的点开始，重复上述过程。\n当所有边缘子被连接到一条边中或曾被考虑过是否是边的一部分时，算法结束。\n\n优点：\n\n适用于找到任意曲线的边缘。\n\n通过霍夫变换进行全局聚合 Global Aggregation Via Hough Transform比较局部聚合，全局聚合希望能够找到一条边缘的矢量表示。比如将一个直线用一个 \ny=mx+b\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 的方程来表示，而不是一系列像素点。\n霍夫变换考虑的是找到边缘子构成的直线。\n对于一个直线方程 \ny=mx+b\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n，可以写成:\n\nb = y - mx\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n其中，\n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 为像素点的坐标，\nm\n\n\n\n\n \n\n 为斜率，\nb\n\n\n\n\n \n\n 为偏置。上式可以表达，对于同一直线上的点 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n， 它们的 \nm\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n 都是固定的。\n对于竖直的直线，\nm\n\n\n\n\n \n\n 为无穷，这就比较麻烦，所以我们干脆换成直线的极坐标的表示，用距离 \nd\n\n\n\n\n \n\n 表示直线与原点的垂直距离，角度 \n\\theta\n\n\n\n\n \n\n 为直线的垂直线与坐标轴的角度。由此，可以将 \n(m,b)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 空间转换成 \n(d, \\theta)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 空间。\n步骤：\n\n对于每个边缘子 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，计算它的 \n(d, \\theta)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，在 \n(d, \\theta)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 空间上投票。\n找到 \n(d, \\theta)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 空间中的极大值。这就是这条直线的矢量表示。\n\n\n\n特征检测 Feature Detection通常来说，特征是一个或一系列与周围不同的像素点。\nMorovac 算子常用的 Morovac 算子(Morovac operator)用来衡量图像中某个点附近的自相似(self similarity)程度。一个像素点 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 自相似意味着像素点 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 邻近像素的周围和像素点 \n(x,y)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 附近很接近。\n\n计算自相似的方法： (以上图为例，计算 \nA5\n\n\n\n\n\n \n \n\n 像素点的自相似度)\n\n将像素点 \nA5\n\n\n\n\n\n \n \n\n 周围的 \nA1 \\dots A9\n\n\n\n\n\n\n\n \n \n \n \n \n\n 记为块 \nA\n\n\n\n\n \n\n。\nA5\n\n\n\n\n\n \n \n\n 的邻域中有9个 \n3 \\times 3\n\n\n\n\n\n \n \n \n\n 的块与其重叠，其中一个是块 \nB\n\n\n\n\n \n\n，在图中用蓝色框表示，另一个绿色的框表示另外一个块。\n\n块 \nA\n\n\n\n\n \n\n 与块 \nB\n\n\n\n\n \n\n 的相似度定义为\n\nS_{AB} = \\sum_{i=1}^n \\left( A_i - B_i \\right)^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n\nA5\n\n\n\n\n\n \n \n\n 的自相似度为块 \nA\n\n\n\n\n \n\n 与其周围9个块的相似度的和。 \n\n\n一个图像中自相似度的极大值点称为角点(corner)。\n缺点：\n\nMorovac 算子对噪声会产生响应，也会被一条边触发。\nMorovac 算子并非各向同性(isotropic)。如果图像旋转一下，像素点是不是角点判断也会轻易改变。\n\n\nHarris and Stephens-Plessey 角点检测器为了解决Morovac算子的缺陷，Harris and Stephens-Plessey角点检测器被提出。\n步骤：\n\n生成图像的梯度图像 \ng_x\n\n\n\n\n\n \n \n\n 和 \ng_y\n\n\n\n\n\n \n \n\n。\n\n对于像素附近的点 \n(u, v)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，计算以下矩阵\n\nA = \\sum_u \\sum_v w(u, v)\n   \\begin{pmatrix}\n   g_x(u, v)^2 &amp; g_x(u,v) g_y(u,v) \\\\\n   g_x(u,v) g_y(u,v) &amp; g_y(u, v)^2\n   \\end{pmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n \n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n\n \n \n\n\n\n\n \n\n\n\n其中，\nw(u,v)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是一个用来描述距离 \n(u,v)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 距离的权重。当 \nw\n\n\n\n\n \n\n 是一个高斯函数时，这个检测器就是各向同性的。\n\n计算上述矩阵 \nA\n\n\n\n\n \n\n 的特征值 \n\\lambda_1\n\n\n\n\n\n \n \n\n 和 \n\\lambda_2\n\n\n\n\n\n \n \n\n，这两个特征值与点 \n(u,v)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 处的主曲率成正比。当 \n\\lambda_1\n\n\n\n\n\n \n \n\n 和 \n\\lambda_2\n\n\n\n\n\n \n \n\n 都大时，判定该点为角点；当 \n\\lambda_1\n\n\n\n\n\n \n \n\n 和 \n\\lambda_2\n\n\n\n\n\n \n \n\n 都小时，不考虑该点；当 \n\\lambda_1\n\n\n\n\n\n \n \n\n 和 \n\\lambda_2\n\n\n\n\n\n \n \n\n 一大一小时，判定该点是边缘。\n\n\n\n\n参考资料\nIntroduction to Visual Computing: Core Concepts in Computer Vision, Graphics, and Image Processing\n\n","thumbnail":"computer-vision-feature-detection/feature-logo.png","plink":"https://yuxinzhao.net/computer-vision-feature-detection/"},{"title":"频谱分析","date":"2020-03-21T19:59:16.000Z","updated":"2022-01-04T08:35:48.563Z","content":"频谱分析(spectral analysis)是将一个复杂的信号分解成一系列简单的信号的技术。\n\n\n离散傅里叶变换 Discrete Fourier Transform离散傅里叶(DFT)将一个无穷的周期函数分解为一系列余弦波和正弦波。\n在使用DFT之前，我们碰到的第一个问题是如何用在数字信号上使用DFT？数字信号并不一定具有周期性，且是有穷的，这并不满足DFT的条件。但没关系，非周期函数又有穷，我们将他构造成无穷周期函数就可以了。\n\nDFT的条件达到了，那么DFT到底是什么？\nDFT 将带有\nN\n\n\n\n\n \n\n个采样点的输入信号\nx\n\n\n\n\n \n\n分解为\n\\frac{N}{2}+1\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n个余弦波和\n\\frac{N}{2}+1\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n个正弦波，这些正弦波和余弦波加在一起可以重新得到我们的输入信号\nx\n\n\n\n\n \n\n。\n我们表示输入信号为\nx[0,1,\\dots,N-1]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n，余弦波为\nx_c[0,1,\\dots,\\frac{N}{2}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n\n，正弦波为\nx_s[0,1,\\dots,\\frac{N}{2}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n\n，其中\nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n表示余弦波的振幅，索引 \nk\n\n\n\n\n \n\n 表示在这\nN\n\n\n\n\n \n\n个采样点的范围内余弦波有 \nk\n\n\n\n\n \n\n 个周期，也就是说，这个余弦波的频率为 \nf=\\frac{k}{N}\n\n\n\n\n\n\n\n \n \n\n\n\n \n \n\n\n\n，正弦波同理。\n\nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的\nx\n\n\n\n\n \n\n轴与频率 \nf\n\n\n\n\n \n\n 成线性关系，相当于是幅值随频率的变化，由此\nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 就可以用来表示信号 \nx\n\n\n\n\n \n\n 的频域。\n\n每一个 \nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 都对应着一个余弦波 \nc_k\n\n\n\n\n\n \n \n\n，这个余弦波有 \nN\n\n\n\n\n \n\n 个采样点。\n\nc_k[i] = x_c[k] \\cos(\\frac{2\\pi k i}{N}) = x_c[k] \\cos(2 \\pi f i) = x_c[k] \\cos(\\omega i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n\n\n每一个 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 都对应着一个正弦波 \ns_k\n\n\n\n\n\n \n \n\n，这个正弦波有 \nN\n\n\n\n\n \n\n 个采样点。\n\ns_k[i] = x_s[k] \\sin(\\frac{2 \\pi k i}{N}) = x_s[k] \\sin(2 \\pi f i) = x_s[k] \\sin(\\omega i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n\n\n由此，信号 \nx\n\n\n\n\n \n\n 可以表示为:\n\nx[i] = \\sum_{k=0}^{\\frac{N}{2}} x_c[k] \\cos(\\frac{2 \\pi k i}{N}) + \\sum_{k=0}^{\\frac{N}{2}} x_s[k] \\sin(\\frac{2 \\pi k i}{N})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n \n\n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n\n\n但事情没有这么简单，\nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 并非这些余弦波真实的幅值，真实的只是 \n\\hat x_c[k]\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 与 \n\\hat x_s[k]\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 与它们成倍数关系:\n\n\\hat x_c[k] =\n\\begin{cases}\n\\frac{2}{N} x_c[k], &amp; k \\ne 0 \\\\\n\\frac{1}{N} x_c[k], &amp; k = 0\n\\end{cases}\n\\\\\n\\hat x_s[k] =\n\\begin{cases}\n\\frac{2}{N} x_s[k], &amp; k \\ne \\frac{N}{2} \\\\\n\\frac{1}{N} x_s[k], &amp; k = \\frac{N}{2}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n\n\n\n\n \n \n\n\n \n \n\n \n \n \n \n\n\n\n\n \n \n\n\n \n \n\n \n \n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n\n\n\n\n \n \n\n\n \n \n\n \n \n \n \n\n\n\n\n \n \n\n\n \n \n\n \n \n \n \n\n\n\n\n \n \n\n\n\n \n \n\n\n\n\n \n \n\n\n\n \n \n\n\n\n\n\n\n\n\n\n真正的表达式应该为:\n\nx[i] = \\sum_{k=0}^{\\frac{N}{2}} \\hat x_c[k] \\cos(\\frac{2 \\pi k i}{N}) + \\sum_{k=0}^{\\frac{N}{2}} \\hat x_s[k] \\sin(\\frac{2 \\pi k i}{N})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n \n\n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n\n\n而 \nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 可以用以下公式得到:\n\n\\begin{align}\nx_c[k] &amp;= \\sum_{i=0}^{N-1} x[i] \\cos \\left( \\frac{2 \\pi k i}{N} \\right) \\\\\nx_s[k] &amp;= \\sum_{i=0}^{N-1} x[i] \\sin \\left( \\frac{2 \\pi k i}{N} \\right)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n\n \n \n \n\n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n\n\n\n \n\n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n\n \n \n \n\n\n \n\n\n\n\n \n \n \n \n\n \n\n\n \n\n\n\n\n\n\n频域的极坐标表示上文中，我们用 \nx_c\n\n\n\n\n\n \n \n\n 和 \nx_s\n\n\n\n\n\n \n \n\n 来表示信号 \nx\n\n\n\n\n \n\n 的频域，这种表示方法称为频域的矩形表示(rectangular representation)。\n对于同一个频率的正弦波和余弦波，有以下关系:\n\nx_c[k] \\cos(\\omega i) + x_s[k] \\sin(\\omega i) = M[k] \\cos(\\omega i + \\theta[k])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n\n\n其中 \nM[k] = \\sqrt{x_c[k]^2 + x_s[k]^2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n\n\n\n， \n\\theta[k] = \\tan^{-1} \\left( \\frac{x_s[k]}{x_c[k]} \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n\n\n\n \n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n \n\n\n。\n也就是说，一对 \nx_c[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 和 \nx_s[k]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 可以用一对 \nM[k]\n\n\n\n\n\n\n\n \n \n \n \n\n 和 \n\\theta[k]\n\n\n\n\n\n\n\n \n \n \n \n\n 来表示，这种表示称为频域的极坐标表示(polar representation)。\n离散傅里叶变换的性质\n齐次性 (Homogeneity)\n\n\\begin{align}\n   &amp; x[t] \\to (M[f], \\theta[f]) \\\\\n   \\Rightarrow &amp; kx[t] \\to (kM[f], \\theta[f])\n   \\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n可加性 (Additivity)\n\n\\begin{align}\n   &amp;x[t] \\to (x_c[f], x_s[f]), y[t] \\to (y_c[f], y_s[f]) \\\\\n   \\Rightarrow &amp; x[t] + y[t] \\to (x_c[f] + y_c[f], x_s[f] + y_s[f])\n   \\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n\n\n\n线性相位平移 (Linear Phase Shift)\n\n\\begin{align}\n   &amp; x[t] \\to (M[f], \\theta[f]) \\\\\n   \\Rightarrow &amp; x[t+s] \\to (M[f], \\theta[f] + 2 \\pi f s)\n   \\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n信号的复共轭 complex conjugate如果一个信号有着和信号 \nx\n\n\n\n\n \n\n 相同的幅值，但有着相反的相位，这个信号被称为信号 \nx\n\n\n\n\n \n\n 的复共轭，用 \nx*\n\n\n\n\n\n \n \n\n 表示。即若 \nx \\to (M[f], \\theta[f])\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n, 则 \nx* \\to (M[f], -\\theta[f])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n。\n性质：\n\n一个信号与其复共轭信号相加会消除所有频率的相位，使得相加后的信号只有多个余弦波构成，不含正弦波。\n复共轭信号在时域上是原信号以 \n\\frac{N}{2}\n\n\n\n\n\n\n\n \n \n\n\n 为轴的水平翻转。\n\n\n频域的周期性和奇偶性由于余弦函数的周期性和对称性:\n\nA \\cos(f) = A \\cos(-f) = A \\cos(2\\pi -f) = A \\cos(n2\\pi -f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n\n我们可以得到\n\nM[f] = M[-f]\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n因此，\nM[f]\n\n\n\n\n\n\n\n \n \n \n \n\n 是一个周期为 \n2\\pi\n\n\n\n\n\n \n \n\n 的偶函数。\n\n由于\n\n\\cos (f+\\theta) = \\cos(-f-\\theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n\n我们可以的得到\n\n\\theta[f] = -\\theta[-f]\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n因此，\n\\theta[f]\n\n\n\n\n\n\n\n \n \n \n \n\n 是一个周期为 \n2\\pi\n\n\n\n\n\n \n \n\n 的奇函数。\n\n幅度调制 amplitude modulation对于一个信号 \nx\n\n\n\n\n \n\n，我们将其乘以一个高频的正弦波 \ny\n\n\n\n\n \n\n (又称载波 carrier ware 或载波频率 carrier frequency)，这个过程称为幅度调制(amplitude modulation, AM)。\n\n幅度调制有两点需要注意：\n\n由于幅度是偶函数，频域幅度卷积时应考虑在内，卷积后的结果才会是关于 \nf=c\n\n\n\n\n\n\n \n \n \n\n 轴对称。\n在添加多个信号是，幅度调制可以将不同信号的频率分布到高频的不同位置，只要使用不同频率的载波即可。但为了信号之间不混叠，载波之间频率应该相隔 \n2b\n\n\n\n\n\n \n \n\n。\n\n混叠 aliasing在幅度调制中我们已经提及了，如果载波的频率相隔小于 \n2b\n\n\n\n\n\n \n \n\n ，不同信号幅度调制后就会发生重叠，这个现象称为混叠。\n\n对偶性 DualityDFT有着奇妙的对偶性，我们可以看看以下几个例子初步理解对偶性的含义。\n\n一个尖峰的频域是一个定值，一个定值的频域则是一个尖峰。\n\n随着时域上高斯函数的拉伸，频域上开始变窄。\n\n\n\n参考资料\nIntroduction to Visual Computing: Core Concepts in Computer Vision, Graphics, and Image Processing\n\n","thumbnail":"computer-vision-spectral-analysis/fourier.jpg","plink":"https://yuxinzhao.net/computer-vision-spectral-analysis/"},{"title":"不通过数据卷在docker容器与宿主机间交换数据","date":"2020-03-21T13:36:03.000Z","updated":"2022-01-04T08:35:48.567Z","content":"通常情况下，我们让docker容器与宿主机进行文件传输，最好的方式就是使用挂载数据卷，但偶尔会遇到一些特殊的情况，容器没有挂在数据卷，但容器内又保存着重要的数据，我们希望将它移到宿主机上，但关闭容器重新挂载数据集又会丢失这些数据，为此我们需要一个方法让宿主机和已经启动的容器间能够交换数据。\n本文介绍docker的 cp 复制指令。\n\n\n将本地文件复制到docker容器内1$ sudo docker cp &lt;path-in-localhost&gt; &lt;container-id&gt;:&lt;path-in-container&gt;\n\n将docker容器内的文件复制到本地1$ sudo docker cp &lt;container-id&gt;:&lt;path-in-container&gt; &lt;path-in-localhost&gt;\n\n","plink":"https://yuxinzhao.net/copy-files-from-docker-container/"},{"title":"卷积(Convolution)","date":"2020-03-13T22:06:27.000Z","updated":"2022-01-04T08:35:48.551Z","content":"本文详细介绍卷积的定义及性质。\n\n\n记号说明\n\n\n记号\n说明\n\n\n\n\nS\n\n\n\n\n \n\n\n系统(具体语境下指线性系统)\n\n\n\n*\n\n\n\n\n \n\n\n卷积\n\n\n线性系统 Linear System在介绍卷积之前，我们需要先弄清楚线性系统，以便更好地理解卷积的由来。\n线性系统的定义一个系统(system)被定义为修改信号的一种方法，而线性系统(linear system)是满足以下线性的性质的系统：\n\n齐次性(Homogeneity)\n\n\n可加性(Additivity)\n\n\n平移不变性(Shift Invariance)\n\n\n可交换性(Commutative)\n\n\n可叠加性(Superpostion)\n\n\n\n线性系统的响应一个脉冲(impulse), \ni[t]\n\n\n\n\n\n\n\n \n \n \n \n\n，是一个只有一个非零值的离散信号，即\n\ni[t] = \n\\begin{cases}\nk, &amp;t=s \\\\\n0, &amp;t \\ne s\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n特别地，Delta, \n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n, 是一个当\nt=0\n\n\n\n\n\n\n \n \n \n\n时为\n1\n\n\n\n\n \n\n，当 \nt\n\n\n\n\n \n\n 取其他值时为0的脉冲函数，即\n\n\\delta[t] = \n\\begin{cases}\n1, &amp; t=0 \\\\\n0, &amp; t \\ne 0\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n\n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 可以被认为是最简单的脉冲函数。\n任意一个脉冲函数 \ni[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 都可以用 \n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 通过平移和缩放得到:\n\ni[t] = k \\delta[t-s]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n让一个 \n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 通过一个线性系统得到输出 \nh[t]\n\n\n\n\n\n\n\n \n \n \n \n\n ，我们称之为该线性系统的脉冲响应(impulse response)，又称卷积核(kernel)或者滤波器(filter)。\nh[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 的大小(1D 情况下的宽)也叫做这个核的支撑(support)。\n对于线性系统的一个一般输入 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n，我们可以将其分解成 \nn\n\n\n\n\n \n\n 个脉冲 \ni_1[t]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n, \ni_2[t]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n, \n...\n\n\n\n\n \n \n \n\n, \ni_n[t]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，使得\ni_l[t]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 当 \nt=l\n\n\n\n\n\n\n \n \n \n\n 时取非0值，当 \nt \\ne 0\n\n\n\n\n\n\n \n \n \n\n 时取0。由此一个一般输入 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 可以表示为多个脉冲的和：\n\nx[t] = \\sum_{i=l}^n i_l[t]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n \n\n\n对于一个脉冲输入\ni_l[t]\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n，其响应为 \ni_l[l]h[t-l]=x[l]h[t-l]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n。\n由线性系统的可加性，\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 的响应为:\n\nR[t] = \\sum_{l=1}^n x[l]h[t-l] = x[t] *h[t]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n其中 \nx[t]*h[t]\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n 为 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 与 \nh[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 的卷积。于是，一个线性系统的计算可以用输入与脉冲响应的卷积来表示。\n在 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 的边界条件下，\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 将取不到值可以进行计算，此时常用的措施是假设该点的值，如取0，或者取边界值，但无论如何，这个措施给卷积带来了两个缺点:\n\n\nR\n\n\n\n\n \n\n 的大小比线性系统的输入\nx\n\n\n\n\n \n\n 要大，假如\nh\n\n\n\n\n \n\n的大小是\nm\n\n\n\n\n \n\n，则\nR\n\n\n\n\n \n\n的大小就是 \nn+m-1\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n。\n由于引入了一些假设的值，计算出的\nR\n\n\n\n\n \n\n并不是精确的。\n\n卷积的性质全通系统(all pass system): 特殊情况下，线性系统的脉冲响应为\n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n，那么这个系统就是全通系统。\n\nx[t] * \\delta[t] = x[t]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n延迟系统(delay system): 不改变信号而对信号进行偏移的系统。\n\nx[t] * \\delta[t+s] = x[t+s]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n卷积操作满足以下性质:\n\n交换律(commutative)\n\na[t] * b[t] = b[t]*a[t]\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n结合律(associative)\n\n(a[t]*b[t])*c[t] = a[t] * (b[t]*c[t])\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n分配律(distributive)\n\na[t]*b[t]+a[t]*c[t]=a[t]*(b[t]+c[t])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n线性滤波器 Linear Filter从时域到频域对于一个时域上的输入 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n，我们可以将其转换到频域进行观察。\n时域与频域的转换有以下性质:\n\n其中, \nF\n\n\n\n\n \n\n代表一个将函数由时域转换到频域的函数。\n\n观察上图，我们可以建立一个直观的认识：如果输入\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 曲线在某一点\nt\n\n\n\n\n \n\n变化越平缓，则该点的频率越低，相反，变化越陡峭，则频率越高。由此，一个输入\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n涵盖一定频率\nf\n\n\n\n\n \n\n的区间，而可能有一部分区间没有涵盖到。特别地，\n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n涵盖了所有的频率，因此其频率分布是一条直线。\n全通滤波器 All Pass Filter前面我们已经介绍了 \n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 就是全通滤波器，但这个”全通”是什么意思呢？\n\n由于 \n\\delta[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 涵盖了所有频率，且频率分布为一条定值直线，可以使得输入 \nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n 的频率分布不发生改变，所有频率都通过了，因此，称之为全通滤波器。\n低通滤波器 Low Pass Filter\n上图中，蓝色滤波器通过卷积使得输入\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n高于\nf_2\n\n\n\n\n\n \n \n\n的频率被截断，绿色滤波器使得\nx[t]\n\n\n\n\n\n\n\n \n \n \n \n\n高于\nf_1\n\n\n\n\n\n \n \n\n的频率被截断，此类使得低频率通过，而使高频率截断的滤波器统称为低通滤波器。\n低通滤波器在二维图像上应用最明显的特点就是图像变模糊了，由于频率可以看作是采样的密度，当使用低通滤波器时，高频率被截去，使得采样的密度降低，由此，图像就变得模糊了。\n通过将其转换到傅里叶频谱进行观察，我们可以更直观地对低通滤波器截断高频率进行理解：\n\n傅里叶频谱中，离中心点越远的点，频率越高。应用低通滤波器后，原傅里叶频谱中高于一定频率的部分被截断，由此，傅里叶频谱被挖出了中间的一个圆。\n需要注意的是，低通滤波器不仅仅是截断了高频率，通过的低频率部分也被改变。\n高通滤波器 High Pass Filter相对于低通滤波器，高通滤波器使得高频率的部分通过。一个较为简单的构造高通滤波器的方法是让一个全通滤波器减去一个低通滤波器。\n假设\nI\n\n\n\n\n \n\n为输入的图像，\nl\n\n\n\n\n \n\n为低通滤波器，经过低通滤波器的图像为\nI_l\n\n\n\n\n\n \n \n\n，经过高通滤波器的图像为\nI_h\n\n\n\n\n\n \n \n\n，由此：\n\n\\begin{align}\nI_h &amp;= I - I_l \\\\\n&amp;= I*\\delta -I*l \\\\\n&amp;= I*(\\delta - l)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n\n\n\n得到的高通滤波器 \n\\delta - l\n\n\n\n\n\n\n \n \n \n\n 见下图:\n\n从傅里叶频谱来看:\n\n高通滤波器相当于挖去了原频谱中低频部分的圆，需要注意的是上图并不能完全表明高通滤波器对傅里叶频谱的作用，由于低通滤波器对于低频信号进行了修改，使得高通滤波器并不能完美挖去这个圆，仍有部分低通信号残余。\n带通滤波器 Band Pass Filter带通滤波器保留了频率在 \nf_1\n\n\n\n\n\n \n \n\n 到 \nf_2\n\n\n\n\n\n \n \n\n 之间的信号，截断小于\nf_1\n\n\n\n\n\n \n \n\n和大于\nf_2\n\n\n\n\n\n \n \n\n的信号，可以认为是两个低通滤波器的差。\n从傅里叶频谱来看：\n\n二维滤波器的可分离性当一个二维滤波器\nh\n\n\n\n\n \n\n可以用两个一维滤波器\na\n\n\n\n\n \n\n, \nb\n\n\n\n\n \n\n进行表示，使得\nh[i][j]=a[i] \\times b[j]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n时，这个二维滤波器是一个可分离的二维滤波器。\n\nI * h = I * (a*b)= (I * a) * b\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n我们可以图像先用 \na\n\n\n\n\n \n\n 进行滤波，再用 \nb\n\n\n\n\n \n\n 进行滤波，达到和直接使用二位滤波器 \nh\n\n\n\n\n \n\n 进行滤波同样的效果。\n可分离的滤波器的好处是可以更高效地进行运算：\n假设一张图像有\nN\n\n\n\n\n \n\n个像素，二维滤波器\nh\n\n\n\n\n \n\n的大小为\np \\times q\n\n\n\n\n\n\n \n \n \n\n，且可以拆分成大小分别为\np\n\n\n\n\n \n\n和\nq\n\n\n\n\n \n\n的一维滤波器\na\n\n\n\n\n \n\n, \nb\n\n\n\n\n \n\n。直接通过\nh\n\n\n\n\n \n\n对图像进行卷积需要\npqN\n\n\n\n\n\n\n \n \n \n\n个乘法和\npqN\n\n\n\n\n\n\n \n \n \n\n个加法，加起来就是\n2pqN\n\n\n\n\n\n\n\n \n \n \n \n\n个浮点运算。而用\na\n\n\n\n\n \n\n和\nb\n\n\n\n\n \n\n依次进行卷积，\na\n\n\n\n\n \n\n需要\n2pN\n\n\n\n\n\n\n \n \n \n\n个浮点运算，\nb\n\n\n\n\n \n\n需要\n2qN\n\n\n\n\n\n\n \n \n \n\n个浮点运算，加起来就是\n2(p+q)N\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n个浮点运算。由此可见，可分离的二维滤波器分离后再进行卷积会更高效。\n参考资料\nIntroduction to Visual Computing: Core Concepts in Computer Vision, Graphics, and Image Processing\n\n","thumbnail":"computer-vision-convolution/conv.jpg","plink":"https://yuxinzhao.net/computer-vision-convolution/"},{"title":"GitHub Action自动化部署hexo博客到GitHub Page和Coding Page","date":"2020-03-07T05:19:05.000Z","updated":"2022-01-04T08:35:48.595Z","content":"由于国内访问GitHub很慢，我的GitHub Page有时需要很久才能加载出来，刚好最近捣鼓了以下GitHub Action，便尝试了用GitHub Action实现对GitHub Page和Coding Page的自动化部署，将博客同时部署到GitHub和Coding，再利用域名DNS将两个国内和国外的访问分流到Coding和GitHub上。\n\n\nGitHub准备工作打开GitHub Settings &gt; Developer settings &gt; Personal access tokens，添加一个新的token。\n记录下这个token的值，此处我们将其称为GH_TOKEN，稍后会用到。\nCoding准备工作打开Coding，新建一个跟你用户名同名的项目。\n打开该项目的设置 &gt; 功能开关，开启”构建与部署”\n回到项目页面，即可在左侧工具栏找到 构建与部署 &gt; 静态网站，自行设置即可。\n接下来最关键的是获取这个仓库的访问token：打开项目的设置 &gt; 开发者选项 &gt; 项目令牌，创建一个新的令牌，你会获得一个用户名和一个密码，我们将&lt;用户名&gt;:&lt;密码&gt;记录为CD_TOKEN。\nGitHub Action设置打开博客的源码仓库，在Settings &gt; Secrets中增加以下键值对：\n\n\n\n键\n值\n\n\n\nGIT_NAME\n你的git设置的用户名，一般为你的github用户名\n\n\nGIT_EMAIL\n你的git设置的邮箱\n\n\nGH_TOKEN\n在github获得个人访问token\n\n\nGH_REF\n用于部署的github仓库的git地址，格式为&lt;user-name&gt;/&lt;repo-name&gt;.git\n\n\nCD_TOKEN\n在coding获得的仓库访问token，格式为&lt;token-user-name&gt;:&lt;token-password&gt;\n\n\nCD_REF\n用于部署的github仓库的git地址，格式为&lt;user-name&gt;/&lt;repo-name&gt;.git\n\n\n添加GitHub Action:\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051name: 自动部署 Hexoon:  push:    branches:      - masterjobs:  build:    runs-on: ubuntu-latest    strategy:      matrix:        node-version: [10.x]    steps:      - name: 开始运行        uses: actions/checkout@v1      - name: 设置 Node.js $&#123;&#123; matrix.node-version &#125;&#125;        uses: actions/setup-node@v1        with:          node-version: $&#123;&#123; matrix.node-version &#125;&#125;      - name: 安装 Hexo CI        run: |          export TZ='Asia/Shanghai'          npm install hexo-cli -g      - name: 缓存        uses: actions/cache@v1        id: cache-dependencies        with:          path: node_modules          key: \n{{runner.OS}}-\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n&#123;&#123;hashFiles('**/package-lock.json')&#125;&#125;      - name: 安装插件        if: steps.cache-dependencies.outputs.cache-hit != 'true'        run: |          npm install      - name: 部署博客        run: |          mkdir themes          git clone https://github.com/&lt;user-name&gt;/&lt;theme-repo&gt;.git themes/inside  #克隆hexo主题仓库到主题目录下          hexo clean &amp;&amp; hexo g          cd ./public          git init          git config user.name \"$&#123;&#123;secrets.GIT_NAME&#125;&#125;\"          git config user.email \"$&#123;&#123;secrets.GIT_EMAIL&#125;&#125;\"          git add .          git commit -m \"Update\"          git push --force --quiet \"https://\n{{secrets.GH_TOKEN}}@\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n&#123;&#123;secrets.GH_REF&#125;&#125;\" master:master          git push --force --quiet \"https://\n{{secrets.CD_TOKEN}}@\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n&#123;&#123;secrets.CD_REF&#125;&#125;\" master:master\n\nDNS设置到自己域名的服务商修改DNS，增加两个CNAME记录，一个将你的域名映射至GitHub Page的 &lt;user-name&gt;.github.io并设置线路为境外，一个将域名映射至Coding Page的&lt;user-name&gt;.coding-pages.com并设置线路为境内。\n参考资料\nGenerating a new SSH key and adding it to the ssh-agent\ntravisci/GitHub Actions部署GitHub Pages和Coding Pages\n\n","thumbnail":"github-action-auto-deploy-hexo/logo.jpg","plink":"https://yuxinzhao.net/github-action-auto-deploy-hexo/"},{"title":"git clone 克隆大项目时发生error RPC failed","date":"2020-03-07T03:47:56.000Z","updated":"2022-01-04T08:35:48.591Z","content":"本文介绍如何解决 git 在克隆较大项目时容易发生的错误 error: RPC failed。\n\n\n错误内容1234error: RPC failed; curl 18 transfer closed with 31812496 bytes remaining to readfatal: The remote end hung up unexpectedlyfatal: early EOFfatal: index-pack failed\n\n解决方法通常我们从github克隆是采用https的方式，即\n1$ git clone https://github.com/&lt;user-name&gt;/&lt;repo-name&gt;.git\n\n这种方式克隆时会有一个buffer的上限，超过此上限时就会报错。\n通过使用ssh方式进行克隆可以解决这个问题。\n\n在本机上生成一个新的SSH密钥。\n1234$ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"Enter file in which to save the key (~/.ssh/id_rsa): #存储密钥的路径，直接EnterEnter passphrase (empty for no passphrase): # 密码，可设定可不设定，设定后每次上传和下载都需要输入密码Enter same passphrase again:  # 再输入一次密码\n\n到密钥存储的位置可以查看密钥:\n12$ cat ~/.ssh/id_rsa.pubssh-rsa 一长串密钥 you_email@example.com\n\n在github上添加生成的密钥。\n在github的 settings &gt; SSH and GPG keys 页面下添加新的ssh key，密钥内容为id_rsa.pub的全部内容，包含开头的ssh-rsa和结尾的邮箱地址。\n\n采用ssh方式进行clone。\n1$ git clone git@github.com:&lt;user-name&gt;/&lt;repo-name&gt;.git\n\n\n\n","thumbnail":"git-clone-error-RPC-failed/logo.jpg","plink":"https://yuxinzhao.net/git-clone-error-RPC-failed/"},{"title":"easycore多GPU并行加速","date":"2020-03-03T20:04:55.000Z","updated":"2022-01-04T08:35:48.567Z","content":"本文介绍如何使用easycore进行多GPU并行加速。\n\n\n上期“python多进程及pytorch多GPU并行推断”我介绍如何使用python自带的multiprocessing 库进行加速的方案，该方案存在一定缺陷：\n\n在Windows下无法运行，仅支持fork模式。\n消费者传回数据量过大时无法成功传输。\n\n以上问题通过将消费者从进程改为线程后都得到了解决。\n我已将完整的代码封装进了我正在开发的一个工具库easycore，欢迎star。项目文档见此处，含API文档及教程。\n以下介绍如何使用我的工具库easycore进行多GPU并行加速：\n安装easycore1pip install easycore==0.2.0\n\n注： 0.2.0版本才加入多进程并行加速工具。\n国内可使用清华镜像加速：\n1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple easycore==0.2.0\n\n多进程加速API多进程加速主要涉及easycore.common.parallel.UnorderedRunner 和easycore.common.parallel.OrderedRunner两个类，详细的API接口请参见API文档。\n通过继承UnorderedRunner或OrderedRunner两个类，并重写以下六个类静态方法，可以自定义一个并行运行器。\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@staticmethoddef producer_init(device, cfg):    \"\"\"     function for producer initialization.        Args:        device (str): device for the this process.        cfg (easycore.common.config.CfgNode): config of this process, you can use it to transfer data            to `producer_work` and `producer_end` function.    \"\"\"    pass@staticmethoddef producer_work(device, cfg, data):    \"\"\"     function specify how the producer processes the data.        Args:        device (str): device for this process.        cfg (easycore.common.config.CfgNode): config of this process, you can use it to get data from            `producer_init` function and transfer data to the next `producer_work` and `producer_end`            function.        data (Any): data get from input of `__call__` method.        Returns:        Any: processed data    \"\"\"    return data@staticmethoddef producer_end(device, cfg):    \"\"\"     function after finishing all of its task and before close the process.        Args:        device (str): device for this process.        cfg (easycore.common.config.CfgNode): config of this process, you can use it to get data            from `producer_init` and `producer_work` function.    \"\"\"    pass@staticmethoddef consumer_init(cfg):    \"\"\"    function for consumer initialization.        Args:        cfg (easycore.common.config.CfgNode): config of this process, you can use it to transfer data            to `consumer_work` and `consumer_end` function.    \"\"\"    pass@staticmethoddef consumer_work(cfg, data):    \"\"\"    function specify how the consumer processses the data from producers.        Args:        cfg (easycore.common.config.CfgNode): config of this process, you can use it to get data from            `consumer_init` function and transfer data to the next `consumer_work` and `consumer_end`            function.    \"\"\"    pass@staticmethoddef consumer_end(cfg):    \"\"\"    function after receiving all data from producers.        Args:        cfg (easycore.common.config.CfgNode): config of this process, you can use it get data from            `consumer_work` function.    Returns:        Any: processed data    \"\"\"    return None\n\nExample 1: 平方和通常我们可以通过以下方法实现求平方和的功能：\n12345678data_list = list(range(100))result = sum([data * data for data in data_list])# or more simpleresult = 0for data in data_list:    square = data * data    result += square\n\n上述实现中，我们先计算了每个元素的平方，再将它们加到一起。在这种情况下，我们可以将它拆成两个任务分别分配给生产者和消费者，从而实现并行：\n123456789101112131415161718192021222324252627282930from easycore.common.config import CfgNodefrom easycore.common.parallel import UnorderedRunnerclass Runner(UnorderedRunner):    @staticmethod    def producer_work(device, cfg, data):        return data * data  # calculate square of data    @staticmethod    def consumer_init(cfg):        cfg.sum = 0  # init a sum variable with 0, you can use cfg to transfer data    @staticmethod    def consumer_work(cfg, data):        cfg.sum += data  # add the square to the sum variable    @staticmethod    def consumer_end(cfg):        return cfg.sum  # return the result you needif __name__ == '__main__':    runner = Runner(devices=3)  # if you specify `device with a integer`, it will use cpus.    # You can specify a list of str instead, such as:     # runner = Runner(devices=[\"cpu\", \"cpu\", \"cpu\"])         data_list = list(range(100))  # prepare data, it must be iterable    result = runner(data_list)  # call the runner    print(result)    runner.close()  # close the runner and shutdown all processes it opens.\n\nExample 2: 一个神经网络推断器我们先在 network.py 定义一个简单的神经网络:\n12345678910111213import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.fc = nn.Linear(1, 3)        def forward(self, x):        x = self.fc(x)        x = F.relu(x)        return x\n\n通过以下方法，可以将推断任务分配到四个GPU上:\n123456789101112131415161718192021222324252627282930313233343536373839404142434445from easycore.common.config import CfgNodefrom easycore.common.parallel import OrderedRunnerfrom network import Netimport torchclass Predictor(OrderedRunner):    @staticmethod    def producer_init(device, cfg):        cfg.model = Net()     # init the producer with a model        cfg.model.to(device)  # transfer the model to certain device    @staticmethod    def producer_work(device, cfg, data):        with torch.no_grad():            data = torch.Tensor([[data]])  # preprocess data            data = data.to(device)  # transfer data to certain device            output = cfg.model(data)  # predict            output = output.cpu()  # transfer result to cpu        return output    @staticmethod    def producer_end(device, cfg):        del cfg.model  # delete the model when all data has been predicted.    @staticmethod    def consumer_init(cfg):        cfg.data_list = []  # prepare a list to store all data from producers.    @staticmethod    def consumer_work(cfg, data):        cfg.data_list.append(data)  # store data from producers.    @staticmethod    def consumer_end(cfg):        data = torch.cat(cfg.data_list, dim=0)  # postprocess data.        return dataif __name__ == '__main__':    predictor = Predictor(devices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"])  # init a parallel predictor    data_list = list(range(100))  # prepare data    result = predictor(data_list)  # predict    print(result.shape)    predictor.close()  # close the predictor when you no longer need it.\n\nExample 3: 批处理数据通过使用一个简单的生成器或者pytorch的dataloader，可以很轻松地生成批数据并通过UnorderedRunner或OrderedRunner进行加速。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from easycore.common.config import CfgNodefrom easycore.torch.parallel import OrderedRunnerfrom network import Netimport torchdef batch_generator(data_list, batch_size):    for i in range(0, len(data_list), batch_size):        data_batch = data_list[i : i+batch_size]        yield data_batchclass Predictor(OrderedRunner):    @staticmethod    def producer_init(device, cfg):        cfg.model = Net()        cfg.model.to(device)    @staticmethod    def producer_work(device, cfg, data):        with torch.no_grad():            data = torch.Tensor(data).view(-1,1)            data = data.to(device)            output = cfg.model(data)            output = output.cpu()        return output    @staticmethod    def producer_end(device, cfg):        del cfg.model    @staticmethod    def consumer_init(cfg):        cfg.data_list = []    @staticmethod    def consumer_work(cfg, data):        cfg.data_list.append(data)    @staticmethod    def consumer_end(cfg):        data = torch.cat(cfg.data_list, dim=0)        return dataif __name__ == '__main__':    predictor = Rredictor(devices=[\"cuda:0\", \"cuda:1\"])        data_list = list(range(100))    result = predictor(batch_generator(data_list, batch_size=10))      print(result.shape)    predictor.close()\n\n这里，我们将easycore.common.parallel替换成了easycore.torch.parallel。这两个库有着完全相同的API，区别是easycore.torch.parallel使用了torch.multiprocessing库实现而不是multiprocessing。\nExample 4: 将Runner外的参数传入Runner中通过cfg参数，你可以很轻松的将任何参数传入自定义的Runner中。cfg是一个easycore.common.config.CfgNode的对象，它的具体使用可以参见教程“Light weight config tools”。\n接下来我们使用“幂之和”作为案例：\n1234567891011121314151617181920212223242526272829303132from easycore.common.config import CfgNode as CNfrom easycore.common.parallel import UnorderedRunnerclass Runner(UnorderedRunner):    @staticmethod    def producer_work(device, cfg, data):        return data ** cfg.exponent  # calculate power of data with outside parameter \"exponent\".    @staticmethod    def consumer_init(cfg):        cfg.sum = 0  # init a sum variable with 0, you can use cfg to transfer data    @staticmethod    def consumer_work(cfg, data):        cfg.sum += data  # add the square to the sum variable    @staticmethod    def consumer_end(cfg):        return cfg.sum  # return the result you needif __name__ == '__main__':    # set parameters outside.    cfg = CN()    cfg.exponent = 3    runner = Runner(devices=3, cfg=cfg)  # transfer `cfg` into the runner         data_list = list(range(100))    result = runner(data_list)    print(result)    runner.close()\n\nAPI 文档\neasycore.common.parallel\neasycore.torch.parallel\n\n","thumbnail":"easycore-parallel-multi-gpu/python.jpg","plink":"https://yuxinzhao.net/easycore-parallel-multi-gpu/"},{"title":"发布第一个python包到PyPI","date":"2020-03-01T15:04:41.000Z","updated":"2022-01-04T08:35:48.623Z","content":"本文介绍如何将python包发布到PyPI上。\n\n\n关于setup.py\n确保你当前的根目录下能找到setup.py。\nversion 最好符合语义化版本号规则，如1.2.0\n\n模板:\n1234567891011121314151617from setuptools import setup, find_packagessetup(    name = '&lt;your package name&gt;',    version = '0.1.0',    author = '&lt;your name&gt;',    author_email = 'your@your_mail',    packages = find_packages(),    url = 'https://github.com/&lt;your repo&gt;',    license = 'LICENSE',  # refer to your LICENSE file    description = '&lt;short description&gt;',    long_description = open('README.md').read(),    python_requires = '&gt;=3',    install_requires = [        \"&lt;package required&gt; &gt;= 1.3.0\",    ])\n\n本地打包1python setup.py sdist bdist_wheel\n\n该命令会在本地建立dist目录用于存放生成的包。\n\nsdist 会生成一个包含源码的包，用户获取后会在本地重新编译构建。\nbdist_wheel 会生成一个预编译的.whl 文件。\n\n创建PyPI账号分别在官网和另一用于测试的网站注册一个账号，记得就算成功登陆了账号也要到注册的邮箱点开那个激活的邮件。\n创建用户验证文件在用户目录下新建文件 ~/.pypirc，加入以下内容:\n123456789101112[distutils]index-servers=\tpypi\ttestpypi\t[pypi]repository = https://upload.pypi.org/legacy/username = &lt;username in pypi.org&gt;[testpypi]repository = https://test.pypi.org/legacy/username = &lt;username in test.pypi.org&gt;\n\n上传到测试网站首先安装一个工具twine。\n1pip install twine\n\n接下来可以通过twine上传生成的包文件。\n123twine upload -r testpypi dist/*# ortwine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\n下载安装一下试试：\n1pip install -i https://test.pypi.org/simple &lt;your package&gt;\n\n确认没有问题、能够正常使用后我们就可以进行下一步了。\n上传到PyPI步骤同测试网站，但是目标地址换成了 https://upload.pypi.org/legacy/。\n123twine upload -r pypi dist/*# ortwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n\n接下来就可以愉快地从PyPI安装了。\n1pip install &lt;your package&gt;\n\n","thumbnail":"publish-first-python-package-to-pypi/logo.png","plink":"https://yuxinzhao.net/publish-first-python-package-to-pypi/"},{"title":"python多进程及pytorch多GPU并行推断","date":"2020-02-28T20:30:16.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文通过案例介绍我使用python自带的multiprocessing库进行多GPU并行推断的方案。\n\n\n阅前提示本文采用的方案已在Linux经过测试，但在Windows不可行，原因是Windows和Linux在多进程的实现上不同，Windows使用spawn模式，而Linux使用fork模式，这导致了Windows下使用multiprocessing存在着一些限制：\n\n必须保证传给 Process.__init__() 的所有参数均可pickle化。\n子进程访问父进程的全局变量时，得到的值并不能保证与父进程的全局变量相同(全局变量是模块级别除外)。\n必须确保在 if __name__ == &#39;__main__&#39;: 之后调用。\n更详细的说明见此处。\n\n注：本文代码不能在Windows上运行，原因是违反第一项限制，虽然也有方法可以让它兼容Windows，但是目前我使用多进程的场景基本都在Linux上，哪天有需要了再写个Windows兼容的版本，兼容Windows的实现已完成，请参见easycore多GPU并行加速。\n设计思路我的方案是使用多生产者-单消费者模型。\n一个简单的解释：众多生产者们不断地往仓库中放入产品(过程耗时长)，而有一个消费者将仓库中的产品一件件地放到自己的车上，拿到一定数量后整辆车拉走。\n整体流程如下：\n\n将总任务拆分成多个可以并行的子任务。\n通知消费者需要从仓库中拿走的数量(即子任务的数量)。\n将子任务一项项加入任务队列，生产者只要看到任务队列有任务就会竞争这个任务。\n生产者执行完任务后，将产品(子任务的结果)放入仓库，消费者一看到仓库中有产品(子任务的结果)就会放到自己手上(对子任务的结果进行汇总和后处理)。\n主进程将所有子任务都加入任务队列后，就会等待消费者完成，从消费者处获得最终结果。\n\n\n案例无序并行：求列表元素平方和我们先从一个简单的案例开始：求列表元素平方和。简单的案例更容易理解。\n我们将这个任务分成两部分：一个是对单个元素求平方，一个是将平方后的所有值相加。前者只需要部分数据，可独立运行，我们将这个任务分给生产者Sumer._TaskWorker；后者需要汇总所有前者生成的所有数据，我们将其分配给消费者Sumer._ReceiveWorker。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import multiprocessing as mpimport atexitclass Sumer:    class _StopToken:        pass    class _TaskWorker(mp.Process):        def __init__(self, input_queue, output_queue):            super(Sumer._TaskWorker, self).__init__()            self.input_queue = input_queue    # the queue to get task            self.output_queue = output_queue  # the queue to put result                    def run(self):            # initialization            # anything                        # loop to get and do the tasks            while True:                task = self.input_queue.get()  # pick a task from the queue                if isinstance(task, Sumer._StopToken):  # `_StopToken` is a signal to stop this worker                    break                                # decode task, it can be anything you defined.                num = task                                  # do the task. it's to get the suquare of the `num` here.                result = num * num                                # put the result into the queue                self.output_queue.put(result)      class _ReceiveWorker(mp.Process):        def __init__(self, receive_func, input_queue, output_queue):            super(Sumer._ReceiveWorker, self).__init__()            self.receive_func = receive_func  # the function to get the result from '_TaskWorker'            self.input_queue = input_queue              self.output_queue = output_queue        def run(self):            while True:                task = self.input_queue.get()                if isinstance(task, Sumer._StopToken):                    break                # decode task, get the number of tasks the worker will collect                length = task                # collect and postprocess the result from `_TaskWorker`                sum = 0                for _ in range(length):                    # collect data                    data = self.receive_func()                                        # postprocess data.                     sum += data                # put the final result into the queue                task = self.output_queue.put(sum)    def __init__(self, num_proc):        self.num_proc = num_proc  # number of process        self.task_worker_input_queue = mp.Queue(maxsize=self.num_proc * 3)        self.task_worker_output_queue = mp.Queue(maxsize=self.num_proc * 3)        self.receive_worker_input_queue = mp.Queue(maxsize=1)        self.receive_worker_output_queue = mp.Queue(maxsize=1)        # create workers        self.task_workers = []        for _ in range(self.num_proc):            self.task_workers.append(Sumer._TaskWorker(self.task_worker_input_queue, self.task_worker_output_queue))        self.receive_worker = Sumer._ReceiveWorker(self.get_from_task_worker, self.receive_worker_input_queue, self.receive_worker_output_queue)        # start workers        for worker in self.task_workers:            worker.start()        self.receive_worker.start()        atexit.register(self.shutdown)    def __del__(self):        self.shutdown()    def shutdown(self):        for _ in range(self.num_proc):            self.put_into_task_worker(Sumer._StopToken())        self.put_into_receive_worker(Sumer._StopToken())    def put_into_task_worker(self, data):        self.task_worker_input_queue.put(data)    def get_from_task_worker(self):        return self.task_worker_output_queue.get()    def put_into_receive_worker(self, data):        self.receive_worker_input_queue.put(data)        def get_from_receive_worker(self):        return self.receive_worker_output_queue.get()    def __call__(self, data_list):        \"\"\"        Args:            data_list (list[float]):                Returns:            float: sum of squares        \"\"\"        # inform the receive worker the number of data to receive        self.put_into_receive_worker(len(data_list))        # put data to task worker        for data in data_list:            self.put_into_task_worker(data)        # get result from receive worker        result = self.get_from_receive_worker()        return resultif __name__ == '__main__':    num_proc = 3    sumer = Sumer(num_proc)        data_list = list(range(1000))    result = sumer(data_list)    print(result)    del sumer  # delete the object to remove process manually, otherwise these process will be auto closed when the program exits.\n\n注：\n\n最后的 del 通过删除该对象可以手动关闭开的所有子进程，如果没有手动del的话，所有子进程直到主进程结束时才会自动关闭。\n传入的数据不能过多，多少合适呢？只要保证消费者对象Sumer._ReceiveWorker放到输出队列的结果别太大就行，这是因为python进程间的通信使用的是pickle序列化对象，而pickle限制了最大只能序列化4GB的对象。PS: easycore已克服该缺点。\n\n有序并行：求列表元素平方上个案例介绍了求列表元素平方和的案例，但在这过程中，每个生产者处理子任务的速度不能保证相同，所以时常会出现先接到任务的生成者后完成的情况，这种情况下，消费者从仓库中拿出产品的顺序就乱了，对于”求列表元素平方和“这一案例，顺序改变并没有影响，但对于部分任务，就必须要保证消费者处理的顺序。\n接下来通过一个看似更简单的案例：求列表元素平方，我们来实现对数据的有序接收。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154import multiprocessing as mpimport atexitimport bisectclass Squarer:    class _StopToken:        pass    class _TaskWorker(mp.Process):        def __init__(self, input_queue, output_queue):            super(Squarer._TaskWorker, self).__init__()            self.input_queue = input_queue    # the queue to get task            self.output_queue = output_queue  # the queue to put result                    def run(self):            # initialization            # anything                        # loop to get and do the tasks            while True:                task = self.input_queue.get()  # pick a task from the queue                if isinstance(task, Squarer._StopToken):  # `_StopToken` is a signal to stop this worker                    break                                # decode task, it can be anything you defined.                task_id, num = task                                  # do the task. it's to get the suquare of the `num` here.                result = num * num                                # put the result into the queue                self.output_queue.put((task_id, result))      class _ReceiveWorker(mp.Process):        def __init__(self, receive_func, input_queue, output_queue):            super(Squarer._ReceiveWorker, self).__init__()            self.receive_func = receive_func  # the function to get the result from '_TaskWorker'            self.input_queue = input_queue              self.output_queue = output_queue        def run(self):            while True:                task = self.input_queue.get()                if isinstance(task, Squarer._StopToken):                    break                # decode task, get the number of tasks the worker will collect                length = task                # collect and postprocess the result from `_TaskWorker`                data_list = []                for _ in range(length):                    # collect data                    data = self.receive_func()                                        # postprocess data.                     data_list.append(data)                # put the final result into the queue                task = self.output_queue.put(data_list)    def __init__(self, num_proc):        self.num_proc = num_proc  # number of process        self.put_id = 0        self.get_id = 0        self.id_buffer = []        self.data_buffer = []        self.task_worker_input_queue = mp.Queue(maxsize=self.num_proc * 3)        self.task_worker_output_queue = mp.Queue(maxsize=self.num_proc * 3)        self.receive_worker_input_queue = mp.Queue(maxsize=1)        self.receive_worker_output_queue = mp.Queue(maxsize=1)        # create workers        self.task_workers = []        for _ in range(self.num_proc):            self.task_workers.append(Squarer._TaskWorker(self.task_worker_input_queue, self.task_worker_output_queue))        self.receive_worker = Squarer._ReceiveWorker(self.get_from_task_worker, self.receive_worker_input_queue, self.receive_worker_output_queue)        # start workers        for worker in self.task_workers:            worker.start()        self.receive_worker.start()        atexit.register(self.shutdown)    def __del__(self):        self.shutdown()    def shutdown(self):        for _ in range(self.num_proc):            self.task_worker_input_queue.put(Squarer._StopToken())        self.receive_worker_input_queue.put(Squarer._StopToken())    def put_into_task_worker(self, data):        task_id = self.put_id        self.put_id += 1        self.task_worker_input_queue.put((task_id, data))    def get_from_task_worker(self):        if len(self.id_buffer) and self.id_buffer[0] == self.get_id:            data = self.data_buffer[0]            del self.id_buffer[0], self.data_buffer[0]            self.get_id += 1            return data                while True:            task_id, data = self.task_worker_output_queue.get()            if task_id == self.get_id:                self.get_id += 1                return data            insert_position = bisect.bisect(self.id_buffer, task_id)            self.id_buffer.insert(insert_position, task_id)            self.data_buffer.insert(insert_position, data)        def put_into_receive_worker(self, data):        self.receive_worker_input_queue.put(data)        def get_from_receive_worker(self):        return self.receive_worker_output_queue.get()    def __call__(self, data_list):        \"\"\"        Args:            data_list (list[float]):                Returns:            list[float]: list of square of the data        \"\"\"        # inform the receive worker the number of data to receive        self.put_into_receive_worker(len(data_list))        # put data to task worker        for data in data_list:            self.put_into_task_worker(data)        # get result from receive worker        result = self.get_from_receive_worker()        return resultif __name__ == '__main__':    num_proc = 3    squarer = Squarer(num_proc)        data_list = list(range(100))    result = squarer(data_list)    print(result)    del squarer  # delete the object to remove process manually, otherwise these process will be auto closed when the program exits.\n\n注：\n\n上述代码主要修改了Squarer.get_from_task_worker部分，使其获取到的结果不是当前想要的下一个结果时，先将该结果缓存起来，直到获取到想要的结果，后续需要该结果时就可以直接从缓存中将其取出。这样的好处是不会导致生产者的输出队列积累过多结果导致死锁。\n\n多GPU并行：推断以前面有序并行的案例为蓝本，我们来实现将pytorch模型并行到多个gpu上。\n首先我们先来设计一个简单的模型：\nnetwork.py12345678910111213import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.fc = nn.Linear(1, 3)        def forward(self, x):        x = self.fc(x)        x = F.relu(x)        return x\n\n接下来将网络并行到多个设备上：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163import torch.multiprocessing as mpimport atexitimport bisectfrom network import Netimport torchclass Predictor:    class _StopToken:        pass    class _TaskWorker(mp.Process):        def __init__(self, input_queue, output_queue, device):            super(Predictor._TaskWorker, self).__init__()            self.input_queue = input_queue    # the queue to get task            self.output_queue = output_queue  # the queue to put result            self.device = device                    def run(self):            # initialization            model = Net()            model.to(self.device)                        # loop to get and do the tasks            while True:                task = self.input_queue.get()  # pick a task from the queue                if isinstance(task, Predictor._StopToken):  # `_StopToken` is a signal to stop this worker                    break                                # decode task, it can be anything you defined.                task_id, x = task                                  with torch.no_grad():                    # do the task                    x = x.to(self.device)                    output = model(x)                                        # put the result into the queue                    output = output.cpu()  # copy to cpu before send to another process                self.output_queue.put((task_id, output))      class _ReceiveWorker(mp.Process):        def __init__(self, receive_func, input_queue, output_queue):            super(Predictor._ReceiveWorker, self).__init__()            self.receive_func = receive_func  # the function to get the result from '_TaskWorker'            self.input_queue = input_queue              self.output_queue = output_queue        def run(self):            while True:                task = self.input_queue.get()                if isinstance(task, Predictor._StopToken):                    break                # decode task, get the number of tasks the worker will collect                length = task                # collect and postprocess the result from `_TaskWorker`                data_list = []                for _ in range(length):                    # collect data                    data = self.receive_func()                                        # postprocess data.                     data_list.append(data)                # put the final result into the queue                data = torch.cat(data_list, dim=0)                task = self.output_queue.put(data)    def __init__(self, device_list):        self.num_proc = len(device_list)  # number of process        self.put_id = 0        self.get_id = 0        self.id_buffer = []        self.data_buffer = []        self.task_worker_input_queue = mp.Queue(maxsize=self.num_proc * 3)        self.task_worker_output_queue = mp.Queue(maxsize=self.num_proc * 3)        self.receive_worker_input_queue = mp.Queue(maxsize=1)        self.receive_worker_output_queue = mp.Queue(maxsize=1)        # create workers        self.task_workers = []        for device in device_list:            self.task_workers.append(Predictor._TaskWorker(self.task_worker_input_queue, self.task_worker_output_queue, device))        self.receive_worker = Predictor._ReceiveWorker(self.get_from_task_worker, self.receive_worker_input_queue, self.receive_worker_output_queue)        # start workers        for worker in self.task_workers:            worker.start()        self.receive_worker.start()        atexit.register(self.shutdown)    def __del__(self):        self.shutdown()    def shutdown(self):        for _ in range(self.num_proc):            self.task_worker_input_queue.put(Predictor._StopToken())        self.receive_worker_input_queue.put(Predictor._StopToken())    def put_into_task_worker(self, data):        task_id = self.put_id        self.put_id += 1        self.task_worker_input_queue.put((task_id, data))    def get_from_task_worker(self):        if len(self.id_buffer) and self.id_buffer[0] == self.get_id:            data = self.data_buffer[0]            del self.id_buffer[0], self.data_buffer[0]            self.get_id += 1            return data                while True:            task_id, data = self.task_worker_output_queue.get()            if task_id == self.get_id:                self.get_id += 1                return data            insert_position = bisect.bisect(self.id_buffer, task_id)            self.id_buffer.insert(insert_position, task_id)            self.data_buffer.insert(insert_position, data)        def put_into_receive_worker(self, data):        self.receive_worker_input_queue.put(data)        def get_from_receive_worker(self):        return self.receive_worker_output_queue.get()    def __call__(self, data_list):        \"\"\"        Args:            data_list (list[float]): input data list                Returns:            torch.FloatTensor:        \"\"\"        # inform the receive worker the number of data to receive        self.put_into_receive_worker(len(data_list))        # put data to task worker        for data in data_list:            data = torch.Tensor([[data]])  # preprocess data            self.put_into_task_worker(data)        # get result from receive worker        result = self.get_from_receive_worker()        return resultif __name__ == '__main__':    device_list = [\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"]    predictor = Predictor(device_list)        data_list = list(range(100))    result = predictor(data_list)    print(result.shape)    del predictor  # delete the object to remove process manually, otherwise these process will be auto closed when the program exits.\n\n注：\n\n首先，这里将python自带的multiprocessing库替换成了torch.multiprocessing，这是因为pytorch官方对multiprocessing做了一层封装，使得不同进程之间可以共享Tensor，完全兼容multiprocessing的接口。本例中将torch.multiprocessing替换回multiprocessing并无影响。\n生产者Predictor._TaskWorker在将结果放进输出队列之前，最好将结果从GPU移到CPU(实际上是内存)，以避免GPU上的Tensor在不同进程间移动。\n通过device_list = [&quot;cuda:0&quot;, &quot;cuda:1&quot;] 可以指定多个GPU，而使用device_list = [&quot;cpu&quot;, &quot;cpu&quot;]还可以实现多CPU并行 (在有多个CPU的情况下，系统会自动将进程分配给不同的CPU)，当然也可以混合使用CPU和GPU: device_list = [&quot;cpu&quot;, &quot;cuda:0&quot;]。\n\n","thumbnail":"python-multi-gpu-multiprocessing/python.jpg","plink":"https://yuxinzhao.net/python-multi-gpu-multiprocessing/"},{"title":"使用manim制作数学动画","date":"2020-02-14T15:52:55.000Z","updated":"2022-01-04T08:35:48.599Z","content":"本文介绍如何使用manim制作动画。\n\n\n\n\n基本结构12345from manimlib.imports import *class NameOfScene(Scene):    def construct(self):        # Animation progress\n\n基本运行方式1python -m manim example_scenes.py NameOfScene -pl\n\n\np 表示preview，即在渲染完成后立即播放\nl 表三 low quality ，以 480p 15fps 渲染\n\n更多选项：\n12345678910111213141516-h,    --help show this help message and exit-p,    --preview-w,    --write_to_movie-s,    --show_last_frame-l,    --low_quality-m,    --medium_quality-g,    --save_pngs-f,    --show_file_in_finder-t,    --transparent-q,    --quiet-a,    --write_all-o OUTPUT_NAME,    --output_name OUTPUT_NAME-n START_AT_ANIMATION_NUMBER,    --start_at_animation_number START_AT_ANIMATION_NUMBER-r RESOLUTION,    --resolution RESOLUTION-c COLOR,    --color COLOR-d OUTPUT_DIRECTORY,    --output_directory OUTPUT_DIRECTORY\n\n基本形状点1234class TutorialShapeDot(Scene):    def construct(self):        dot = Dot(ORIGIN, buff=0.1)        self.add(dot)\n\n线段1234class TutorialShapeLine(Scene):    def construct(self):        line = Line(LEFT, ORIGIN)        self.add(line)\n\n圆弧1234class TutorialShapeArc(Scene):    def construct(self):        arc = Arc(0, PI/2).scale(2)        self.add(arc)\n\n圆形1234class TutorialShapeCircle(Scene):    def construct(self):        circle = Circle()        self.add(circle)\n\n方形12345class TutorialShapeSquare(Scene):    def construct(self):        square = Square()        square.move_to(LEFT)        self.add(square)\n\n矩形123456class TutorialShapeRectangle(Scene):    def construct(self):        rectangle = Rectangle()        rectangle.set_width(2)        rectangle.set_height(1)        self.add(rectangle)\n\n多边形1234class TutorialShapePolygon(Scene):    def construct(self):        polygon = Polygon(np.array([-1,0,0]), np.array([1,0,0]), np.array([2,1,0]), np.array([2,2,0]))        self.add(polygon)\n\n箭头1234class TutorialShapeArrow(Scene):    def construct(self):        arrow = Arrow().scale(1.5)        self.add(arrow)\n\n双向箭头12345class TutorialShapeDoubleArrow(Scene):    def construct(self):        double_arrow = DoubleArrow().scale(1.5)        double_arrow.move_to(UR)         self.add(double_arrow)\n\n向量12345class TutorialShapeVector(Scene):    def construct(self):        vector = Vector().scale(1.5)        vector.move_to(DR)         self.add(vector)\n\n数学公式12345class TutorialShapeTex(Scene):    def construct(self):        tex = TexMobject(r\"\\frac&#123;d&#125;&#123;dx&#125; f(x)g(x)\").set_color(RED)        tex.move_to(UP)        self.add(tex)\n\n文本123456789class TutorialShapeText(Scene):    def construct(self):        text = TextMobject(r\"Text with LaTeX \n\\frac{1}{2}\n\n\n\n\n\n\n\n \n \n\n\n\", color=RED)        text.move_to(DOWN)        self.add(text)        text = Text(r\"Text without LaTeX 1/2\", color=RED)        text.move_to(UP)        self.add(text)\n\n基本动画淡入 / 淡出12345678910111213class AnimationFadeInOut(GraphScene):    CONFIG = &#123;  # 设置坐标轴        \"x_min\": -5,        \"x_max\": 5,        \"y_min\": -4,        \"y_max\": 4,        \"graph_origin\": ORIGIN,  # 原点    &#125;    def construct(self):        self.setup_axes() # 显示坐标轴        polygon = Polygon(np.array([-1,0,0]), np.array([1,0,0]), np.array([2,1,0]), np.array([2,2,0]))        self.play(FadeIn(polygon), run_time=2)        self.play(FadeOut(polygon), run_time=2)\n\n显示生成过程 (描边)123456789101112class AnimationShowCreation(GraphScene):    CONFIG = &#123;  # 设置坐标轴        \"x_min\": -5,        \"x_max\": 5,        \"y_min\": -4,        \"y_max\": 4,        \"graph_origin\": ORIGIN,  # 原点    &#125;    def construct(self):        self.setup_axes() # 显示坐标轴        polygon = Polygon(np.array([-1,0,0]), np.array([1,0,0]), np.array([2,1,0]), np.array([2,2,0]))        self.play(ShowCreation(polygon))\n\n平移1234567891011121314151617class AnimationMove(GraphScene):    CONFIG = &#123;  # 设置坐标轴        \"x_min\": -5,        \"x_max\": 5,        \"y_min\": -4,        \"y_max\": 4,        \"graph_origin\": ORIGIN,  # 原点    &#125;    def construct(self):        self.setup_axes() # 显示坐标轴        square = Square()        self.play(ShowCreation(square))        self.play(            square.move_to, DOWN,            rate_func=linear,            run_time=1        )\n\n旋转12345678910111213class AnimationRotate(GraphScene):    CONFIG = &#123;  # 设置坐标轴        \"x_min\": -5,        \"x_max\": 5,        \"y_min\": -4,        \"y_max\": 4,        \"graph_origin\": ORIGIN,  # 原点    &#125;    def construct(self):        self.setup_axes() # 显示坐标轴        square = Square()        self.play(ShowCreation(square))        self.play(Rotate(square, PI, IN))  # 第二个参数为旋转角度，第三个参数为旋转轴(IN表示垂直屏幕向里), 旋转的方向为右手螺旋\n\n变换12345678910class AnimationTransform(Scene):    def construct(self):        square = Square()        circle1 = Circle()        circle2 = Circle()        circle1.set_color(GREEN)        circle2.set_color(RED)        circle2.next_to(circle1) # 让circle2在circle1右边        self.play(ShowCreation(circle1), ShowCreation(circle2)) # 同时播放多个动画        self.play(Transform(circle1, square)) # 变换过后 circle1和square是同一个对象\n\nWrite1234567class AnimationWrite(Scene):    def construct(self):        square = Square()        square.set_color(WHITE)        square.set_fill(RED, opacity=0.5)        self.play(Write(square))        self.wait(2)\n\nupdater123456789101112131415class UpdaterExample(Scene):    def construct(self):        text = Text(\"text\")        dot = Dot()        text.next_to(dot)        def update(obj):            obj.next_to(dot) # 让obj近挨dot        text.add_updater(update) # 将该updater添加给text，即随着dot的移动，text会紧跟dot，保持相对静止        self.add(dot)        self.add(text)        self.play(dot.move_to, DOWN, run_time=2)                text.remove_updater(update) # 将updater从text中移除\n\n绘制函数曲线123456789101112class AnimationFunction(GraphScene):    CONFIG = &#123;  # 设置坐标轴        \"x_min\": -5,        \"x_max\": 5,        \"y_min\": -4,        \"y_max\": 4,        \"graph_origin\": ORIGIN,  # 原点    &#125;    def construct(self):        self.setup_axes() # 显示坐标轴        graph = self.get_graph(lambda x: x**2, color = GREEN)        self.play(ShowCreation(graph))\n\n外部导入SVG123456class OutsideSVGImage(Scene):    def construct(self):        svg_image = SVGMobject(\"test.svg\")        svg_image_scale = svg_image.copy().scale(2)        self.play(ShowCreation(svg_image), run_time=2)        self.play(Transform(svg_image, svg_image_scale))\n\n","thumbnail":"manim/manim-logo.png","plink":"https://yuxinzhao.net/manim/"},{"title":"linux上安装docker CE","date":"2020-01-17T21:39:02.000Z","updated":"2022-01-04T08:35:48.567Z","content":"本文介绍在linux系统上安装docker CE，官方手册见此处。\n\n\n设置仓库注: 此”仓库”不是指github仓库，而是apt的软件源。\n\n安装一些必要的库\n1234567sudo apt-get updatesudo apt-get install \\    apt-transport-https \\    ca-certificates \\    curl \\    gnupg-agent \\    software-properties-common\n\n添加docker官方的GPG密钥\n1curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\n加入docker的仓库\n机器是 x86_64/amd64 架构的，指令如下：（其他架构见官方手册）\n1234sudo add-apt-repository \\   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\   $(lsb_release -cs) \\   stable\"\n\n\n\n安装12sudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd.io\n\n","thumbnail":"docker-linux-install/logo.jpg","plink":"https://yuxinzhao.net/docker-linux-install/"},{"title":"frp内网穿透基本使用","date":"2020-01-17T21:06:04.000Z","updated":"2022-01-04T08:35:48.591Z","content":"每次回家连不上学校内网，就没法连接实验室的服务器做实验，有什么办法解决呢？内网穿透是解决这一问题的方法之一。\n本文介绍 frp 内网穿透工具的基础使用，仅涉及 frp 的 ssh 访问功能，更多功能见官方仓库。\n\n\n前提\n一台公网服务器（有公网ip）(没有怎么办？在腾讯云或者百度云上买学生服务器)\n一台内网服务器（实验室的服务器）\n\n下载从github仓库的release中下载最新版本解压即可，公网服务器和内网服务器上都要下载。\n也可以用指令下载：\n12wget https://github.com/fatedier/frp/releases/download/v0.31.1/frp_0.31.1_linux_amd64.tar.gztar -zxvf frp_0.31.1_linux_amd64.tar.gz\n\n基础使用公网服务器设置修改 frps.ini 文件。\nfrps.ini12[common]bind_port = 7000\n\n启动\n1./frps -c frps.ini\n\n若想后台运行该指令，详见文章 Linux nohup 后台执行指令。\n内网服务器设置修改 frpc.ini 文件\nfrpc.ini123456789[common]server_addr = x.x.x.x  # 你的公网服务器的ipserver_port = 7000     # 对应公网服务器上frps.ini里bind_port的端口[ssh]type = tcplocal_ip = 127.0.0.1local_port = 22remote_port = 6000     # 之后访问ssh时使用的端口号\n\n启动\n1./frpc -c frpc.ini\n\n使用1ssh -oPort=6000 username@x.x.x.x\n\n","thumbnail":"frp-basic/architecture.png","plink":"https://yuxinzhao.net/frp-basic/"},{"title":"Ubuntu上从CUDA开始构建深度学习镜像","date":"2020-01-14T15:53:47.000Z","updated":"2022-01-04T08:35:48.567Z","content":"本文介绍如何在ubuntu上从CUDA镜像开始构建深度学习镜像。\n\n\n前置条件\n已安装 docker 和 nvidia-docker\n安装好 nvidia driver （不要求安装 cuda 和 cudnn）\n\n构建下载 cuda 镜像1sudo docker pull nvidia/cuda:9.2-cudnn7-devel\n\n更多cuda版本见此处。\n注意： \n\n下载cuda版本前请先确认宿主机的 nvidia driver 版本高于要安装的 cuda 版本要求，对应关系见此处。\n\nnvidia driver的版本可以通过 nvidia-smi 指令查看。\n\n\n下载完成后使用以下命令验证是否安装成功：\n12sudo nvidia-docker run --rm nvidia/cuda:9.2-cudnn7-devel nvidia-smisudo nvidia-docker run --rm nvidia/cuda:9.2-cudnn7-devel nvcc --version\n\n若nvidia driver版本不满足要求，下载完成后运行会出现以下错误：\n1docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused &quot;process_linux.go:430: container init caused \\&quot;process_linux.go:413: running prestart hook 1 caused \\\\\\&quot;error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda&gt;=10.1 brand=tesla,driver&gt;=384,driver&lt;385 brand=tesla,driver&gt;=396,driver&lt;397 brand=tesla,driver&gt;=410,driver&lt;411 --pid=7808 /home/data/overlay2/5404e537b8e59d7717372339aa3c739b7dc498fe8dc00ca16b69149edab4a498/merged]\\\\\\\\nnvidia-container-cli: requirement error: unsatisfied condition: brand = tesla\\\\\\\\n\\\\\\&quot;\\&quot;&quot;: unknown.\n\n启动cuda容器1sudo nvidia-docker run --name container_name -it nvidia/cuda:9.2-cudnn7-devel /bin/bash\n\n注： \n\ncontainer_name 可以自己改。\n\n如果中途退出该容器了，可以通过以下指令进入：\n12sudo nvidia-docker start container_namesudo nvidia-docker attach container_name\n\n\n\n\n\n\n\napt-get 换源：操作指南见此处。\n12345apt-get update        # 刷新原本的软件源apt-get install vim   # 安装 vim# 按照操作指南更换 apt-get 软件源，可以使用 vi 命令编辑 /etc/apt/sources.listapt-get update        # 更新为清华镜像源apt-get install wget  # 安装 wget\n\n注：必须先 apt-get update 否则软件源不会更新，会出现以下错误：\n1E: Unable to locate package vim\n\n安装 anaconda3注：也可以选择更小的版本miniconda。\n在anaconda官网复制安装脚本的链接地址，使用 wget 下载：\n12cd /homewget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\n\n下载后运行脚本即可安装：\n12chmod +x Anaconda3-2019.10-Linux-x86_64.sh./Anaconda3-2019.10-Linux-x86_64.sh\n\n安装完成后退出，使安装生效：\n1exit\n\n重新进入：\n12sudo nvidia-docker start container_namesudo nvidia-docker attach container_name\n\n验证是否成功安装：\n1conda --version\n\npip 换源操作指南见此处。\nconda 换源1vi ~/.condarc   # 没有就新建\n\n添加以下内容\n1234567channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r  - defaults  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorchshow_channel_urls: true\n\n设置jupyter notebook注： 由于anaconda自带jupyter，此处略去jupyter的安装步骤。\n设置密码：\n12jupyter notebook --generate-configjupyter notebook password\n\n后台启动notebook：\n1jupyter notebook /base_dir --ip=0.0.0.0 --port=8888 --allow-root &amp;\n\n注： 使用 CTRL + P + Q 可以退出容器但不关闭容器，保持jupyter的运行\n安装pytorch1conda install pytorch torchvision cudatoolkit=9.2  # 不需要加 -c pytorch\n\n打包成镜像先退出容器，执行以下命令打包镜像\n1sudo docker commit -m=\"comment\" -a=\"author_name\" container_id image_name:tag\n\n使用1sudo nvidia-docker run --shm-size 20G -p 9888:8888 -v /src_dir:/dst_dir --name container_name -it image_name:tag /bin/bash\n\n\n--shm-size 用于指定共享内存的大小，默认只有 64M。\n\n使用中可以用 CTRL + P + Q 退出容器但不关闭容器。\n使用以下命令可以在容器上另外开启一个终端，该终端中可以直接 exit 而不会影响容器运行:\n1sudo docker exec -it container_name /bin/bash\n\n","thumbnail":"create-deep-learning-docker-image-in-ubuntu/nvidia-docker-logo.png","plink":"https://yuxinzhao.net/create-deep-learning-docker-image-in-ubuntu/"},{"title":"python的奇妙技巧","date":"2020-01-12T14:47:26.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文记录平时阅读大牛的python项目代码时看到的一些奇妙的技巧， 同时也记录一些常用(boring)的技巧，持续更新中…\n本文涉及的技巧除了纯python的技巧，还包含以下常用python库的技巧：\n\nnumpy\npytorch\nmultiprocessing\n\n\n\n\n\nnumpy反转 numpy array 某一维度123A = np.random.rand(3,3,3)A = A[:,:,::-1]  # 反转第三维度A = A[:,::-1,:]  # 反转第二维度\n\n该方法可以用于：\n\n反转图像通道RGB到BGR\n反转图像x，y轴\n\n该方法同样使用于 pytorch, tensorflow 等库\nnumpy array 与 python list 的相互转换1234python_list = [[1,2,3],[4,5,6]]         # a python listnumpy_array = np.array(python_list)     # list --&gt; arraynumpy_array = np.asarray(python_list)   # list --&gt; arraypython_list = numpy_array.tolist()      # array --&gt; list\n\nnp.where() 向量化 if-else 结构123A = np.array([2, 3, 5, 6])B = np.array([1, 2, 3, 4])C = np.array([5, 6, 7, 8])\n\n对于上述数组，假设我们想要实现以下逻辑：\n12345for i in range(A.shape[0]):    if A[i] % 2 == 0:        D[i] = B[i]    else:        D[i] = C[i]\n\n使用 np.where() 可以向量化上述过程：\n1D = np.where(A % 2 == 0, B, C)\n\nnp.select() 向量化 if-elif-else 结构1A = np.array([2, 4, 6, 7, 8, 9])\n\n假设我们想实现以下逻辑：\n123456789for i in range(A.shape[0]):    if A[i] % 4 == 0:        B[i] = 4    elif A[i] % 3 == 0:        B[i] = 3    elif A[i] % 2 == 0:        B[i] = 2    else:        B[i] = 1\n\n使用 np.select() 可以向量化上述过程：\n12345678910111213conditions = [    A % 4 == 0,    A % 3 == 0,    A % 2 == 0]choices = [    4,    3,    2]B = np.select(conditions, choices, default = 1)\n\n对于嵌套的 if-elif-else 结构，我们依然可以用 np.select 将其向量化，如：\n1234567891011121314for i in range(A.shape[0]):    if A[i] % 2 == 0:        if A[i] &lt;= 4:            B[i] = 4        elif A[i] &gt; 8:            B[i] = 8        else:            B[i] = 2    elif A[i] % 3 == 0:        B[i] = 3    elif A[i] % 4 == 0:        B[i] = 4    else:        B[i] = 1\n\n可以将其向量化为以下形式：\n1234567891011121314151617conditions = [    ((A % 2 == 0) and (A &lt;= 4)),    ((A % 2 == 0) and (A &gt; 8)),    A % 2 == 0，    A % 3 == 0,    A % 4 == 0]choices = [    4,    8,    2,    3,    4]B = np.select(conditions, choices, default = 1)\n\nnp.vectorize() 向量化普通 python 函数12A = np.array([1, 3, 5])B = np.array([1, 2, 3])\n\n12345def python_func(a, b):    if a &gt; b:        return a + b    else:        return b - a\n\n假设我们想实现的逻辑如下：\n12for i in range(A.shape[0]):    C[i] = python_func(A[i], B[i])\n\n该过程可以用 np.vectorize() 向量化：\n12vec_func = np.vectorize(python_func)C = vec_func(A, B)\n\nnp.vectorize() 将python函数向量化后可以达到 C 语言循环的速度。\n根据条件修改图片中满足条件的像素值假定我们想将RGB图片中R大于180且B大于200的的像素点的数值改成R=180,G=190,B=200。\n123print(img.shape)  # (512,512,3)img = np.where(np.repeat(np.bitwise_and(img[:,:,0] &gt; 180, img[:,:,2] &gt; 200)[:,:,None], 3, axis=2), np.array([180,190,200], dtype=np.uint8).reshape(1,1,3), img)\n\npytorchtorch tensor 与 python list 的相互转换123python_list = [[1,2,3],[4,5,6]]           # a python listtorch_tensor = torch.Tensor(python_list)  # list --&gt; tensorpython_list = torch_tensor.tolist()       # tensor --&gt; list\n\ntorch tensor 与 numpy array 的相互转换123numpy_array = np.array([[1,2,3],[4,5,6]])     # a numpy arraytorch_tensor = torch.from_numpy(numpy_array)  # array --&gt; tensornumpy_array = torch_tensor.numpy()            # tensor --&gt; array\n\n注意： 以上转换共用一个内存，即改变其中一个的值，另一个也会改变。\nmultiprocessing多进程加速对于某些需要处理大批量数据的函数，可以通过将数据拆分后在多个CPU上并行执行进行加速：\n123456789from multiprocessing import Pooldef parallel_apply(data, func, num_cores=4):    data_split = np.array_split(data, num_cores)    pool = Pool(num_cores)    data = np.concatenate(pool.map(func, data_split))    pool.close()    pool.join()    return data\n\n","thumbnail":"python-skill/python.jpg","plink":"https://yuxinzhao.net/python-skill/"},{"title":"PASCAL VOC 数据集格式","date":"2020-01-07T15:18:30.000Z","updated":"2022-01-04T08:35:48.615Z","content":"本文介绍 PASCAL VOC 数据集的标注格式。\n\nPASCAL的全称是Pattern Analysis, Statistical Modelling and Computational Learning\n\nVOC的全称是Visual Object Classes\n\n\n详细的官方说明见此处。\n\n\n\n\n数据集目录结构123456789101112.└── VOCdevkit     #根目录    └── VOC2012   #不同年份的数据集，目前最新是2012        ├── Annotations        #存放xml文件，与JPEGImages中的图片一一对应，解释图片的内容等等        ├── ImageSets          #该目录下存放的都是txt文件，txt文件中每一行包含一个图片的名称，末尾会加上±1表示正负样本        │   ├── Action        │   ├── Layout        │   ├── Main        │   └── Segmentation        ├── JPEGImages         #存放源图片        ├── SegmentationClass  #存放的是图片，语义分割相关        └── SegmentationObject #存放的是图片，实例分割相关\n\n\n必备的目录只有Annotations, JPEGImages 和 ImageSets/Main。\nAnnotations 文件夹存放的是xml文件，该文件是对图片的解释，每张图片都对于一个同名的xml文件。\nImageSets文件夹存放的是txt文件，这些txt将数据集的图片分成了各种集合。如Main下的train.txt中记录的是用于训练的图片集合\nJPEGImages文件夹存放的是数据集的原图片\nSegmentationClass以及SegmentationObject文件夹存放的都是图片，且都是图像分割结果图\n\nAnnotations 目录12345678Annotations/├── 2007_000027.xml├── 2007_000032.xml├── 2007_000033.xml├── 2007_000039.xml├── 2007_000042.xml...└── 2012_004331.xml\n\nAnnotations包含一系列标注文件。对于目标检测来说，每一张图片对应一个xml格式的标注文件。\n下面是其中一个xml文件的示例：\nAnnotations/2007_000027.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;annotation&gt;\t&lt;folder&gt;VOC2012&lt;/folder&gt;  #表明图片来源\t&lt;filename&gt;2007_000027.jpg&lt;/filename&gt; #图片名称\t&lt;source&gt;                  #图片来源相关信息\t\t&lt;database&gt;The VOC2007 Database&lt;/database&gt;\t\t&lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;\t\t&lt;image&gt;flickr&lt;/image&gt;\t&lt;/source&gt;\t&lt;size&gt;     #图像尺寸\t\t&lt;width&gt;486&lt;/width&gt;\t\t&lt;height&gt;500&lt;/height&gt;\t\t&lt;depth&gt;3&lt;/depth&gt;\t&lt;/size&gt;\t&lt;segmented&gt;0&lt;/segmented&gt; #是否用于分割\t&lt;object&gt;  #包含的物体\t\t&lt;name&gt;person&lt;/name&gt; #物体类别\t\t&lt;pose&gt;Unspecified&lt;/pose&gt;\t\t&lt;truncated&gt;0&lt;/truncated&gt;\t\t&lt;difficult&gt;0&lt;/difficult&gt;\t\t&lt;bndbox&gt;  #物体的bbox\t\t\t&lt;xmin&gt;174&lt;/xmin&gt;\t\t\t&lt;ymin&gt;101&lt;/ymin&gt;\t\t\t&lt;xmax&gt;349&lt;/xmax&gt;\t\t\t&lt;ymax&gt;351&lt;/ymax&gt;\t\t&lt;/bndbox&gt;\t\t&lt;part&gt; #物体的头\t\t\t&lt;name&gt;head&lt;/name&gt;\t\t\t&lt;bndbox&gt;\t\t\t\t&lt;xmin&gt;169&lt;/xmin&gt;\t\t\t\t&lt;ymin&gt;104&lt;/ymin&gt;\t\t\t\t&lt;xmax&gt;209&lt;/xmax&gt;\t\t\t\t&lt;ymax&gt;146&lt;/ymax&gt;\t\t\t&lt;/bndbox&gt;\t\t&lt;/part&gt;\t\t&lt;part&gt;   #物体的手\t\t\t&lt;name&gt;hand&lt;/name&gt;\t\t\t&lt;bndbox&gt;\t\t\t\t&lt;xmin&gt;278&lt;/xmin&gt;\t\t\t\t&lt;ymin&gt;210&lt;/ymin&gt;\t\t\t\t&lt;xmax&gt;297&lt;/xmax&gt;\t\t\t\t&lt;ymax&gt;233&lt;/ymax&gt;\t\t\t&lt;/bndbox&gt;\t\t&lt;/part&gt;\t\t&lt;part&gt;\t\t\t&lt;name&gt;foot&lt;/name&gt;\t\t\t&lt;bndbox&gt;\t\t\t\t&lt;xmin&gt;273&lt;/xmin&gt;\t\t\t\t&lt;ymin&gt;333&lt;/ymin&gt;\t\t\t\t&lt;xmax&gt;297&lt;/xmax&gt;\t\t\t\t&lt;ymax&gt;354&lt;/ymax&gt;\t\t\t&lt;/bndbox&gt;\t\t&lt;/part&gt;\t\t&lt;part&gt;\t\t\t&lt;name&gt;foot&lt;/name&gt;\t\t\t&lt;bndbox&gt;\t\t\t\t&lt;xmin&gt;319&lt;/xmin&gt;\t\t\t\t&lt;ymin&gt;307&lt;/ymin&gt;\t\t\t\t&lt;xmax&gt;340&lt;/xmax&gt;\t\t\t\t&lt;ymax&gt;326&lt;/ymax&gt;\t\t\t&lt;/bndbox&gt;\t\t&lt;/part&gt;\t&lt;/object&gt;&lt;/annotation&gt;\n\n\nbndbox 是一个轴对齐的矩形，它框住的是目标在照片中的可见部分\ntruncated 表明这个目标因为各种原因没有被框完整（被截断了），比如说一辆车有一部分在画面外\noccluded 是说一个目标的重要部分被遮挡了（不管是被背景的什么东西，还是被另一个待检测目标遮挡）\ndifficult 表明这个待检测目标很难识别，有可能是虽然视觉上很清楚，但是没有上下文的话还是很难确认它属于哪个分类；标为difficult的目标在测试成绩的评估中一般会被忽略。\n\n注意：在一个&lt;object /&gt;中，&lt;name /&gt; 标签要放在前面，否则的话，目标检测的一个重要工程会出现解析数据集错误\nImageSets 目录123456789101112ImageSets/├── Action├── Layout├── Main│   ├── person_train.txt│   ├── person_trainval.txt│   ├── person_val.txt│   ...│   ├── train.txt│   ├── trainval.txt│   └── val.txt    └── Segmentation\n\n各文件夹存放着各种用途的 TXT 文件。\n我们着重介绍Main子目录下的 TXT 文件格式，该文件下的数据分为两种：\n\nXXX_train.txt,  XXX_trainval.txt,  XXX_val.txt 为类别XXX的数据集划分文件\nImageSets/Main/person_val.txt123456789102008_000002 -12008_000003  12008_000007 -12008_000009 -12008_000016 -12008_000021 -12008_000026  12008_000027 -12008_000032  1...\n\n\n每一行有两项，\n第一项为数据实例的名称，如 2008_000002 对应着 Annotations/2008_000002.xml 和 JPEGImages/2008_000002.jpg\n第二项为1或者-1，表示正类或者负类\n\n\n\n\ntrain.txt，trainval.txt，val.txt 为所有类别的数据集划分文件\nImageSets/Main/val.txt123456782008_0000022008_0000032008_0000072008_0000092008_0000162008_0000212008_000026...\n\n\n每一行为数据实例的名称，如 2008_000002 对应着 Annotations/2008_000002.xml 和 JPEGImages/2008_000002.jpg\n\n\n\nJPEGImages 目录该文件夹存放数据集的所有源图片。\n1234567JPEGImages/├── 2007_000027.jpg├── 2007_000032.jpg├── 2007_000033.jpg...├── 2012_004330.jpg└── 2012_004331.jpg\n\n示例 (2007_000032)：\n\nSegmentationClass 目录该目录包含语义风格相关的图片\n1234567SegmentationClass/├── 2007_000032.jpg├── 2007_000033.jpg├── 2007_000039.jpg...├── 2011_003256.jpg└── 2011_003271.jpg\n\n示例 (2007_000032)：\n\nSegmentationObject 目录该目录包含实例分割相关的图片\n1234567SegmentationObject/├── 2007_000032.jpg├── 2007_000033.jpg├── 2007_000039.jpg...├── 2011_003256.jpg└── 2011_003271.jpg\n\n示例 (2007_000032)：\n\n参考资料\nPASCAL VOC 数据集详细分析\nPASCAL VOC 数据集的标注格式\n\n","thumbnail":"pascal-voc-dataset-format/pascal-logo.png","plink":"https://yuxinzhao.net/pascal-voc-dataset-format/"},{"title":"COCO数据集格式","date":"2020-01-07T13:33:56.000Z","updated":"2022-01-04T08:35:48.547Z","content":"本文介绍COCO数据集的标注格式。\nCOCO 全称为Common Objects in Context，是微软团队提供的一个可以用来进行图像识别的数据集。\nCOCO数据集目前有3种标注类型：\n\nobject instances (目标实例)\nobject keypoints (目标上的关键点)\nimage captions (看图说话)\n\nCOCO 数据集的信息采用JSON文件存储。\n\n\n\n\n基本的JSON结构体类型object instances (目标实例)，object keypoints (目标上的关键点),，image captions (看图说话) 这3种类型共享这些基本类型：info、image、license。而 annotation 类型则呈现出多态。\n123456789101112131415161718192021222324252627282930&#123;    \"info\": info,    \"licenses\": [license],    \"images\": [image],    \"annotations\": [annotation],&#125;    info&#123;    \"year\": int,    \"version\": str,    \"description\": str,    \"contributor\": str,    \"url\": str,    \"date_created\": datetime,&#125;license&#123;    \"id\": int,    \"name\": str,    \"url\": str,&#125; image&#123;    \"id\": int,    \"width\": int,    \"height\": int,    \"file_name\": str,    \"license\": int,    \"flickr_url\": str,    \"coco_url\": str,    \"date_captured\": datetime,&#125;\n\n\ninfo 类型，比如一个 info 类型的实例：\n1234567&#123;\t\"description\":\"This is stable 1.0 version of the 2014 MS COCO dataset.\",\t\"url\":\"http:\\/\\/mscoco.org\",\t\"version\":\"1.0\",\"year\":2014,\t\"contributor\":\"Microsoft COCO group\",\t\"date_created\":\"2015-01-27 09:11:52.357475\"&#125;\n\nimages 是包含多个 image 实例的数组，对于一个 image 类型的实例\n12345678&#123;\t\"license\":3,\t\"file_name\":\"COCO_val2014_000000391895.jpg\",\t\"coco_url\":\"http:\\/\\/mscoco.org\\/images\\/391895\",\t\"height\":360,\"width\":640,\"date_captured\":\"2013-11-14 11:18:45\",\t\"flickr_url\":\"http:\\/\\/farm9.staticflickr.com\\/8186\\/8119368305_4e622c8349_z.jpg\",\t\"id\":391895&#125;\n\nlicenses 是包含多个 license 实例的数组，对于一个 license 类型的实例\n12345&#123;\t\"url\":\"http:\\/\\/creativecommons.org\\/licenses\\/by-nc-sa\\/2.0\\/\",\t\"id\":1,\t\"name\":\"Attribution-NonCommercial-ShareAlike License\"&#125;\n\n\n\n\n\nObject Instance 类型的标注格式整体JSON文件格式1234567&#123;    \"info\": info,    \"licenses\": [license],    \"images\": [image],    \"annotations\": [annotation],    \"categories\": [category]&#125;\n\ninfo, licenses, image 这三个结构体在不同的标注类型是统一的，只有 annotation 和 category 这两个结构体在不同类型的JSON文件中是不同的。\n\nimages 数组元素的数量等同于划入训练集（或者测试集）的图片的数量\nannotations 数组元素的数量等同于训练集（或者测试集）中bounding box的数量\ncategories 数组元素的数量为分类类别的数量\n\nannotations 字段annotations 字段是包含多个 annotation 实例的一个数组，annotation 类型本身又包含了一系列的字段，如这个目标的 category id 和 segmentation mask 。segmentation 格式取决于这个实例是一个单个的对象（即 iscrowd=0，将使用polygons格式）还是一组对象（即 iscrowd=1，将使用RLE格式）。如下所示：\n123456789annotation&#123;    \"id\": int,        \"image_id\": int,    \"category_id\": int,    \"segmentation\": RLE or [polygon],    \"area\": float,    \"bbox\": [x,y,width,height],    \"iscrowd\": 0 or 1,&#125;\n\n\niscrowd=0 (单个对象) 时, segmentation 就是 polygon 格式，单个的对象可能需要多个polygon来表示，比如这个对象在图像中被挡住了。而iscrowd=1（将标注一组对象，比如一群人）时，segmentation使用的就是RLE格式。\narea 是area of encoded masks，是标注区域的面积。如果是矩形框，那就是高乘宽；如果是polygon或者RLE，那就复杂点。\ncategories 字段存储的是当前对象所属的category的id，以及所属的supercategory的name。\n\npolygon 格式 annotation 示例：\n123456789&#123;\t\"segmentation\": [[510.66,423.01,511.72,420.03,510.45......]],\t\"area\": 702.1057499999998,\t\"iscrowd\": 0,\t\"image_id\": 289343,\t\"bbox\": [473.07,395.93,38.65,28.67],\t\"category_id\": 18,\t\"id\": 1768&#125;\n\n\npolygon 格式比较简单，这些数按照相邻的顺序两两组成一个点的xy坐标，如果有n个数（必定是偶数），那么就是n/2个点坐标。\n\nRLE 格式 annotation 示例：\n12345\"segmentation\" : &#123;    u'counts': [272, 2, 4, 4, 4, 4, 2, 9, 1, 2, 16, 43, 143, 24......],     u'size': [240, 320]&#125;\n\n\nRLE (Run-length encoding)格式：对于一张图240x320的图片，共有76800个像素点，根据每个像素点在不在目标区域中，共有76800个bit；对于00000111100111110…，记录连续0和1的个数得到 [5,4,2,5,…]，这就是上文的 counts 数组。\nsize 为这个图片的宽和高\n\ncategories 字段categories 是一个包含多个 category 实例的数组，而 category 结构体描述如下：\n12345category&#123;    \"id\": int,    \"name\": str,    \"supercategory\": str,&#125;\n\n示例：\n12345678910&#123;\t\"supercategory\": \"person\",\t\"id\": 1,\t\"name\": \"person\"&#125;,&#123;\t\"supercategory\": \"vehicle\",\t\"id\": 2,\t\"name\": \"bicycle\"&#125;,\n\nObject Keypoint 类型的标注格式整体JSON文件格式1234567&#123;    \"info\": info,    \"licenses\": [license],    \"images\": [image],    \"annotations\": [annotation],    \"categories\": [category]&#125;\n\n\nimages 数组元素数量是划入训练集（测试集）的图片的数量\nannotations 是bounding box的数量，在这里只有人这个类别的bounding box\ncategories 数组元素的数量为1，只有一个：person（2017年）\n\nannotations 字段这个类型中的 annotation 结构体包含了Object Instance中 annotation 结构体的所有字段，再加上2个额外的字段：keypoints 和 num_keypoints。\n1234567891011annotation&#123;    \"keypoints\": [x1,y1,v1,...],    \"num_keypoints\": int,    \"id\": int,    \"image_id\": int,    \"category_id\": int,    \"segmentation\": RLE or [polygon],    \"area\": float,    \"bbox\": [x,y,width,height],    \"iscrowd\": 0 or 1,&#125;\n\n\nkeypoints 是一个长度为3*k的数组，其中k是 category 中 keypoints 的总数量。每一个 keypoint 是一个长度为3的数组，第一和第二个元素分别是x和y坐标值，第三个元素是个标志位v，v为0时表示这个关键点没有标注（这种情况下x=y=v=0），v为1时表示这个关键点标注了但是不可见（被遮挡了），v为2时表示这个关键点标注了同时也可见\nnum_keypoints 表示这个目标上被标注的关键点的数量（v&gt;0），比较小的目标上可能就无法标注关键点\n\n示例：\n123456789&#123;\t\"segmentation\": [[125.12,539.69,140.94,522.43...]],\t\"num_keypoints\": 10,\t\"area\": 47803.27955,\t\"iscrowd\": 0,\t\"keypoints\": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,142,309,1,177,320,2,191,398...],\t\"image_id\": 425226,\"bbox\": [73.35,206.02,300.58,372.5],\"category_id\": 1,\t\"id\": 183126&#125;\n\ncategories 字段对于每一个 category 结构体，相比Object Instance中的 category 新增了2个额外的字段。\n1234567category&#123;    \"id\": int,    \"name\": str,    \"supercategory\": str,    \"keypoints\": [str],    \"skeleton\": [edge]&#125;\n\n\nkeypoints 是一个长度为k的数组，包含了每个关键点的名字\nskeleton 定义了各个关键点之间的连接性（比如人的左手腕和左肘就是连接的，但是左手腕和右手腕就不是）。目前，COCO的keypoints只标注了person category （分类为人）。\n\n示例：\n1234567&#123;\t\"supercategory\": \"person\",\t\"id\": 1,\t\"name\": \"person\",\t\"keypoints\": [\"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"],\t\"skeleton\": [[16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13],[6,7],[6,8],[7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]&#125;\n\nImage Caption 类型的标注格式整体JSON文件格式123456&#123;    \"info\": info,    \"licenses\": [license],    \"images\": [image],    \"annotations\": [annotation]&#125;\n\n\nimages 数组的元素数量等于划入训练集（或者测试集）的图片的数量\nannotations 的数量要多于图片的数量，这是因为一个图片可以有多个场景描述；\n\nannotations 字段12345annotation&#123;    \"id\": int,    \"image_id\": int,    \"caption\": str&#125;\n\n示例：\n12345&#123;\t\"image_id\": 179765,\t\"id\": 38,\t\"caption\": \"A black Honda motorcycle parked in front of a garage.\"&#125;\n\n参考资料\nCOCO数据集的标注格式\n\n","thumbnail":"coco-dataset-format/coco-logo.png","plink":"https://yuxinzhao.net/coco-dataset-format/"},{"title":"Windows上安装pycocotools报错","date":"2020-01-06T17:23:00.000Z","updated":"2022-01-04T08:35:48.659Z","content":"本文记录Windows上pip安装pycocotools是遇到的问题及解决办法。\n\n\n问题常规安装操作为\n12pip install cythonpip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n\n但是安装失败，出现错误信息：\n1cl: 命令行 error D8021 :无效的数值参数 &quot;/Wno-cpp&quot;\n\n解决办法\n从官方github仓库下载源码\n1git clone https://github.com/cocodataset/cocoapi.git\n\n修改安装程序源码  cocoapi/PythonAPI/setup.py\n删除第12行的 -Wno-cpp 和 -Wno-unused-function 两个参数。\n12345678ext_modules = [    Extension(        &apos;pycocotools._mask&apos;,        sources=[&apos;../common/maskApi.c&apos;, &apos;pycocotools/_mask.pyx&apos;],        include_dirs = [np.get_include(), &apos;../common&apos;],        extra_compile_args=[&apos;-Wno-cpp&apos;, &apos;-Wno-unused-function&apos;, &apos;-std=c99&apos;],    )]\n\n保存后开始安装\n12cd cocoapi/PythonAPIpython setup.py install\n\n\n\n","thumbnail":"windows-pycocotools/coco-logo.png","plink":"https://yuxinzhao.net/windows-pycocotools/"},{"title":"Windows上vim的安装与配置","date":"2020-01-06T14:29:15.000Z","updated":"2022-01-04T08:35:48.655Z","content":"本文对我在windows平台上安装和配置vim的过程中遇到的问题以及解决措施进行记录。\n\n\n\n\n安装vim一开始我按照GitHub官方仓库vim/vim上的官方教程，从vim官网下载页面下载并安装了vim，基本能够正常使用，但是在后续安装vim插件的过程中，发现官网编译的vim并不支持python，所有依赖于python的vim插件都无法正常使用。解决方法有二：\n\n使用第三方编译的版本\n将github repo的代码clone下来自己编译\n\n为了节省时间，我直接选择了直接使用第三方编译的版本，下载地址在此处。\n下载后在本地创建文件夹 vim/vim82，此处的vim82仅表示我安装的是vim8.2版本，将下载的压缩包的内容放在vim82内，确保能找到vim/vim82/vim.exe主程序。接下来将vim/vim82路径添加到环境变量中即可通过命令行指令使用vim了：\n1vim test.py\n\n配置vimvim的配置在unix下是通过.vimrc文件的进行，而在windows下是通过_vimrc文件。\n将 vim/vim82/vimrc_example.vim 复制到 vim 目录下并重命名为 _vimrc，即vim/_vimrc。\n基本配置在 vim/_vimrc 文件末尾添加以下配置：\n1234567891011121314set showcmd     &quot; 在右下角显示输入的命令syntax on       &quot; 开启语法高亮set number      &quot; 显示行号set relativenumber  &quot; 显示相对行号set wrapset rulerset incsearch   &quot; 使得在用/搜索时将匹配的位置实时显示出来set showmatchset autoindent  &quot; 自动缩进set backspace=endofline,start,indent  &quot;使得backspace可以删除回车、换行及缩进set encoding=utf-8set nobackup    &quot; 不生成备份文件 *~set nowritebackupset noundofile  &quot; 撤销后不保留备份文件 *.un\n\n插件管理器 vim-plug按照 github仓库 junegunn/vim-plug 的教程安装vim。\n\n下载 plug.vim 文件到 vim/vim82/autoload 文件夹下\n\n新建 vim/vim82/plugged 文件夹\n\n修改 vim/_vimrc，添加如下配置\n123456789call plug#begin(&apos;./vim82/plugged&apos;)&quot; 中间放插件的安装指令，如&quot; Plug &apos;preservim/nerdtree&apos;call plug#end()&quot; 外面放插件的配置指令，如&quot; map &lt;C-f&gt; :NERDTreeToggle&lt;CR&gt;\n\n\n\n添加插件：\n\n在 vim/_vimrc 中添加对应的插件地址，保存\n在vim中执行 :PlugInstall 安装插件，vim-plug 会到 github 上将对应插件的仓库克隆下来自动安装\n退出vim，重新进入即可\n\n插件vim-airlinevim-airline 插件在底部增加一个状态栏，可以显示文件路径，文件编码，光标位置等信息\n安装指令：\n1Plug &apos;vim-airline/vim-airline&apos;\n\nUltisnipsUltisnips 插件可以通过 Tab 键触发预设的补全。\n安装指令：\n12Plug &apos;SirVer/ultisnips&apos;Plug &apos;honza/vim-snippets&apos;\n\n注意：\n该插件依赖于python，要求本机已安装python，且对版本有要求，在vim的下载页面 找到编译vim使用的python版本，如3.8.1，在python官网下载对应版本的python安装，别忘了添加环境变量。\nNERDTreeNERDTree 插件允许呼出文件目录树，可以方便地在vim中查找目录，切换文件。\n安装指令：\n1Plug &apos;preservim/nerdtree&apos;\n\n配置指令：\n1map &lt;C-f&gt; :NERDTreeToggle&lt;CR&gt;\n\n该配置指令将 &lt;C-f&gt; 即 Ctrl + f 映射到 :NERDTreeToggle&lt;CR&gt; 指令上。\n使用：\n\n按下 Ctrl + f 即可呼出/收回左侧栏的目录树\n\n在打开左侧栏目录树的情况下，通过 Ctrl + w + h 和  Ctrl + w + l 可以在左右栏中移动\n\n光标在左侧目录树的情况下，可以通过以下按键触发不同功能\n12345678910111213141516171819202122232425262728?: 快速帮助文档m: 修改文件（新建文件/移动文件/重命名文件/删除文件/复制文件）o: 打开一个目录或者打开文件，创建的是buffer，也可以用来打开书签go: 打开一个文件，但是光标仍然留在NERDTree，创建的是buffert: 打开一个文件，创建的是Tab，对书签同样生效T: 打开一个文件，但是光标仍然留在NERDTree，创建的是Tab，对书签同样生效i: 水平分割创建文件的窗口，创建的是buffergi: 水平分割创建文件的窗口，但是光标仍然留在NERDTrees: 垂直分割创建文件的窗口，创建的是buffergs: 和gi，go类似x: 收起当前打开的目录X: 收起所有打开的目录e: 以文件管理的方式打开选中的目录D: 删除书签P: 大写，跳转到当前根路径p: 小写，跳转到光标所在的上一级路径K: 跳转到第一个子路径J: 跳转到最后一个子路径&lt;C-j&gt;和&lt;C-k&gt;: 在同级目录和文件间移动，忽略子目录和子文件C: 将根路径设置为光标所在的目录u: 设置上级目录为根路径U: 设置上级目录为跟路径，但是维持原来目录打开的状态r: 刷新光标所在的目录R: 刷新当前根路径I: 显示或者不显示隐藏文件f: 打开和关闭文件过滤器q: 关闭NERDTreeA: 全屏显示NERDTree，或者关闭全屏\n\n\n\nNERDCommender一个注释的辅助插件\n安装指令：\n1Plug &apos;preservim/nerdcommenter&apos;\n\n使用：\n12345&lt;leader&gt;cc   加注释&lt;leader&gt;cu   解开注释&lt;leader&gt;c&lt;space&gt;  加上/解开注释, 智能判断&lt;leader&gt;cy   先复制, 再注解(p可以进行黏贴)\n\n默认情况下 &lt;leader&gt; 为 \\ 键。\nAuto Pairsautopairs 插件能够在输入左括号时自动添加右括号。\n安装指令：\n1Plug &apos;jiangmiao/auto-pairs&apos;\n\nMarkdown以下插件添加对markdown的支持\n安装指令：\n1234Plug &apos;godlygeek/tabular&apos;Plug &apos;plasticboy/vim-markdown&apos;Plug &apos;mzlogin/vim-markdown-toc&apos;Plug &apos;iamcco/markdown-preview.nvim&apos;, &#123;&apos;do&apos;: &apos;cd app &amp; yarn install&apos;&#125;\n\n配置指令：\n1let g:vim_markdown_folding_disabled = 1\n\n使用：\n12:MarkdownPreview      &quot; 打开预览:MarkdownPreviewStop  &quot; 关闭预览\n\nLeaderFLeaderF 提供模糊查找的功能\n安装指令：\n1Plug &apos;Yggdroot/LeaderF&apos;, &#123;&apos;do&apos;: &apos;./install.bat&apos;&#125;\n\n配置指令：\n1map &lt;C-g&gt; :LeaderfFunction&lt;CR&gt;\n\n使用：\n\n\n\n快捷键\n指令\n说明\n\n\n\n&lt;leader&gt;f\n:LeaderfFile\n搜索当前目录下的文件\n\n\n&lt;leader&gt;b\n:LeaderfBuffer\n搜索当前的Buffer中的文件\n\n\n&lt;leader&gt;g (自行设置映射)\n:LeaderfFunction\n搜索当前文件的函数\n\n\n注意：\n要使用 :LeaderfFunction  功能需要先安装 ctags，将压缩包下载后解压再添加到环境变量中即可。\n","thumbnail":"vim-for-windows/bg.jpg","plink":"https://yuxinzhao.net/vim-for-windows/"},{"title":"使用UML顺序图表示物件交互及时序关系","date":"2019-10-24T19:13:36.000Z","updated":"2022-01-04T08:35:48.643Z","content":"本文介绍UML顺序图(Sequence Diagram，又称时序图)表示物件间的交互以及时序关系。\n\n\n顺序图说明使用顺序图就是要清楚地传达各物件在时间轴上的交互情景，下面通过一张图来展示构成顺序图的所有物件有哪些，以及在各时间点彼此交互的情况。\n\n\n参与者(Participants)：参与交互的物件，内文格式为 名称 : 类别名称\n生命线(Lifeline)：表示交互发生的时间轴\n执行发起(Execution Occurrence)：描述生命线内某个执行单元\n讯息(Message)：呼叫目标物件的实际行为\n合并片段(Combined Fragment)：描述不同情况下可能发生的变化\n回传讯息(Reply Message)：非必须符号，因为执行发起的结束点就隐含此意了\n\n范例假定这样一个场景：医院里，病患看病都是需要预约挂号的，最直接的就是跟经理约个时间，而经理通常回询问病患是否为初诊，如果是初诊就会要求病患填写基本资料，并且通过医院系统注册新病患的资料。接着就会询问要挂哪一位医生的门诊，所以需要通过医院系统来查询门诊是否额满，此时就会有两种情况，因此需要通过 alt 片段来区分不同情况下所发生的变化。当满诊的情况下，助理会将病患列入候补清单，若有其他病患退挂会主动通知（此流程于此不详述）；当该医生的门诊尚未额满时，助理会通过医院系统建立一张预约表，并且执行预约作业（输入病患及医生门诊信息），完成后会取得一组预约序号。最后整个流程结束后由助理回应病患此次预约挂号的处理结果。\n\n组合片段类型在绘制顺序图时经常会使用到组合片段，且会因各种情景存在许多种类型，以下介绍一些比较常见的片段。\nalt 互斥条件alt  (Alternatives)：互斥条件，任何情况只有一个序列发生\n例：工程师有工作就用电脑写程序，否则就用电脑听音乐\n\nopt 选择项opt (Option)：选择项，不一定会发生的序列（符合条件才会执行）\n例：工程师遇到问题的时候会查询 stackoverflow 求解答\n\npar 并行处理par (Parallel)：并行处理，片段中的事件并行执行\n例：工程师用电脑边写程序边听音乐\n\nloop 重复片段loop ：重复片段。可以设立重复的条件，若未设定最小级最大重复次数，默认表示无限制\n例：工程师只要遇到问题，直接无尽无尽的使用电脑进行Google搜索，直到找到解决方案为止\n\nref 参考ref (Reference)：表示参考另一个交互顺序图，属于 Interaction Use 范畴\n例：顾客在使用网站时都需要登录，而登录细节就此不再赘述（直接参考登录顺序图即可）\n\n以下是被参考的 Login 交互顺序图，所有登录细节统一在这个顺序图中说明。\n\n参考资料\n[UML] 使用循序圖傳達各物件互動及時序關係\n\nCombined Fragment\n\n\n","plink":"https://yuxinzhao.net/sequence-diagram/"},{"title":"背包问题动态规划求解","date":"2019-10-21T20:17:02.000Z","updated":"2022-01-04T08:35:48.539Z","content":"本文介绍通过动态规划求解背包问题。\n\n\n问题给定 \nn\n\n\n\n\n \n\n 个重量为 \nw_1, \\cdots, w_n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n ，价值为 \nv_1, \\cdots, v_n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n 的物品和一个承重量为 \nW\n\n\n\n\n \n\n 的背包，求这些物品中最有价值的一个子集，并且要能够装进背包里。\n算法思想设 \nF(i,j)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 为物品重量为 \nw_1, \\cdots, w_i\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n ，价值为 \nv_1, \\cdots, v_i\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n 的物品和一个承重量为 \nj\n\n\n\n\n \n\n 的背包的背包问题的最优解的总价值。也就是说 \nF(i,j)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n 是能够放进承重量为 \nj\n\n\n\n\n \n\n  的背包的前 \ni\n\n\n\n\n \n\n 个物品中最有价值子集的总价值。\n推导式：\n\nF(i, j) = \n\\begin{cases}\n\\max \\{F(i-1, j), v_i + F(i-1, j-w_i) \\}, &amp;j-w_i \\ge 0 \\\\\nF(i-1, j), &amp;j-w_i \\lt 0\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n\n\n\n\n\n\n边界条件：\n\nF(i, 0) = 0 , \\forall i \\\\\nF(0, j) = 0 , \\forall j\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n代码实现C++实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;int Max(int a, int b) &#123;    return a &gt; b ? a : b;&#125;/* Dynamic programming method to solve bag problem * * Args: *      dst (int*): pointer to the output array of size (n+1) x (maxBagWeight+1), index of (i, j) means the maximum value if  *                  the bag max weight equals to j and only consider the 1 to i items. *      numItem (int): the number of items. *      maxBagWeight (int): the max weight of bag. *      value (int*): the value of items. array of shape n *      weight (int*): the weight of items. array of shape maxBagWeight *  * Returns: *      int: the max value the bag can store. */int BagDP(int* dst, int numItem, int maxBagWeight, int* value, int* weight) &#123;    // initialize the bounder    for (int i = 0; i &lt;= numItem; i++) dst[i * (maxBagWeight + 1)] = 0;    for (int j = 0; j &lt;= maxBagWeight; j++) dst[j] = 0;        // calcualte the max weight    for (int i = 1; i &lt;= numItem; i++) &#123;        for (int j = 1; j &lt;= maxBagWeight; j++) &#123;            dst[i * (maxBagWeight + 1) + j] = j - weight[i - 1] &gt;= 0 ? Max(dst[(i - 1) * (maxBagWeight + 1) + j], value[i - 1] + dst[(i - 1) * (maxBagWeight + 1) + (j - weight[i - 1])]) : dst[(i - 1) * (maxBagWeight + 1) + j];        &#125;    &#125;    return dst[numItem * (maxBagWeight + 1) + maxBagWeight];&#125;void Test() &#123;    const int numItem = 4;    const int maxBagWeight = 5;    int value[numItem] = &#123;12, 10, 20, 15&#125;;    int weight[numItem] = &#123;2, 1, 3, 2&#125;;    int dst[numItem + 1][maxBagWeight + 1];    int maxValue = BagDP(&amp;dst[0][0], numItem, maxBagWeight, value, weight);        std::cout &lt;&lt; \"max value : \" &lt;&lt; maxValue &lt;&lt; std::endl;    for (int i = 0; i &lt;= numItem; i++) &#123;        for (int j = 0; j &lt;= maxBagWeight; j++) &#123;            std::cout &lt;&lt; dst[i][j] &lt;&lt; \" \";        &#125;        std::cout &lt;&lt; std::endl;    &#125; &#125;int main() &#123;    Test();    return 0;&#125;\n\n","plink":"https://yuxinzhao.net/bag-problem-dynamic-programming/"},{"title":"Floyd 算法计算完全最短路径","date":"2019-10-21T19:39:45.000Z","updated":"2022-01-04T08:35:48.587Z","content":"本文介绍使用 Floyd 算法求解完全最短路径。\n\n\n问题完全最短路径问题(all-pairs shortest-paths problem)给定一个加权连通图，找到从每个顶点出发到其他所有顶点之间的最短距离(最短路径的长度)。\n为了方便起见，将最短路径的长度记录在一个称为距离矩阵的 \nn\n\n\n\n\n \n\n 阶矩阵中。我们要求的就是距离矩阵 \nD\n\n\n\n\n \n\n.\n有向图：\n\n对应的权重矩阵：\n\nW = \n\\begin{bmatrix}\n0 &amp; \\infty &amp; 3 &amp; \\infty \\\\\n2 &amp; 0 &amp; \\infty &amp; \\infty \\\\\n\\infty &amp; 7 &amp; 0 &amp; 1 \\\\\n6 &amp; \\infty &amp; \\infty &amp; 0\n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n对应的距离矩阵：\n\nD =\n\\begin{bmatrix}\n0 &amp; 10 &amp; 3 &amp; 4 \\\\\n2 &amp; 0 &amp; 5 &amp; 6 \\\\\n7 &amp; 7 &amp; 0 &amp; 1 \\\\\n6 &amp; 16 &amp; 9 &amp; 0 \n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n算法思想Floyd 算法和 Warshall的思想类似，通过对于图中的一个点，如果图中另外任意两个点不通过该点的距离比通过该点的距离大，则这两点的最短路径应该通过该点，由此可将距离迭代地缩小。\nFloyd 算法通过一系列 \nn\n\n\n\n\n \n\n 阶矩阵来计算距离矩阵。\n\nD^{(0)}, \\cdots , D^{(n-1)}, D^{(n)}\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n\n\n\n\n其中，\nD^{(0)}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 即为有向图对应的权重矩阵，\nD^{(n)}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 即为我们要求解的距离矩阵。\n矩阵间的关系如下：\n\nD_{ij}^{(k)} = \\min \\left(D_{ij}^{(k-1)}, \\quad D_{ik}^{(k-1)} + D_{kj}^{(k-1)} \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n\n\n边界条件为：\n\nD_{i0} = D_{0j} = 0\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n\n\n \n \n\n\n利用动态规划即可求解。\n代码实现C++实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;const int inf = 5000;  // assume that this number is infinity and unreachableint Min(int a, int b) &#123;    return a &gt; b? b : a;&#125;/* Floyd Algorithm * to solve the completed shortest path problem *  * Args: *     src (int*): a source matrix of n x n, each element [i, j] is the weight from node i to node j, *                 if the element greater than or equal to inf, it means that node i and node is not connected directly. *     dst (int*): a destination matrix of n x n to store the shortest paths distances,  *                 if dst is NULL, the result will be stored in the src *     n (int): the width and height of matrix src and dst */void Floyd(int* src, int* dst, int n) &#123;    if (dst != NULL) &#123;        // copy the src to dst        for(int i = 0; i &lt; n * n; i++) &#123;            dst[i] = src[i];        &#125;    &#125;    else &#123;        dst = src;    &#125;        // calcualte the shortest path distances    for (int k = 0; k &lt; n; k++) &#123;        for (int i = 0; i &lt; n; i++) &#123;            for (int j = 0; j &lt; n; j++) &#123;                dst[i * n + j] = Min(dst[i * n + j], dst[i * n + k] + dst[k * n + j]);            &#125;        &#125;    &#125;&#125;void Test() &#123;    int A[4][4] = &#123;        &#123;0,   inf, 3,   inf&#125;,        &#123;2,   0,   inf, inf&#125;,        &#123;inf, 7,   0,   1  &#125;,        &#123;6,   inf, inf, 0  &#125;    &#125;;    int D[4][4];    Floyd(&amp;A[0][0], &amp;D[0][0], 4);        for (int i = 0; i &lt; 4; i++) &#123;        for (int j = 0; j &lt; 4; j++) &#123;            std::cout &lt;&lt; D[i][j] &lt;&lt; \" \";        &#125;        std::cout &lt;&lt; std::endl;    &#125;&#125;int main() &#123;    Test();    return 0;&#125;\n\n","plink":"https://yuxinzhao.net/floyd/"},{"title":"Warshall 算法求解传递闭包","date":"2019-10-21T19:10:41.000Z","updated":"2022-01-04T08:35:48.655Z","content":"本文介绍使用 Warshall 算法求解传递闭包。\n\n\n\n\n定义一个 \nn\n\n\n\n\n \n\n 顶点有向图的传递闭包(transitive-closure)可以定义为一个 \nn\n\n\n\n\n \n\n 阶布尔矩阵 \nT = \\{ t_{ij}\\}\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n\n\n \n\n，如果从第 \ni\n\n\n\n\n \n\n 个顶点到第 \nj\n\n\n\n\n \n\n 个顶点之间存在一条有效的有向路径(即长度大于0的有向路径)，矩阵第 \ni\n\n\n\n\n \n\n 行(\n1 \\le i \\le n\n\n\n\n\n\n\n\n \n \n \n \n \n\n)第 \nj\n\n\n\n\n \n\n 列(\n1 \\le j \\le n\n\n\n\n\n\n\n\n \n \n \n \n \n\n)的元素为1，否则，\nt_{ij}\n\n\n\n\n\n\n \n\n \n \n\n\n 为 0。\n例子有向图：\n\n对应的邻接矩阵：\n\nA = \n\\begin{bmatrix}\n0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; 1 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 \\\\\n1 &amp; 0 &amp; 1 &amp; 0\n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n对应的传递闭包：\n\nT = \n\\begin{bmatrix}\n1 &amp; 1 &amp; 1 &amp; 1 \\\\\n1 &amp; 1 &amp; 1 &amp; 1 \\\\\n0 &amp; 0 &amp; 0 &amp; 0 \\\\\n1 &amp; 1 &amp; 1 &amp; 1 \n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n\n \n\n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n算法思想Warshall 算法通过一系列 \nn\n\n\n\n\n \n\n 阶布尔矩阵来构造传递闭包。\n\nR^{(0)}, \\cdots , R^{(n-1)}, R^{(n)}\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n\n\n\n\n其中 \nR^{(0)}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 就是有向图的邻接矩阵 \nA\n\n\n\n\n \n\n，\nR^{(n)}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 就是对应的传递闭包。\n矩阵间的关系如下：\n\nR_{ij}^{(k)} = R_{ij}^{(k-1)} \\lor \\left( R_{ik}^{(k-1)} \\land R_{kj}^{(k-1)} \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n \n \n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n \n\n\n\n利用动态规划即可求解。\n代码实现C++实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;/* Warshall Algorithm  * to calcualte the transitive closure *  * Args: *     src (int*): a source matrix of n x n, each element of it is either 0 or 1 *     dst (int*): a destination matrix of n x n to store the generated transitive closure,  *                 if dst is NULL, the result will be stored in the src *     n (int): the width and height of matrix src and dst */void Warshall(int* src, int* dst, int n) &#123;    if (dst != NULL) &#123;        // copy the src to dst        for(int i = 0; i &lt; n * n; i++) &#123;            dst[i] = src[i];        &#125;    &#125;    else &#123;        dst = src;    &#125;        // calcualte the transitive closure    for (int k = 0; k &lt; n; k++) &#123;        for (int i = 0; i &lt; n; i++) &#123;            for (int j = 0; j &lt; n; j++) &#123;                dst[i * n + j] = (dst[i * n + j] || (dst[i * n + k] &amp;&amp; dst[k * n + j]));            &#125;        &#125;    &#125;&#125;void Test() &#123;    int A[4][4] = &#123;        &#123;0, 1, 0, 0&#125;,        &#123;0, 0, 0, 1&#125;,        &#123;0, 0, 0, 0&#125;,        &#123;1, 0, 1, 0&#125;    &#125;;    int B[4][4];    Warshall(&amp;A[0][0], &amp;B[0][0], 4);        for (int i = 0; i &lt; 4; i++) &#123;        for (int j = 0; j &lt; 4; j++) &#123;            std::cout &lt;&lt; B[i][j] &lt;&lt; \" \";        &#125;        std::cout &lt;&lt; std::endl;    &#125;    &#125;int main() &#123;    Test();    return 0;&#125;\n\n","plink":"https://yuxinzhao.net/warshall/"},{"title":"Linux nohup 后台执行指令","date":"2019-10-21T00:09:18.000Z","updated":"2022-01-04T08:35:48.595Z","content":"通常情况下，执行 Linux 指令必须终端一直开着才能指令执行下去，直到指令执行完成，才会结束。nohup 指令可以让指令在后台执行，而不需要操作者一直等待其执行完成，即使操作者登出指令依然会继续执行。\n\n\nnohup 全程为 no hangup。 hangup (HUP) 信号会在用户登出时由系统发出，通知所有进程结束，但通过 nohup 执行的指令，HUP信号会被 nohup 拦截，从而让指令继续执行。\n后台运行1nohup your-command &amp;\n\nnohup 默认会将输出重定向到  nohup.out  文件，注意默认是将输出追加到文件尾。\n可以通过 tail 指令自动”即时”显示最新的输出。\n注： 不一定能看到即时的输出，因为 stdout 通常有缓存，只有缓存满了才会写到输出文件中\n1tail -f nohup.out\n\n也可以自已重定向输出到指定文件。\n1nohup your-command &gt; output.file 2&gt;&amp;1 &amp;\n\n以低优先级执行因为使用 nohup 执行的指令一般都是要跑很久的，用户登出后继续运行，有时候可能占用太多的系统资源，我们可以结合 nice 指令让指令以较低优先级在后台执行，尽量不影响其他正常指令的执行。\n1nohup nice your-command &amp;\n\n查看后台进程由于 nohup 指令在后台执行，要查看及终止后台进程都需要先获取该进程的 pid。\n1jobs -l\n\n输出样例： \n1[1]+   490 Running    nohup go run main.go &amp;\n\n其中，1 是任务标识 job-spec，490 是进程的 pid。\n注： jobs 命令在重新登录后就无法查看后台进程了，可以使用 top 指令查看所有进程。\n1top\n\n终止后台进程1kill your-nohup-pid\n\n将后台程序移到前台1fg your-nohup-job-spec\n\n注：虽然移到了前台，但 stdout 仍是带缓存的，只有等缓冲区满了，输出才会刷新到屏幕上。\n","plink":"https://yuxinzhao.net/linux-command-nohup/"},{"title":"Pickle Python对象序列化","date":"2019-10-20T00:50:03.000Z","updated":"2022-01-04T08:35:48.615Z","content":"pickle 是一个用于序列化 Python 对象的库，它可以序列化任何对象并将其保存到文件中，需要时再将其从文件中读出。本文将介绍如何使用 pickle 序列化 python 对象。\n\n\n\n\n保存python对象到文件123456import pickleobj = &#123;\"pickle\": \"test\"&#125;  # 可以是任何对象，包括自定义的类对象with open('file.pkl', 'wb') as f:    pickle.dump(obj, f)  # 将obj序列化后保存到文件f中，f只要是有write方法的对象都可以\n\n从文件读取python对象1234import picklewith open('file.pkl', 'rb') as f:    obj = pickle.load(f)  # 从f中读取obj, 如果读取的是自定义的类对象，则要求读取时已经对该类进行定义\n\n参考资料\n官方文档\nPython Wiki\n\n","plink":"https://yuxinzhao.net/pickle/"},{"title":"字符串匹配技术之 Horspool算法和Boyer-Moore算法","date":"2019-10-15T18:12:24.000Z","updated":"2022-01-04T08:35:48.595Z","content":"字符串匹配问题要求在一个较长的字符串中，匹配一个较短的字符串，找到短字符串在长字符串中第一次出现的位置。本文将介绍 Boyer-Moore 算法及其简化版本 Horspool 算法。\n\n\n问题在一个被称为文本(text)的n个字符的串中，寻找一个被称为模式(pattern)的给定m个字符的串。\nHorspool 算法代码实现C++ 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;string.h&gt;class ShiftTable &#123;public:    ShiftTable(const char* pattern) &#123;        len = strlen(pattern);        for (int i = 0; i &lt; len - 1; i++) &#123;            table[pattern[i]] =  len - 1 - i;        &#125;    &#125;    int Get(char c) &#123;        return (table.find(c) == table.end() ? len : table[c]);    &#125;    int operator[](char c) &#123;        return Get(c);        &#125;private:    std::map&lt;char, int&gt; table;    int len;&#125;;int HorspoolMatching(const char* pattern,const char* text) &#123;    int lenPattern = strlen(pattern);    int lenText = strlen(text);    // text长度比pattern小时text必定不包含pattern    if (lenText &lt; lenPattern) &#123;         return -1;    &#125;    // 构建移动表    ShiftTable table(pattern);    int i = lenPattern - 1;    while (i &lt; lenText) &#123;        int k = 0;        while (k &lt;= lenPattern - 1 &amp;&amp; pattern[lenPattern - 1 - k] == text[i - k]) &#123;            k++;        &#125;        if (k == lenPattern) &#123;            return i - lenPattern + 1;        &#125;        else &#123;            i += table[text[i]];        &#125;    &#125;    return -1;&#125;void Test() &#123;    std::cout &lt;&lt; HorspoolMatching(\"cde\", \"abcdefg\") &lt;&lt; std::endl;&#125;int main() &#123;    Test();    return 0;&#125;\n\nBoyer-Moore 算法代码实现C++ 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;string.h&gt;int Max(int a, int b) &#123;    return a &gt; b ? a : b;&#125;int Min(int a, int b) &#123;    return a &gt; b ? b : a;&#125;class ShiftTable &#123;public:    ShiftTable(const char* pattern) &#123;        len = strlen(pattern);        for (int i = 0; i &lt; len - 1; i++) &#123;            table[pattern[i]] =  len - 1 - i;        &#125;    &#125;    int Get(char c) &#123;        return (table.find(c) == table.end() ? len : table[c]);    &#125;    int operator[](char c) &#123;        return Get(c);        &#125;private:    std::map&lt;char, int&gt; table;    int len;&#125;;class BadSymbolShiftTable: public ShiftTable &#123;public:    BadSymbolShiftTable(const char* pattern): ShiftTable(pattern) &#123;&#125;    int Get(char c, int k) &#123;        return Max(ShiftTable::Get(c) - k, 1);    &#125;&#125;;class GoodSuffixShiftTable &#123;public:    GoodSuffixShiftTable(const char* pattern) &#123;        len = strlen(pattern);        table = new int[len];                table[0] = len;        char* filpPattern = FlipString(pattern);        for (int k = 1; k &lt; len; k++) &#123;             char* filpPostPattern = FlipString(&amp;pattern[len - k]);            table[k] = HorspoolMatchingWithSuffix(filpPostPattern, &amp;filpPattern[1]) + 1;            if (table[k] == 0) &#123;                table[k] = len;            &#125;            delete[] filpPostPattern;        &#125;        delete[] filpPattern;    &#125;    ~GoodSuffixShiftTable() &#123;        delete[] table;    &#125;    int Get(int k) &#123;        if (0 &lt;= k &lt; len) &#123;            return table[k];        &#125;        else &#123;            throw \"GoodShuffixShiftTable index should be in [0, len).\";        &#125;    &#125;    int operator[](int k) &#123;        return Get(k);    &#125;private:    char* FlipString(const char* str) &#123;        int len = strlen(str);        char* flipString = new char[len + 1];        for (int i = 0; i &lt; len; i++) &#123;            flipString[len - 1 - i] = str[i];        &#125;        flipString[len] = '\\0';        return flipString;    &#125;    int HorspoolMatchingWithSuffix(const char* pattern,const char* text) &#123;        int lenPattern = strlen(pattern);        int lenText = strlen(text);        // text长度比pattern小时text必定不包含pattern        if (lenText &lt; lenPattern) &#123;             return -1;        &#125;        // 构建移动表        ShiftTable table(pattern);        // text边界处只匹配pattern的前缀也判定为匹配        int i = lenPattern - 1;        while (i &lt; lenText + lenPattern - 1) &#123;            int k = Max(0, i - lenText + 1);             while (k &lt;= lenPattern - 1 &amp;&amp; pattern[lenPattern - 1 - k] == text[i - k]) &#123;                k++;            &#125;            if (k == Min(lenPattern, i + 1)) &#123;                return i - lenPattern + 1;            &#125;            else &#123;                i += table[text[i]];            &#125;        &#125;        return -1;    &#125;private:    int* table;    int len;&#125;;int BoyerMooreMatching(const char* pattern,const char* text) &#123;    int lenPattern = strlen(pattern);    int lenText = strlen(text);    // text长度比pattern小时text必定不包含pattern    if (lenText &lt; lenPattern) &#123;         return -1;    &#125;    // 构建移动表    BadSymbolShiftTable d1(pattern);    GoodSuffixShiftTable d2(pattern);    int i = lenPattern - 1;    while (i &lt; lenText) &#123;        int k = 0;        while (k &lt;= lenPattern - 1 &amp;&amp; pattern[lenPattern - 1 - k] == text[i - k]) &#123;            k++;        &#125;        if (k == lenPattern) &#123;            return i - lenPattern + 1;        &#125;        else &#123;            if (k == 0) &#123;                i += d1.Get(text[i], k);            &#125;            else &#123;                i += Max(d1.Get(text[i], k), d2.Get(k));            &#125;        &#125;    &#125;    return -1;&#125;void Test() &#123;    GoodSuffixShiftTable table(\"ABCBAB\");    for (int i=1; i&lt;6; i++) &#123;        std::cout &lt;&lt; table.Get(i) &lt;&lt; \" \";    &#125;    std::cout &lt;&lt; \"\\n-------------------\\n\";    std::cout &lt;&lt; BoyerMooreMatching(\"efg\", \"abcdefg\") &lt;&lt; std::endl;&#125;int main() &#123;    Test();    return 0;&#125;\n\n","plink":"https://yuxinzhao.net/horspool-boyer-moore/"},{"title":"Support Vector Machine 支持向量机","date":"2019-10-11T17:06:07.000Z","updated":"2022-01-04T08:35:48.647Z","content":"本文介绍用支持向量机 (Support Vector Machine, SVM) 解决二分类问题。\n\n\n问题给定训练样本集 \n D = \\{(\\boldsymbol{x_1}, y_1), \\dots ,(\\boldsymbol{x_m}, y_m) \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n , \n\\boldsymbol{x_i} \\in \\mathbb{R}^n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n,  \ny_i \\in \\{ -1 , +1 \\}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n ， 在训练集的样本空间中找到一个超平面将不同类别的样本区分开。\n线性支持向量机建模划分超平面可用一下线性方程表示:\n\n\\boldsymbol w^T \\boldsymbol x + b = 0\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n假设超平面 \n(w,b)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 能将训练样本正确分类，即对于 \n(x_i, y_i) \\in D\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n ，\n\n\\begin{cases}\n\\boldsymbol w^T \\boldsymbol x_i + b \\ge +1 ,&amp; y_i = +1 \\\\\n\\boldsymbol w^T \\boldsymbol x_i + b \\le -1 ,&amp; y_i = -1 \n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n \n \n\n \n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n\n\n我们可以将上式整理一下，得到约束条件\n\ny_i (\\boldsymbol w^T \\boldsymbol x_i + b) \\ge 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n \n \n\n\n距离超平面最近的样本点能够使上述不等式的等号成立，即刚好在正确分类的边缘，这样的样本点被称为支持向量(support vector)。\n样本空间中任一点到超平面 \n(w,b)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的距离可写成\n\nr = \\frac{| \\boldsymbol w^T \\boldsymbol x+b |}{\\| \\boldsymbol w \\|}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n\n \n \n\n \n \n \n \n\n\n \n \n \n\n\n\n\n\n超平面两边的两个支持向量 \nx_1\n\n\n\n\n\n \n \n\n, \nx_2\n\n\n\n\n\n \n \n\n 到屏幕的距离被成为间隔(margin):\n\n\\begin{align}\n\\gamma &amp;= r_1+r_2 \\\\\n&amp;= \\frac{| \\boldsymbol w^T \\boldsymbol x_1+b |}{\\| \\boldsymbol w \\|} + \\frac{|\\boldsymbol w^T \\boldsymbol x_2+b |}{\\| \\boldsymbol w \\|} \\\\\n&amp;= \\frac{| +1 |}{\\| \\boldsymbol w \\|} + \\frac{| -1 |}{\\| \\boldsymbol w \\|} \\\\\n&amp;= \\frac{2}{\\| \\boldsymbol w \\|}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n\n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n \n\n\n\n \n\n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n \n\n\n\n\n\n \n\n\n\n\n \n \n \n \n\n\n \n \n \n\n\n\n \n\n\n\n\n \n \n \n \n\n\n \n \n \n\n\n\n\n\n \n\n\n\n \n\n \n \n \n\n\n\n\n\n\n\n\n于是我们需要做的事情就是在满足约束条件 \ny_i (w^T x_i + b) \\ge 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n \n \n\n 的同时，最大化这两个支持向量间的距离 \n\\gamma\n\n\n\n\n \n\n， 即\n\n\\max_{w,b} \\frac{2}{\\| \\boldsymbol w \\|} \\\\\ns.t. \\quad y_i( \\boldsymbol w^T \\boldsymbol x_i +b) \\ge 1 , \\quad i=1,2, \\cdots, m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n\n\n \n\n \n \n \n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n但是这个最大化间隔 \n\\gamma = \\frac{2}{\\| w \\|}\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n\n \n \n \n\n\n\n\n 并不好优化，于是我们转而用一个等价的方式来代替,\n\n\\min_{w,b} \\frac{\\| \\boldsymbol w \\|^2}{2} \\\\\ns.t. \\quad y_i(\\boldsymbol w^T \\boldsymbol x_i +b) \\ge 1 , \\quad i=1,2, \\cdots, m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n\n \n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n求解构造拉格朗日函数\n\nL(\\boldsymbol w,b, \\boldsymbol \\alpha) = \\frac 1 2 ||\\boldsymbol w||^2 + \\sum_{i=1}^m \\alpha_i (1-y_i(\\boldsymbol{w^T x_i}+b))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n\n\n其中\n\\alpha_i\n\n\n\n\n\n \n \n\n 为拉格朗日乘子，且\n\\alpha_i  \\ge 0\n\n\n\n\n\n\n\n \n \n \n \n\n 。令\n\n\\theta(\\boldsymbol w) = \\max_{\\boldsymbol \\alpha:\\alpha_i \\ge 0} L(\\boldsymbol w,b,\\boldsymbol \\alpha)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n \n \n \n\n\n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n\n\n当样本解不满足约束条件，即在可行解区域外时：\n\ny_i(\\boldsymbol{w^T x_i}+b) &lt; 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n\n\n此时,将\n\\alpha_i\n\n\n\n\n\n \n \n\n设置为无穷大，则\n\\theta(\\boldsymbol w)\n\n\n\n\n\n\n\n \n \n \n \n\n也为无穷大。\n当样本解满足约束条件，即在可行解区域内时：\n\ny_i(\\boldsymbol{w^T x_i}+b) \\ge 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n\n\n此时，\n\\theta(\\boldsymbol w)\n\n\n\n\n\n\n\n \n \n \n \n\n为原函数本身。\n综合以上两种情况，\n\n\\theta(\\boldsymbol w) =\n\\begin{cases}\n\\frac 1 2 ||\\boldsymbol w||^2 &amp;,\\boldsymbol x \\in可行解区域 \\\\\n+ \\infty &amp;,\\boldsymbol x \\notin 可行解区域\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n\n\n \n \n\n \n \n \n \n\n \n \n\n\n\n \n \n\n\n\n\n \n \n \n\n可\n\n\n行\n\n\n解\n\n\n区\n\n\n域\n\n\n\n \n \n \n\n可\n\n\n行\n\n\n解\n\n\n区\n\n\n域\n\n\n\n\n\n\n\n原约束问题等价于\n\n\\min_{\\boldsymbol w,b} \\theta(\\boldsymbol w) = \\min_{\\boldsymbol w,b} \\max_{\\alpha_i \\ge 0} L(\\boldsymbol w, b, \\boldsymbol \\alpha) = p^*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n\n \n \n \n\n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n由拉格朗日函数的对偶性，将最小和最大的位置交换得到\n\n\\max_{\\alpha_i \\ge 0}\\min_{\\boldsymbol w,b} L(\\boldsymbol w, b, \\boldsymbol \\alpha) = d^*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n\n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n为使\np^* = d^*\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n，需满足两个条件：\n\n优化问题是凸优化问题\n满足KKT条件\n\n首先，本优化问题显然是一个凸优化问题，所以条件一满足。而要满足条件二，即要求\n\n\\begin{cases}\n\\alpha_i  \\ge 0  \\\\\n1-y_i(\\boldsymbol{w^T x_i}+b) \\le 0 \\\\\n\\alpha_i (1-y_i(\\boldsymbol{w^T x_i}+b)) = 0\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n \n \n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n \n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n \n \n\n\n\n\n\n对 \nL(\\boldsymbol w, b, \\boldsymbol \\alpha)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n 求偏导，得\n\n\\begin{align}\n\\frac{\\partial L}{\\partial \\boldsymbol w} = 0 \\ &amp; \\  \\Rightarrow \\ \\ \\boldsymbol w = \\sum_{i=1}^m \\alpha_i y_i \\boldsymbol x_i \\\\\n\\frac{\\partial L}{\\partial b} = 0 \\ &amp; \\  \\Rightarrow  \\ \\ 0= \\sum_{i=1}^m \\alpha_i y_i\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n\n\n\n\n将以上二式代入\nL(\\boldsymbol w, b, \\boldsymbol \\alpha)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n消去\n\\boldsymbol w\n\n\n\n\n \n\n和\nb\n\n\n\n\n \n\n，得\n\nL(\\boldsymbol w, b, \\boldsymbol \\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i^Tx_j}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n\n\n即\n\n\\min_{\\boldsymbol w,b} L(\\boldsymbol w, b, \\boldsymbol \\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i^Tx_j}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n\n\n最终目标：\n\n\\max_{\\boldsymbol \\alpha} \\min_{\\boldsymbol w,b} L(\\boldsymbol w, b, \\boldsymbol \\alpha) = \\max_{\\boldsymbol \\alpha} ( \\sum_{i=1}^m \\alpha_i - \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i^Tx_j} )  \\\\\ns.t. \\ \\sum_{i=1}^m \\alpha_i y_i = 0 \\\\\n\\alpha_i \\ge 0, \\ i=1,2, \\dots,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n \n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n或\n\n\\min_{\\boldsymbol \\alpha} (  \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i^Tx_j} -  \\sum_{i=1}^m \\alpha_i) = \\min_{\\boldsymbol \\alpha} (  \\frac12 ||w||^2 -  \\sum_{i=1}^m \\alpha_i)  \\\\\ns.t. \\ \\sum_{i=1}^m \\alpha_i y_i = 0 \\\\\n\\alpha_i \\ge 0, \\ i=1,2, \\dots,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n软间隔到目前为止我们讲的都是基于训练集数据线性可分的假设下进行的，但是实际情况下几乎不存在完全线性可分的数据，为了解决这个问题，引入了“软间隔(soft margin)”的概念，即允许某些点不满足约束条件 \ny_i(\\boldsymbol{w^T x_i}+b) \\ge 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n\n。\n采用hinge损失，将原优化问题改写为\n\n\\min_{\\boldsymbol w, b, \\boldsymbol \\xi} (\\frac12||\\boldsymbol w||^2+C \\sum_{i=1}^m \\xi_i) \\\\\ns.t.\\  y_i(\\boldsymbol{w^Tx_i}+b) \\ge 1-\\xi_i \\\\\n\\xi_i \\ge 0,\\ i=1,2,\\dots,m \\\\\nC&gt;0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n\n\n\n其中\n\\xi_i\n\n\n\n\n\n \n \n\n为松弛变量(slack variables)，\n\\xi_i=\\max(0,1-y_i(\\boldsymbol{w^T x_i}+b))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n \n \n \n \n\n ,即hinge损失函数。\n每个样本都有一个对应的松弛变量，表征该样本不满足约束的程度。\nC\n\n\n\n\n \n\n称为惩罚参数，\nC\n\n\n\n\n \n\n值越大，对分类的惩罚越大。\n总结输入：训练集 \n D = \\{(\\boldsymbol{x_1}, y_1), \\dots ,(\\boldsymbol{x_m}, y_m) \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n ，其中，\n\\boldsymbol{x_i} \\in \\mathbb{R}^n , y_i \\in \\{ -1, +1\\}, i = 1,2,\\dots,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n输出：分离超平面和分类决策函数\n(1) 选择惩罚参数\nC\n\n\n\n\n \n\n，构造并求解凸二次规划问题\n\n\\min_{\\boldsymbol \\alpha} (  \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x_i^Tx_j} -  \\sum_{i=1}^m \\alpha_i) \\\\\ns.t. \\ \\sum_{i=1}^m \\alpha_i y_i = 0 \\\\\n 0 \\le \\alpha_i \\le C, i = 1,2,\\dots,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n取得最优解 \n \\boldsymbol{\\alpha^*} = (\\alpha_1^* , \\alpha_2^*, \\dots, \\alpha_m^*)^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n\n\n\n(2) 计算\n\n\\boldsymbol{w^*} = \\sum_{i=1}^m \\alpha_i^* y_i \\boldsymbol{x_i}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n\n\n\n选择 \n\\boldsymbol{\\alpha^*}\n\n\n\n\n\n \n \n\n的一个分量\n\\alpha_j^*\n\n\n\n\n\n\n \n \n \n\n满足条件\n0&lt;\\alpha_j^* &lt; C\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n ，计算\n\nb^* = y_j -\\sum_{i=1}^m \\alpha_i^* y_i \\boldsymbol{x_i^Tx_j}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n \n\n\n \n \n\n\n \n \n \n\n \n \n\n\n\n\n(3) 求分离超平面\n\n\\boldsymbol{(w^*)^Tx }+b = 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n \n \n\n\n分类决策函数：\n\nf(x) = sign(\\boldsymbol{(w^*)^Tx }+b )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n \n\n\n序列最小优化(Sequential Minimal Optimization)(SMO)线性支持向量机的核函数为\nK(\\boldsymbol{x_i,x_j}) = \\boldsymbol{x_i^T x_j}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n\n\n \n\n\\min_{\\boldsymbol \\alpha} (  \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j K(\\boldsymbol{x_i,x_j}) -  \\sum_{i=1}^m \\alpha_i) \\\\\ns.t. \\ \\sum_{i=1}^m \\alpha_i y_i = 0 \\\\\n0 \\le \\alpha_i \\le C, \\ i=1,2, \\dots,m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n基本思路是：如果所有变量的解都满足此最优化问题的KKT条件，那么这么最优化问题的解就得到了。 \n令\n\nu_i = \\sum_{j=1}^m \\alpha_j y_j K(\\boldsymbol{x_j,x_i}) +b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n \n\n\n根据KKT条件可以得出二次规划问题中\n\\alpha_i\n\n\n\n\n\n \n \n\n的取值意义为\n\n\\begin{align}\n&amp;\\alpha_i = 0  \\Rightarrow \\xi_i&gt;0  \\Rightarrow  y_i u_i \\ge 1 \\Rightarrow 可行解区域内\\\\\n&amp;\\alpha_i = C  \\Rightarrow  y_i u_i = 1- \\xi_i  \\Rightarrow  y_i u_i \\le 1  \\Rightarrow 可行解区域外\\\\\n&amp;0&lt;\\alpha_i&lt;C  \\Rightarrow   \n\\begin{cases}\n\\xi_i = 0 \\\\\ny_i u_i = 1- \\xi_i\n\\end{cases} \n \\Rightarrow  y_i u_i = 1  \\Rightarrow 支持向量\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n可\n\n\n行\n\n\n解\n\n\n区\n\n\n域\n\n\n内\n\n\n\n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n可\n\n\n行\n\n\n解\n\n\n区\n\n\n域\n\n\n外\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n\n\n\n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n支\n\n\n持\n\n\n向\n\n\n量\n\n\n\n\n\n\n而最优解需要满足KKT条件，即上述3个条件都得满足，也就是说，如果存在不满足KKT条件的\n\\alpha_i\n\n\n\n\n\n \n \n\n，则需要更新该\n\\alpha_i\n\n\n\n\n\n \n \n\n，这是第一个约束条件。违背KKT条件的\n\\alpha_i\n\n\n\n\n\n \n \n\n用一下条件判断：\n\n\\alpha_i &lt; C \\ and \\ y_i u_i \\le 1 \\\\\n\\alpha_i &gt; 0 \\ and \\ y_i u_i \\ge 1 \\\\\n\\alpha_i = 0 \\ or \\ a_i = C \\ and \\ y_i u_i = 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n\n\n\n此外，更新的同时还要受到第二个约束条件的限制，即\n\n\\sum_{i=1}^m \\alpha_i y_i = 0\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n\n \n \n\n \n \n\n\n因为这个条件，我们同时更新两个\n\\alpha\n\n\n\n\n \n\n值，保证更新之后的值仍满足第二个约束条件，假设我们选择的两个乘子为\n\\alpha_1\n\n\n\n\n\n \n \n\n和\n\\alpha_2\n\n\n\n\n\n \n \n\n，其他变量\n\\alpha_i(i=3,4,\\dots,m)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n固定不变，则SMO的最优化问题可以写成\n\n\\min_{\\alpha_1 , \\alpha_2} W(\\alpha_1, \\alpha_2) = \\frac12 \\alpha_1^2 K_{11} + \\frac12\\alpha_2^2K_{22}+ \\alpha_1 \\alpha_2 y_1 y_2 K_{12} +y_1\\alpha_1 \\sum_{i=3}^m y_i \\alpha_i K_{i1} + y_2 \\alpha_2 \\sum_{i=3}^m y_i \\alpha_i K_{i2} - (\\alpha_1+\\alpha_2)\\\\\ns.t. \\ \\alpha_1 y_1 + \\alpha_2 y_2 = -\\sum_{i=3}^m \\alpha_i y_i = \\zeta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n \n \n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n \n \n\n\n\n \n \n \n\n\n \n\n \n \n\n\n \n\n\n\n \n \n\n\n\n \n \n \n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n \n\n\n \n \n\n \n \n\n\n\n其中， \nK_{ij}=K(\\boldsymbol{x_i,x_j})\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n\n \n\n，\n\\zeta\n\n\n\n\n \n\n为常数，目标函数中省略了不含\n\\alpha_1,\\alpha_2\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n的常数项（因为求导后等于0）。\n假设初始可行解为\n\\alpha_1^{old} , \\alpha_2^{old}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n，最优解为\n\\alpha_1^{new},\\alpha_2^{new}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n，且\n\\alpha_2^{new}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n需满足\nL \\le \\alpha_2^{new} \\le H\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}) , H=min(C,  \\alpha_2^{old}-\\alpha_1^{old}+C) , y_1 \\ne y_2 \\\\\nL = max(0,  \\alpha_2^{old}+\\alpha_1^{old}-C) , H = min(C, \\alpha_2^{old}+\\alpha_1^{old}) , y_1 = y_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n\n\n\n\n设\nE_i\n\n\n\n\n\n \n \n\n为误差项，\n\\eta\n\n\n\n\n \n\n为学习率，\n\nE_i = \\left( \\sum_{j=1}^m \\alpha_j K(\\boldsymbol{x_j,x_i})+b \\right) -y_i, i=1,2 \\\\\n\\eta = K_{11} + K_{22} - 2K_{12} = \\| \\phi(\\boldsymbol x_1) - \\phi(\\boldsymbol x_2) \\|^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n\n可得（具体证明可见《统计学习方法》定理7.6的证明）未经剪辑的解为 \n\n\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1-E_2)}{\\eta}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n\n\n考虑约束后，最终得到经剪辑的解为\n\n\\alpha_2^{new} = \n\\begin{cases}\nH,&amp;\\alpha_2^{new,unc}&gt;H \\\\\n\\alpha_2^{new,unc}, &amp;L \\le \\alpha_2^{new,unc} \\le H \\\\\nL, &amp;\\alpha_2^{new,unc}&lt;L\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n\n\n\n\n继而可得\n\n\\alpha_1^{new} = \\alpha_1^{old} + y_1 y_2 (\\alpha_2^{old}- \\alpha_2^{new})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n至此可得SMO算法中的一个子问题的最优解\n(\\alpha_1^{new},\\alpha_2^{new})\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n 。\n简化版SMO算法检查训练样本中每个点\n(x_i,y_i)\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n\n是否满足KKT条件，如果不满足，则它对应的\n\\alpha_i\n\n\n\n\n\n \n \n\n可以被优化，然后随机选择另一个变量\n\\alpha_j\n\n\n\n\n\n \n \n\n进行优化。\n完整版SMO算法先通过外循环来选择第一个\n\\alpha_i\n\n\n\n\n\n \n \n\n，并且选择过程会再两种方式之间进行交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界\n\\alpha\n\n\n\n\n \n\n实现单遍扫描。所谓非边界\n\\alpha\n\n\n\n\n \n\n即那些\n\\alpha \\ne 0 \\  and\\  \\alpha \\ne C\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n的\n\\alpha\n\n\n\n\n \n\n。对整个数据集扫描很容易，而实现非边界扫描，首先要建立这些\n\\alpha\n\n\n\n\n \n\n的列表，然后再对这个表进行遍历。同时，该步骤跳过那些已知的不会改变的\n\\alpha\n\n\n\n\n \n\n值。\n总结（1）计算误差\n\nE_i = \\left( \\sum_{j=1}^m \\alpha_j K(\\boldsymbol{x_j,x_i})+b \\right) -y_i, i=1,2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n\n \n \n \n\n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n \n \n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n\n（2）计算上下边界L，H\n\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}) , H=min(C,  \\alpha_2^{old}-\\alpha_1^{old}+C) , y_1 \\ne y_2 \\\\\nL = max(0,  \\alpha_2^{old}+\\alpha_1^{old}-C) , H = min(C, \\alpha_2^{old}+\\alpha_1^{old}) , y_1 = y_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n\n\n\n\n（3）计算学习率\n\n\\eta = K_{11} + K_{22} - 2K_{12} = \\| \\phi(\\boldsymbol x_1) - \\phi(\\boldsymbol x_2) \\|^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n（4）更新\n\\alpha_2\n\n\n\n\n\n \n \n\n\n\n\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1-E_2)}{\\eta} \\\\\n\\alpha_2^{new} = \n\\begin{cases}\nH,&amp;\\alpha_2^{new,unc}&gt;H \\\\\n\\alpha_2^{new,unc}, &amp;L \\le \\alpha_2^{new,unc} \\le H \\\\\nL, &amp;\\alpha_2^{new,unc}&lt;L\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n\n\n \n\n \n \n \n\n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n\n\n\n\n\n（5）更新\n\\alpha_1\n\n\n\n\n\n \n \n\n\n\n\\alpha_1^{new} = \\alpha_1^{old} + y_1 y_2 (\\alpha_2^{old}- \\alpha_2^{new})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n（6）更新\nb\n\n\n\n\n \n\n\n\nb_1^{new} =b^{old} -E_1 -y_1 K_{11}(\\alpha_1^{new}-\\alpha_1^{old}) - y_2 K_{12}(\\alpha_2^{new}-\\alpha_2^{old}) \\\\\nb_2^{new} = b^{old}-E_1 -y_1 K_{12}(\\alpha_1^{new}-\\alpha_1^{old}) - y_2 K_{22}(\\alpha_2^{new}-\\alpha_2^{old}) \\\\\nb^{new} = \\frac{b_1^{new}+b_2^{new}}2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n\n \n\n \n\n \n \n \n\n \n\n \n\n\n \n\n \n \n \n\n \n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n\n \n\n\n\n\n\nPython实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def loadDataSet(fileName):    dataMat = []    labelMat = []    fr = open(fileName)    for line in fr.readlines():        lineArr = line.strip().split('\\t')        dataMat.append([float(lineArr[0]), float(lineArr[1])])        labelMat.append(float(lineArr[2]))    return dataMat, labelMatdef selectJrandom(i,m):    j=i    while j==i:        j = int(random.uniform(0,m))    return jdef boundAlpha(alphaj, H, L):    if alphaj &gt; H:        alphaj = H    if alphaj &lt; L:        alphaj = L    return alphajdef SMOSimple(dataMat, classLabels, C, toler, maxIter):    dataMatrix = np.mat(dataMat)    labelMat = np.mat(classLabels).transpose()    b = 0    m,n = np.shape(dataMatrix)    alphas = np.mat(np.zeros((m,1)))    iter = 0    while iter &lt; maxIter:        alphaPairsChanged = 0        for i in range(m):\t\t\t            Ei = float(np.multiply(alphas,labelMat).T*dataMatrix*dataMatrix[i,:].T)+ b - float(labelMat[i])            if (labelMat[i]*Ei &lt; -toler and alphas[i]&lt;C) or (labelMat[i]*Ei &gt; toler and alphas[i]&gt;0):                j = selectJrandom(i,m)                Ej = float(np.multiply(alphas,labelMat).T*dataMatrix*dataMatrix[j,:].T)+ b - float(labelMat[j])                alphaIold = alphas[i].copy()                alphaJold = alphas[j].copy()                if labelMat[i] != labelMat[j]:                    L = max(0, alphas[j] - alphas[i])                    H = min(C, alphas[j] - alphas[i] + C)                else:                    L = max(0, alphas[j] + alphas[i] - C)                    H = min(C, alphas[j] + alphas[i])                if L==H:                    print \"L==H\"                    continue                eta =  dataMatrix[i,:]*dataMatrix[i,:].T + dataMatrix[j,:]*dataMatrix[j,:].T - 2.0 * dataMatrix[i,:] * dataMatrix[j,:].T                 if eta &lt;= 0:                    print \"eta&lt;=0\"                    continue                alphas[j] += labelMat[j] * (Ei - Ej)/eta                alphas[j] = boundAlpha(alphas[j], H, L)                if abs(alphas[j]-alphaJold) &lt; 0.00001 :                    print \"j not moving enough\"                    continue                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold-alphas[j])                b1 = b - Ei - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T \\\t\t\t\t            - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T                b2 = b - Ej - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T \\\t\t\t\t            - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T                if 0 &lt; alphas[i] &lt; C :                    b = b1                elif 0 &lt; alphas[j] &lt; C :                    b = b2                else:                    b = 0.5*(b1+b2)                alphaPairsChanged += 1                print \"iter: %d i: %d, pairs changed %d\" % (iter, i, alphaPairsChanged)        if alphaPairsChanged == 0:            iter += 1        else:            iter = 0\t        print \"iteration number: %d\" % iter    return b, alphas\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class OptStruct:    def __init__(self, dataMat, classLabels, C, toler):        self.X = dataMat        self.labelMat = classLabels        self.C = C        self.toler = toler        self.m = np.shape(dataMat)[0]        self.alphas = np.mat(np.zeros((self.m, 1)))        self.b = 0        self.eCache = np.mat(np.zeros((self.m, 2))) # 误差缓存def calcEi(optStruct, i):    Ei = float(np.multiply(optStruct.alphas, optStruct.labelMat).T*(optStruct.X*optStruct.X[i,:].T)) + optStruct.b - float(optStruct.labelMat[i])    return Eidef selectJ(i, optStruct, Ei):    maxK = -1    maxDeltaE = 0    Ej = 0    optStruct.eCache[i] = [1, Ei]    validECacheList = np.nonzero(optStruct.eCache[:,0].A)[0]    if len(validECacheList) &gt; 1:        for k in validECacheList:            if k == i:                continue            Ek = calcEi(optStruct, k)            deltaE = abs(Ei - Ek)            if deltaE &gt; maxDeltaE:                maxK = k                maxDeltaE = deltaE                Ej = Ek        return maxK, Ej    else:        j = selectJrandom(i, optStruct.m)        Ej = calcEi(optStruct, j)    return j, Ejdef UpdateEk(optStruct, k):    Ek = calcEi(optStruct, k)    optStruct.eCache[k] = [1,Ek]def innerL(i, optStruct):    Ei = calcEi(optStruct, i)    if(optStruct.labelMat[i]*Ei &lt; - optStruct.toler and optStruct.alphas[i] &lt; optStruct.C) or (optStruct.labelMat[i]*Ei &gt; optStruct.toler and optStruct.alphas[i] &gt; 0):        j, Ej = selectJ(i, optStruct, Ei)        alphaIold = optStruct.alphas[i].copy()        alphaJold = optStruct.alphas[j].copy()        if optStruct.labelMat[i] != optStruct.labelMat[j]:            L = max(0, alphaJold - alphaIold)            H = min(optStruct.C, alphaJold - alphaIold + optStruct.C)        else:            L = max(0, alphaJold + alphaIold - optStruct.C)            H = min(optStruct.C, alphaJold + alphaIold)        if L==H:            print \"L==H\"            return 0        eta = optStruct.X[i,:]*optStruct.X[i,:].T + optStruct.X[j,:]*optStruct.X[j,:].T - 2.0 * optStruct.X[i,:]* optStruct.X[j,:].T        if eta &lt;= 0:            print \"eta &lt;= 0\"            return 0        optStruct.alphas[j] += optStruct.labelMat[j]*(Ei-Ej)/eta        optStruct.alphas[j] = boundAlpha(optStruct.alphas[j], H, L)        UpdateEk(optStruct, j)\t\t        if abs(optStruct.alphas[j] - alphaJold) &lt; 0.00001:            print \"j not moving enough\"            return 0        optStruct.alphas[i] += optStruct.labelMat[j]*optStruct.labelMat[i]*(alphaJold - optStruct.alphas[j])        UpdateEk(optStruct, i)        b1 = optStruct.b - Ei - optStruct.labelMat[i]*(optStruct.alphas[i]-alphaIold)*optStruct.X[i,:]*optStruct.X[i,:].T \\\t\t                      - optStruct.labelMat[j]*(optStruct.alphas[j]-alphaJold)*optStruct.X[i,:]*optStruct.X[j,:].T\t        b2 = optStruct.b - Ei - optStruct.labelMat[i]*(optStruct.alphas[i]-alphaIold)*optStruct.X[i,:]*optStruct.X[j,:].T \\\t\t                      - optStruct.labelMat[j]*(optStruct.alphas[j]-alphaJold)*optStruct.X[j,:]*optStruct.X[j,:].T\t        if 0 &lt; optStruct.alphas[i] &lt; optStruct.C :            optStruct.b = b1        elif 0 &lt; optStruct.alphas[j] &lt; optStruct.C :            optStruct.b = b2        else:            optStruct.b = 0.5*(b1+b2)\t\t        return 1    else:        return 0def SMOPlatt(dataMat, classLabels, C, toler, maxIter, kTup=('lin', 0)):    optStruct = OptStruct(np.mat(dataMat),np.mat(classLabels).transpose(), C, toler)    iter = 0    entireSet = True    alphaPairsChanged = 0    while iter &lt; maxIter and (alphaPairsChanged &gt; 0 or entireSet):        alphaPairsChanged = 0        if entireSet:            for i in range(optStruct.m):                alphaPairsChanged += innerL(i, optStruct)            print \"fullSet, iter: %d i: %d, pairs changed %d\" %(iter, i, alphaPairsChanged)            iter += 1        else:            nonBoundIs = np.nonzero((optStruct.alphas.A &gt; 0)*(optStruct.alphas.A &lt; C))[0]            for i in nonBoundIs:                alphaPairsChanged += innerL(i, optStruct)                print \"non-bound, iter: %d i: %d, pairs changed %d\" %(iter, i, alphaPairsChanged)            iter += 1        if entireSet:            entireSet = False        elif alphaPairsChanged == 0:            entireSet = True        print \"iteration number: %d\" % iter    return optStruct.b,optStruct.alphas\n\n径向基核函数\nK(\\boldsymbol{x,y}) = exp(-\\frac{\\| \\boldsymbol{x-y} \\|^2}{2\\sigma^2})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n\n\n\n \n\n \n \n \n\n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\nPython实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128class OptStruct:    def __init__(self, dataMat, classLabels, C, toler, kTuple):        self.X = dataMat        self.labelMat = classLabels        self.C = C        self.toler = toler        self.m = np.shape(dataMat)[0]        self.alphas = np.mat(np.zeros((self.m, 1)))        self.b = 0        self.eCache = np.mat(np.zeros((self.m, 2))) # 误差缓存        self.K = np.mat(np.zeros((self.m, self.m)))        for i in range(self.m):            self.K[:, i] = kernelTransform(self.X, self.X[i,:], kTuple)def calcEi(optStruct, i):    Ei = float(np.multiply(optStruct.alphas, optStruct.labelMat).T*optStruct.K[:,i]) + optStruct.b - float(optStruct.labelMat[i])    return Eidef selectJ(i, optStruct, Ei):    maxK = -1    maxDeltaE = 0    Ej = 0    optStruct.eCache[i] = [1, Ei]    validECacheList = np.nonzero(optStruct.eCache[:,0].A)[0]    if len(validECacheList) &gt; 1:        for k in validECacheList:            if k == i:                continue            Ek = calcEi(optStruct, k)            deltaE = abs(Ei - Ek)            if deltaE &gt; maxDeltaE:                maxK = k                maxDeltaE = deltaE                Ej = Ek        return maxK, Ej    else:        j = selectJrandom(i, optStruct.m)        Ej = calcEi(optStruct, j)    return j, Ejdef UpdateEk(optStruct, k):    Ek = calcEi(optStruct, k)    optStruct.eCache[k] = [1,Ek]def innerL(i, optStruct):    Ei = calcEi(optStruct, i)    if(optStruct.labelMat[i]*Ei &lt; - optStruct.toler and optStruct.alphas[i] &lt; optStruct.C) or (optStruct.labelMat[i]*Ei &gt; optStruct.toler and optStruct.alphas[i] &gt; 0):        j, Ej = selectJ(i, optStruct, Ei)        alphaIold = optStruct.alphas[i].copy()        alphaJold = optStruct.alphas[j].copy()        if optStruct.labelMat[i] != optStruct.labelMat[j]:            L = max(0, alphaJold - alphaIold)            H = min(optStruct.C, alphaJold - alphaIold + optStruct.C)        else:            L = max(0, alphaJold + alphaIold - optStruct.C)            H = min(optStruct.C, alphaJold + alphaIold)        if L==H:            print \"L==H\"            return 0        eta = optStruct.K[i,i] + optStruct.K[j,j] - 2.0 * optStruct.K[i,j]        if eta &lt;= 0:            print \"eta &lt;= 0\"            return 0        optStruct.alphas[j] += optStruct.labelMat[j]*(Ei-Ej)/eta        optStruct.alphas[j] = boundAlpha(optStruct.alphas[j], H, L)        UpdateEk(optStruct, j)\t\t        if abs(optStruct.alphas[j] - alphaJold) &lt; 0.00001:            print \"j not moving enough\"            return 0        optStruct.alphas[i] += optStruct.labelMat[j]*optStruct.labelMat[i]*(alphaJold - optStruct.alphas[j])        UpdateEk(optStruct, i)        b1 = optStruct.b - Ei - optStruct.labelMat[i]*(optStruct.alphas[i]-alphaIold)*optStruct.K[i,i] \\\t\t                      - optStruct.labelMat[j]*(optStruct.alphas[j]-alphaJold)*optStruct.K[i,j]        b2 = optStruct.b - Ei - optStruct.labelMat[i]*(optStruct.alphas[i]-alphaIold)*optStruct.K[i,j] \\\t\t                      - optStruct.labelMat[j]*(optStruct.alphas[j]-alphaJold)*optStruct.K[j,j]        if 0 &lt; optStruct.alphas[i] &lt; optStruct.C :            optStruct.b = b1        elif 0 &lt; optStruct.alphas[j] &lt; optStruct.C :            optStruct.b = b2        else:            optStruct.b = 0.5*(b1+b2)\t\t        return 1    else:        return 0def SMOPlatt(dataMat, classLabels, C, toler, maxIter, kTuple=('lin', 0)):    optStruct = OptStruct(np.mat(dataMat),np.mat(classLabels).transpose(), C, toler, kTuple)    iter = 0    entireSet = True    alphaPairsChanged = 0    while iter &lt; maxIter and (alphaPairsChanged &gt; 0 or entireSet):        alphaPairsChanged = 0        if entireSet:            for i in range(optStruct.m):                alphaPairsChanged += innerL(i, optStruct)            print \"fullSet, iter: %d i: %d, pairs changed %d\" %(iter, i, alphaPairsChanged)            iter += 1        else:            nonBoundIs = np.nonzero((optStruct.alphas.A &gt; 0)*(optStruct.alphas.A &lt; C))[0]            for i in nonBoundIs:                alphaPairsChanged += innerL(i, optStruct)                print \"non-bound, iter: %d i: %d, pairs changed %d\" %(iter, i, alphaPairsChanged)            iter += 1        if entireSet:            entireSet = False        elif alphaPairsChanged == 0:            entireSet = True        print \"iteration number: %d\" % iter    return optStruct.b,optStruct.alphasdef kernelTransform(X, A, kTuple):    m,n = np.shape(X)    K = np.mat(np.zeros((m,1)))    if kTuple[0] == 'lin':         K = X*A.T    elif kTuple[0] == 'rbf':        for j in range(m):            deltaRow = X[j,:] - A            K[j] = deltaRow * deltaRow.T        K = np.exp(K/(-1*kTuple[1]**2))    else:        raise NameError('That Kernel is not recognized')    return K\n\n1234567891011121314151617def testRbf(k1 = 1.3):    dataArr, labelArr = loadDataSet('testSetRBF.txt')    b, alphas = SMOPlatt(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))    dataMat = np.mat(dataArr)    labelMat = np.mat(labelArr).transpose()    supportVectorIndex = np.nonzero(alphas.A&gt;0)[0]    supportVectors = dataMat[supportVectorIndex]    labelSupportVector = labelMat[supportVectorIndex]    print 'there are %d Support Vectors' % np.shape(supportVectors)[0]    m,n = np.shape(dataMat)    errorCount = 0    for i in range(m):        kernelEval = kernelTransform(supportVectors, dataMat[i,:],('rbf',k1))        predict = kernelEval.T * np.multiply(labelSupportVector, alphas[supportVectorIndex])+b        if np.sign(predict) != np.sign(labelArr[i]):         errorCount += 1    print 'the training error rate is: %f' %(float(errorCount)/m)\n\n","plink":"https://yuxinzhao.net/support-vecter-machine/"},{"title":"Quick Sort 快速排序","date":"2019-10-11T01:40:08.000Z","updated":"2022-01-04T08:35:48.635Z","content":"基于比较的排序问题的最快时间复杂度为 \nO(n \\log n)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n\n, 而快速排序的平均复杂度为 \nO(n \\log n)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n\n。\n\n\n时间复杂度\n最佳情况： \nO(n)\n\n\n\n\n\n\n\n \n \n \n \n\n\n最坏情况： \nO(n^2)\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n\n平均情况： \nO(n \\log n)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n\n\n\n代码实现C++ 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;void Swap(int* a, int *b) &#123;    int temp = *a;    *a = *b;    *b = temp; &#125;void QuickSort(int arr[], int len) &#123;    if (len &lt; 2) &#123;        return;    &#125;        int front = 0;    int back = len - 1;    for(; front &lt; back; back--)&#123;        if (arr[front] &gt; arr[back]) &#123;            Swap(&amp;arr[front], &amp;arr[back]);            front++;            for(; front &lt; back; front++) &#123;                if (arr[front] &gt; arr[back]) &#123;                    Swap(&amp;arr[front], &amp;arr[back]);                    break;                &#125;            &#125;        &#125;    &#125;        QuickSort(arr, front);    QuickSort(&amp;arr[front + 1], len-front - 1); // front最大会超出边界=len，但是此时len-front-1=-1，进入函数后新的len&lt;2会直接返回，不会发生数组越界&#125;void Test()&#123;    int arr[] = &#123;4, 2, 3, 1, 2, 3, 2&#125;;        std::cout &lt;&lt; \"---before sort---\\n\";    for (int i = 0; i &lt; 7; i++) &#123;        std::cout &lt;&lt; arr[i] &lt;&lt; std::endl;    &#125;    QuickSort(arr, 7);    std::cout &lt;&lt; \"---after sort---\\n\";    for (int i = 0; i &lt; 7; i++) &#123;        std::cout &lt;&lt; arr[i] &lt;&lt; std::endl;    &#125;&#125;int main() &#123;    Test();    return 0;&#125;\n\n","plink":"https://yuxinzhao.net/quick-sort/"},{"title":"Quickhull 快包算法解决凸包问题","date":"2019-10-06T12:10:37.000Z","updated":"2022-01-04T08:35:48.635Z","content":"凸包问题 (convex-hull problem) 是为一个有 n 个点的集合构造凸包的问题, 或者更直观的解释：求能够完全包含平面上 n 个给定点的凸多边形。\n\n\n定义凸集合定义：  对于平面上的一个点集合 (有限或者无限)， 如果以集合中任意两点 \np\n\n\n\n\n \n\n 和 \nq\n\n\n\n\n \n\n 为端点的线段都属于该集合，则这个集合是凸集合。 \n凸包(convex hull)定义： 包含点集合 \nS\n\n\n\n\n \n\n 的最小凸集合 (“最小”指的是 \nS\n\n\n\n\n \n\n的凸包一定是所有包含 \nS\n\n\n\n\n \n\n 的凸集合的子集)。\n\n直观地讲，对于平面上 \nn\n\n\n\n\n \n\n 个点的集合，它的凸包就是包含所有这些点的最小凸多边形。\n\n凸包的极点(extreme point)定义：  对于任何以集合中的点为端点的线段来说，它们不可能是这种线段的中点。\n\n直观地讲，凸包形成的凸多边形的各个顶点就是凸包的极点。 \n\n\n算法假设集合 \nS\n\n\n\n\n \n\n 是平面上 \nn&gt;1\n\n\n\n\n\n\n \n \n \n\n 个点 \np_1(x_1, y_1), \\cdots, p_n(x_n, y_n)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n 构成的，假设这些点是按照x轴坐标升序排列的，如果x坐标相同则按照y坐标排列。由此，最左边的点必定是 \np_1\n\n\n\n\n\n \n \n\n, 最右边的点必定是 \np_n\n\n\n\n\n\n \n \n\n，这两个点必定是凸包的两个极点。\n直线 \n\\overrightarrow{p_1 p_n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n 将集合分为两个子集：上集合 \nS_1\n\n\n\n\n\n \n \n\n,  下集合 \nS_2\n\n\n\n\n\n \n \n\n 。两个子集对应的凸包我们称之为 上包 (upper hull) 和 下包 (lower hull)。\n以下解释构建上包的原理，下包同理：\n\n如果 \nS_1\n\n\n\n\n\n \n \n\n 为空，上包就是以 \np_1\n\n\n\n\n\n \n \n\n 和 \np_n\n\n\n\n\n\n \n \n\n 为端点的线段。\n如果 \nS_1\n\n\n\n\n\n \n \n\n 不为空，找到 \nS_1\n\n\n\n\n\n \n \n\n 中的距离直线 \n\\overrightarrow{p_1 p_n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n 最远的点作为上包的顶点 \np_{max}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n 。如果距离最远的点有多个则选择能使 \n\\angle p_{max} p_1 p_n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n \n\n\n\n \n \n\n\n \n \n\n\n 最大的点。\n接着求 \n\\overrightarrow{p_1 p_{max}}\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n\n\n 和 \n\\overrightarrow{p_{max} p_n}\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n \n \n\n\n\n \n\n \n\n \n\n\n  的上包，递归执行。每次求上包都能找到 \nS\n\n\n\n\n \n\n 上包的一个顶点，最终找到 \nS\n\n\n\n\n \n\n 上包的所有顶点。\n下包进行同样的操作，最终即可获得 \nS\n\n\n\n\n \n\n 的凸包的所有顶点。凸包构建完成。\n\n\nTips运用行列式\n\n\\begin{vmatrix} \nx_1 &amp; y_1 &amp; 1 \\\\\nx_2 &amp; y_2 &amp; 1 \\\\\nx_3 &amp; y_3 &amp; 1 \n\\end{vmatrix}\n= x_1 y_2 + x_3 y_1 + x_2 y_3 - x_3 y_2 - x_2 y_1 - x_1 y_3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n \n \n \n\n\n\n \n\n \n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\n可以在固定时间内判断 点 \np_3=(x_3, y_3)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n 是在直线 \n\\overrightarrow{p_1 p_2}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n 的左侧还是右侧，且可得到该点到直线的距离。当且仅当 \np_3\n\n\n\n\n\n \n \n\n 位于直线 \n\\overrightarrow{p_1 p_2}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n 的左侧时，行列式值为正。点 \np_3\n\n\n\n\n\n \n \n\n 到直线 \n\\overrightarrow{p_1 p_2}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n\n \n\n \n\n\n 的距离为行列式的绝对值。\n时间复杂度\n最佳情况：\n\\Theta (n)\n\n\n\n\n\n\n\n \n \n \n \n\n\n最坏情况：\n\\Theta (n^2)\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n\n\n实现C++实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;struct Vertex &#123;    double x;    double y;&#125;;/*  Compare two vertices, determine whether vertex a &lt; b */bool CompareVertex(Vertex a, Vertex b) &#123;    if (a.x == b.x) &#123;        return a.y &lt; b.y;    &#125;    else &#123;        return a.x &lt; b.x;    &#125;&#125;/* Find the upper or lower hull */ std::vector&lt;Vertex&gt; __findHull(std::vector&lt;Vertex&gt; originSet, Vertex p1, Vertex p2, bool findUpper)&#123;    std::vector&lt;Vertex&gt; upperSet;    Vertex p_max;    double maxDistance = -1.0;    for (auto iter = originSet.begin(); iter != originSet.end(); iter++) &#123;        double distance = p1.x * p2.y + (iter-&gt;x) * p1.y + p2.x * (iter-&gt;y) - (iter-&gt;x) * p2.y - p2.x * p1.y - p1.x * (iter-&gt;y);        if ((findUpper &amp;&amp; distance &gt; 0.0) || (!findUpper &amp;&amp; distance &lt; 0.0)) &#123;            upperSet.push_back(*iter);            distance = std::abs(distance);            if (distance &gt; maxDistance) &#123;                p_max = (*iter);                maxDistance = distance;            &#125;        &#125;    &#125;        if (maxDistance &gt; 0.0) &#123;        std::vector&lt;Vertex&gt; SubHull1 = __findHull(upperSet, p1, p_max, findUpper);        std::vector&lt;Vertex&gt; SubHull2 = __findHull(upperSet, p_max, p2, findUpper);        SubHull1.insert(SubHull1.begin(), SubHull2.begin(), SubHull2.end());        SubHull1.push_back(p_max);        return SubHull1;    &#125;    else &#123;        return std::vector&lt;Vertex&gt;(); // empty    &#125;&#125;/* Algorithm: Qucik Hull *  * Args: *     originSet (std::vector&lt;Vertex&gt;): set of n vertices  *  * Outputs: *     std::vector&lt;Vertex&gt;: convex hull */std::vector&lt;Vertex&gt; QuickHull(std::vector&lt;Vertex&gt; originSet)&#123;    // sort all vertice    std::sort(originSet.begin(), originSet.end(), CompareVertex);    // find upper and lower hulls    std::vector&lt;Vertex&gt; SubHull1 = __findHull(originSet, originSet.front(), originSet.back(), true);    std::vector&lt;Vertex&gt; SubHull2 = __findHull(originSet, originSet.front(), originSet.back(), false);    // combine upper and lower hulls    SubHull1.insert(SubHull1.begin(), SubHull2.begin(), SubHull2.end());    SubHull1.push_back(originSet.front());    SubHull1.push_back(originSet.back());    return SubHull1;&#125;void test() &#123;    std::vector&lt;Vertex&gt; originSet = &#123;&#123;1.0, 1.0&#125;, &#123;2.0, 2.0&#125;, &#123;3.0, 1.0&#125;, &#123;3.0, 3.0&#125; , &#123;1.0, 3.0&#125;&#125;;        originSet = QuickHull(originSet);    for (auto vertex: originSet)&#123;        std::cout &lt;&lt; vertex.x &lt;&lt; ' ' &lt;&lt; vertex.y &lt;&lt; std::endl;     &#125;&#125;int main() &#123;    test();    return 0;&#125;\n\n","thumbnail":"/static/image/quickhull.png","plink":"https://yuxinzhao.net/quickhull/"},{"title":"Sieve of Eratosthenes 埃拉托色尼筛选法","date":"2019-10-06T01:25:24.000Z","updated":"2022-01-04T08:35:48.647Z","content":"埃拉托色尼筛选法 (sieve of Eratosthenes) 由古希腊人发明，它可以给出小于 n 的所有质数。\n\n\n\n\n基本思想该算法首先初始化一个 2 到 \nn\n\n\n\n\n \n\n 的连续整数序列，作为候选质数。接下来从2开始往后循环，每个循环，它会将 2 到 \nn\n\n\n\n\n \n\n 范围内该数的所有倍数从序列中剔除。若循环到一个已经被去除的数，则跳过(因为这个数字在之前的循环中已经被剔除，这意味着它的倍数也必定被剔除了，没有必要再剔除一遍)。实际上只需要从 2 循环到 \n\\lfloor \\sqrt{n} \\rfloor\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n 即可，因为大于 \n\\lfloor \\sqrt{n} \\rfloor\n\n\n\n\n\n\n\n \n\n \n\n \n\n \n\n 的整数在 2 到 \nn\n\n\n\n\n \n\n 的范围不存在倍数。\n伪代码\n\\begin{array}{l}\n\\hline\n{\\textbf{Algorithm   Sieve of Eratosthenes}  } \\\\\n\\hline\n{\\textbf{Input:} \\quad \\text{一个整数} \\ n \\quad (n&gt;1) } \\\\\n{\\textbf{Output:} \\quad \\text{包含所有小于等于} \\ n \\ \\text{的质数的数组} \\ L } \\\\\n\n{\\textbf{function} \\quad \\text{Sieve} (n)} \\\\\n{\\qquad \\textbf{for} \\enspace i \\leftarrow 2 \\enspace \\textbf{to} \\enspace n \\enspace \\textbf{do}}\\\\\n{\\qquad \\qquad A[i] \\leftarrow i }\\\\\n{\\qquad \\textbf{end for} } \\\\\n{\\qquad \\textbf{for} \\enspace i \\leftarrow 2 \\enspace \\textbf{to} \\enspace \\lfloor \\sqrt{n} \\rfloor \\enspace \\textbf{do}  } \\\\\n{\\qquad \\qquad \\textbf{if} \\enspace A[i] \\ne 0} \\enspace \\textbf{do} \\\\\n{\\qquad \\qquad \\qquad j \\leftarrow i * i} \\\\\n{\\qquad \\qquad \\qquad \\textbf{while} \\enspace j \\le n \\enspace \\textbf{do}} \\\\\n{\\qquad \\qquad \\qquad \\qquad A[j] \\leftarrow 0} \\\\\n{\\qquad \\qquad \\qquad \\qquad j \\leftarrow j + i} \\\\\n{\\qquad \\qquad \\qquad \\textbf{end while}} \\\\\n{\\qquad \\qquad \\textbf{end if}} \\\\\n{\\qquad \\textbf{end for}} \\\\\n{\\qquad j \\leftarrow 0 } \\\\\n{\\qquad \\textbf{for} \\enspace i \\leftarrow 2 \\enspace \\textbf{to} \\enspace n \\enspace \\textbf{do}}\\\\\n{\\qquad \\qquad \\textbf{if} \\enspace A[i] \\ne 0 \\enspace \\textbf{do}}\\\\\n{\\qquad \\qquad \\qquad L[j] \\leftarrow i} \\\\\n{\\qquad \\qquad \\qquad j \\leftarrow j + 1} \\\\\n{\\qquad \\qquad \\textbf{end if}}\\\\\n{\\qquad \\textbf{end for} } \\\\\n{\\qquad \\textbf{return} \\enspace L } \\\\\n{\\textbf{end function} } \\\\\n\\hline\n\\end{array}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n\n一\n\n个\n\n\n整\n\n\n数\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n包\n\n含\n\n\n所\n\n\n有\n\n\n小\n\n\n于\n\n\n等\n\n\n于\n\n\n \n\n的\n\n质\n\n\n数\n\n\n的\n\n\n数\n\n\n组\n\n\n \n\n\n \n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \n \n \n\n\n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n \n \n \n \n\n\n\n \n \n \n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n\n \n\n \n\n \n \n\n\n\n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n\n \n \n \n\n\n\n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n\n\n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n\n\n \n \n \n \n \n\n\n\n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n时间复杂度\nO (n \\log \\log n)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n \n \n \n\n \n \n\n\n实现C++ 实现12345678910111213141516171819202122232425262728293031323334353637/* Algorithm: Sieve of Eratosthenes *  * Args: *    n (int): a integer (please ensure that n &gt; 1) *    L (int*): an array to store result (please ensure that the array is enough to store all prime less than n) *  * Returns: *    number of prime that less than n */int sieve(int n, int* L) &#123;    int* A = new int[n+1];    for (int i = 2; i &lt;= n; i++) &#123;        A[i] = i;    &#125;    int sqrt_n = int(std::sqrt(n));    for (int i = 2; i &lt;= sqrt_n; i++) &#123;        if (A[i] != 0) &#123;            int j = i * i;            while (j &lt;= n) &#123;                A[j] = 0;                j = j + i;            &#125;        &#125;    &#125;    int j = 0;    for (int i = 2; i &lt;= n; i++) &#123;        if (A[i] != 0) &#123;            L[j] = i;            j += 1;        &#125;    &#125;    return j;&#125;\n\n","plink":"https://yuxinzhao.net/sieve-of-eratosthenes/"},{"title":"Matrix Calculus 矩阵微积分","date":"2019-10-02T21:50:33.000Z","updated":"2022-01-04T08:35:48.603Z","content":"本文主要介绍矩阵如何求导。\n我们可用网站帮助我们对矩阵进行求导，但更重要的是我们要掌握矩阵求导的基本理论。\n\n\n\n\n\n\n标量对矩阵求导定义从定义来考虑，标量 \nf\n\n\n\n\n \n\n 对矩阵 \nX\n\n\n\n\n \n\n 的导数定义为 \nf\n\n\n\n\n \n\n 对 \nX\n\n\n\n\n \n\n 逐元素求导排成与 \nX\n\n\n\n\n \n\n 尺寸相同的矩阵：\n\n\\frac{\\partial f}{\\partial X} = \\left[ \\frac{\\partial f}{\\partial X_{ij}} \\right]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n \n\n\n\n但这个定义并不好计算，因此我们试图寻找一个从整体出发、不需要在求导时拆开矩阵的算法。、\n一元微积分中导数与微分的关系：\n\n\\mathrm{d}x = f'(x) \\mathrm{d}x\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n \n \n\n\n多元微积分中梯度(标量对向量的导数)与微分的关系：\n\n\\mathrm{d} f = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} \\mathrm{d} x_i = \\left( \\frac{\\partial f}{\\partial \\boldsymbol{x}} \\right)^T \\mathrm{d} \\boldsymbol{x}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n \n\n \n \n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n\n\n受此启发，我们将矩阵导数与微分建立联系：\n\n\\mathrm{d} f = \\sum_{i=1}^m \\sum_{j=1}^n \\frac{\\partial f}{\\partial X_{ij}} \\mathrm{d} X_{ij} = \\mathrm{tr} \\left( \\left(\\frac{\\partial f}{\\partial X}\\right)^T \\mathrm{d} X \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n\n \n\n\n\n\n\n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n \n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n\n\n\n\n其中 \n\\mathrm{tr}(\\cdot)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 代表迹(trace)即方阵对角线元素之和，满足性质：\n\n对于尺寸相同的矩阵 \nA\n\n\n\n\n \n\n, \nB\n\n\n\n\n \n\n, \n\\mathrm{tr}(A^T B)\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n\n 是矩阵 \nA\n\n\n\n\n \n\n, \nB\n\n\n\n\n \n\n 的内积：\n\n\\mathrm{tr}(A^T B) = \\sum_{i,j} A_{ij} B_{ij}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n\n \n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n矩阵微分的运算法则\n加减法： \n\\mathrm{d} (X \\pm Y) = \\mathrm{d} X \\pm \\mathrm{d} Y\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n矩阵乘法： \n\\mathrm{d}(XY) = (\\mathrm{d} X) Y + X\\mathrm{d}Y\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n转置： \n\\mathrm{d}(X^T) = (\\mathrm{d} X)^T\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n\n\n\n逆：   \n\\mathrm{d} (X^{-1}) = -X^{-1}(\\mathrm{d}X)X^{-1}\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n \n \n \n\n \n\n \n \n\n\n\n \n\n迹： \n\\mathrm{d} \\left( \\mathrm{tr}(X) \\right) = \\mathrm{tr} (\\mathrm{d} X)\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n \n\n \n \n \n \n\n \n\n \n \n\n \n \n \n \n\n\n\n行列式： \n\\mathrm{d} | X | = \\mathrm{tr} \\left( X^* \\mathrm{d} X \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n \n\n \n \n\n \n \n \n\n\n， 其中 \nX^*\n\n\n\n\n\n \n \n\n 表示 \nX\n\n\n\n\n \n\n 的伴随矩阵，在 \nX\n\n\n\n\n \n\n 可逆时又可写作 \n\\mathrm{d} | X | = \\mathrm{tr} \\left( X^{-1} \\mathrm{d} X \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n\n\n\nKronecker乘积：   \n\\mathrm{d} (X \\otimes Y) = \\mathrm{d} X \\otimes Y + X \\otimes \\mathrm{d} Y \n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n逐元素乘法： \n\\mathrm{d} (X \\odot Y) = \\mathrm{d} X \\odot Y + X \\odot \\mathrm{d} Y\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n，\n\\odot\n\n\n\n\n \n\n 表示尺寸相同的矩阵\nX\n\n\n\n\n \n\n, \nY\n\n\n\n\n \n\n逐元素相乘\n\n逐元素函数： \n\\mathrm{d} \\sigma(X) = \\sigma'(X) \\odot \\mathrm{d} X\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n，其中，\n\\sigma(X) = \\left[ \\sigma(X_{ij}) \\right]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n\n\n , \n\\sigma'(X) = \\left[ \\sigma'(X_{ij}) \\right]\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n\n\n \n \n\n\n\n\n\n迹的运算法则\n标量的迹： \na = \\mathrm{tr}(a)\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n\n\n转置： \n\\mathrm{tr} \\left( A^T \\right) = \\mathrm{tr}(A)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n\n \n \n \n\n\n\n线性： \n\\mathrm{tr}(A \\pm B) = \\mathrm{tr}(A) \\pm \\mathrm{tr}(B)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n\n\n\n矩阵乘法交换： \n\\mathrm{tr}(AB) = \\mathrm{tr}(BA)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n矩阵乘法/逐元素乘法交换： \n\\mathrm{tr} \\left( A^T (B \\odot C) \\right) = \\mathrm{tr} \\left( (A \\odot B)^T C \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n\n\n \n \n \n \n \n\n \n \n\n \n \n\n\n, 其中 \nA\n\n\n\n\n \n\n, \nB\n\n\n\n\n \n\n, \nC\n\n\n\n\n \n\n 尺寸相同，等号两侧都等于 \n\\sum_{ij} A_{ij} B_{ij} C_{ij}\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n求解方法若标量函数 \nf\n\n\n\n\n \n\n 是矩阵 \nX\n\n\n\n\n \n\n 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 \nf\n\n\n\n\n \n\n 求微分，再使用迹技巧给 \ndf\n\n\n\n\n\n \n \n\n 套上迹并使其他项交换至 \n\\mathrm{d} X\n\n\n\n\n\n \n \n\n 左侧，对照导数与微元的联系 \n\\mathrm{d} f = \\mathrm{tr} \\left( \\left(\\frac{\\partial f}{\\partial X}\\right)^T \\mathrm{d} X \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n\n\n， 即可求得导数。\n复合： 假设已求得 \n\\frac{\\partial f}{\\partial Y}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n， 而 \nY\n\n\n\n\n \n\n 是 \nX\n\n\n\n\n \n\n 的函数，求 \n\\frac{\\partial f}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n。 此时，由链式法则 \n\\frac{\\partial f}{\\partial X} = \\frac{\\partial f}{\\partial Y} \\frac{\\partial Y}{\\partial X}\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n似乎可求解，但问题是 \n\\frac{\\partial Y}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 我们目前还没有定义，因此此处不能用链式法则，我们需要找其他方法。我们从微分入手建立复合法则，先写出 \n\\mathrm{d} f = \\mathrm{tr} \\left( \\left(\\frac{\\partial f}{\\partial Y} \\right)^T \\mathrm{d} Y \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n\n\n ，再将 \n\\mathrm{d} Y\n\n\n\n\n\n \n \n\n 用 \n\\mathrm{d} X\n\n\n\n\n\n \n \n\n 表示出来代入，并用迹的运算法则将其他项交换到 \n\\mathrm{d} X\n\n\n\n\n\n \n \n\n 左侧， 即可得到 \n\\frac{\\partial f}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n。\n例子：\n以 \nY = AXB\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n 为例， 此时\n由于 \nA\n\n\n\n\n \n\n, \nB\n\n\n\n\n \n\n 为常量，\n\\mathrm{d} A = 0\n\n\n\n\n\n\n\n \n \n \n \n\n, \n\\mathrm{d} B = 0\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\\mathrm{d} Y = (\\mathrm{d} A) X B + A(\\mathrm{d}X)B + AX(\\mathrm{d}B) = A(\\mathrm{d}X)B\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\\begin{align}\n\\mathrm{d} f &amp;= \\mathrm{tr} \\left( \\left(\\frac{\\partial f}{\\partial Y} \\right)^T \\mathrm{d} Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left(\\frac{\\partial f}{\\partial Y} \\right)^T A (\\mathrm{d} X) B \\right) \\\\\n&amp;=  \\mathrm{tr} \\left( B\\left(\\frac{\\partial f}{\\partial Y} \\right)^T A \\mathrm{d} X \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left(A^T \\frac{\\partial f}{\\partial Y} B^T \\right)^T  \\mathrm{d} X \\right)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n\n \n \n \n\n\n\n\n\n\n可得 \n\\frac{\\partial f}{\\partial X} = A^T \\frac{\\partial f}{\\partial Y} B^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n 。\n案例例1：   \nf = A^T X B\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n,   \nf \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \nA \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nB \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  求 \n\\frac{\\partial f}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n。\n解：\n\n\\begin{align}\n\\mathrm{d} f &amp;= (\\mathrm{d} (A^T)) X B  + A^T (\\mathrm{d} X) B + A^T X (\\mathrm{d} B) \\\\\n&amp; = A^T (\\mathrm{d} X) B \n\n\\\\\\\\\n\n\\mathrm{d} f &amp;= \\mathrm{tr} \\left( \\mathrm{d} f \\right) \\\\\n&amp;= \\mathrm{tr} \\left( A^T (\\mathrm{d} X) B \\right) \\\\\n&amp;= \\mathrm{tr} \\left( B A^T (\\mathrm{d} X) \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (A B^T)^T \\mathrm{d} X \\right)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n\n\n\n\n\n由此可得 \n\\frac{\\partial f}{\\partial X} = A B^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n\n\n。\n例2：   \nf = A^T \\exp (X B)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n \n\n \n \n \n \n\n,   \nf \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \nA \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nB \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\exp(X) = [\\exp(X_{ij})]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n \n \n\n \n\n \n\n \n \n\n\n \n \n\n, 求 \n\\frac{\\partial f}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n。\n解：\n\n\\begin{align}\n\\mathrm{d} f &amp;= A^T \\mathrm{d} \\left( \\exp(XB) \\right) \\\\\n&amp;= A^T \\left( \\exp(XB) \\odot \\mathrm{d} (X B) \\right) \\\\\n&amp;= A^T \\left( \\exp(XB) \\odot ((\\mathrm{d}X) B) \\right) \n\n\\\\\n\\\\\n\n\\mathrm{d} f &amp;= \\mathrm{tr} \\left( \\mathrm{d} f \\right) \\\\\n&amp;= \\mathrm{tr} \\left( A^T \\left( \\exp(XB) \\odot ((\\mathrm{d}X) B) \\right)  \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left(A \\odot \\exp(XB) \\right)^T (\\mathrm{d}X) B   \\right) \\\\\n&amp;= \\mathrm{tr} \\left( B \\left(A \\odot \\exp(XB) \\right)^T \\mathrm{d}X   \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( \\left(A \\odot \\exp(XB) \\right) B^T \\right)^T \\mathrm{d}X  \\right) \\\\\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n \n\n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n\n\n \n\n \n \n\n\n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n\n\n\n\n\n\n由此可得 \n\\frac{\\partial f}{\\partial X} = \\left(A \\odot \\exp(XB) \\right) B^T \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n\n。\n例3：  \nf = \\mathrm{tr} \\left( Y^T M Y \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n\n \n \n\n \n \n \n\n\n,  \nY = \\sigma(W X)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n,  \nf \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \nW \\in \\mathbb{R}^{l \\times m}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,    \nY \\in \\mathbb{R}^{l \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nM \\in \\mathbb{R}^{l \\times l}\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n 是对称矩阵,  \n\\sigma(X) = [\\sigma(X_{ij})]\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n \n\n, 求 \n\\frac{\\partial f}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n。\n解：\n\n\\begin{align}\n\n\\mathrm{d} f &amp;=  \\mathrm{d} \\left( \\mathrm{tr} \\left( Y^T M Y \\right) \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\mathrm{d}(Y^T M Y) \\right) \\\\ \n&amp;= \\mathrm{tr} \\left( \\mathrm{d}(Y^T) M Y + Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\mathrm{d}(Y^T) M Y \\right) + \\mathrm{tr} \\left( Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (\\mathrm{d}Y)^T M Y \\right) + \\mathrm{tr} \\left( Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (Y^T M^T \\mathrm{d}Y)^T \\right) + \\mathrm{tr} \\left( Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( Y^T M^T \\mathrm{d}Y \\right) + \\mathrm{tr} \\left( Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( Y^T M^T \\mathrm{d}Y + Y^T M \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( Y^T (M^T + M) \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( Y^T (2M^T) \\mathrm{d}Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (2MY)^T \\mathrm{d} Y \\right) \n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n\n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n \n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n \n \n\n\n \n \n\n \n \n\n \n \n\n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n\n\n\n可得 \n\\frac{\\partial f}{\\partial Y} = 2MY\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n \n \n\n.\n\n\\begin{align}\n\\mathrm{d} Y &amp;= \\mathrm{d} ( \\sigma(W X) ) \\\\\n&amp;= \\sigma'(WX) \\odot \\mathrm{d} (WX) \\\\\n&amp;= \\sigma'(WX) \\odot (W \\mathrm{d}X)\n\n\\\\\n\\\\\n\n\\mathrm{d} f &amp;= \\mathrm{tr} \\left( (\\frac{\\partial f}{\\partial Y})^T \\mathrm{d} Y \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (\\frac{\\partial f}{\\partial Y})^T (\\sigma'(WX) \\odot (W \\mathrm{d}X)) \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (\\frac{\\partial f}{\\partial Y} \\odot \\sigma'(WX) )^T  W \\mathrm{d}X \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( W^T \\left(\\frac{\\partial f}{\\partial Y} \\odot \\sigma'(WX) \\right) \\right)^T  \\mathrm{d}X \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( W^T \\left((2MY) \\odot \\sigma'(WX) \\right) \\right)^T  \\mathrm{d}X \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( W^T \\left((2M \\sigma(WX)) \\odot \\sigma'(WX) \\right) \\right)^T  \\mathrm{d}X \\right) \\\\\n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n\n\n \n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n\n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n\n\n\n由此可得 \n\\frac{\\partial f}{\\partial X} = W^T \\left((2M \\sigma(WX)) \\odot \\sigma'(WX) \\right) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n。\n例4 [线性回归]:     \nl = \\| X \\boldsymbol{w} - y \\|^2\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n\n,   \nl \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \n\\boldsymbol{y} \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\boldsymbol{w} \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  求 \nw\n\n\n\n\n \n\n 的最小二乘估计，即 \n\\frac{\\partial l}{\\partial \\boldsymbol{w}}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 的零点。\n解：\n\n\\begin{align}\n\nl &amp;= \\| X \\boldsymbol{w} - \\boldsymbol{y} \\|^2 \\\\\n&amp; = (X \\boldsymbol{w} - \\boldsymbol{y})^T(X \\boldsymbol{w} - \\boldsymbol{y})\n\n\\\\\n\\\\\n\n\\mathrm{d} l &amp;= \\mathrm{d} \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T(X \\boldsymbol{w} - \\boldsymbol{y}) \\right) \\\\\n&amp;= \\mathrm{d} \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T \\right) (X \\boldsymbol{w} - \\boldsymbol{y}) + (X \\boldsymbol{w} - \\boldsymbol{y})^T \\mathrm{d} (X \\boldsymbol{w} - \\boldsymbol{y}) \\\\\n&amp;= \\left(\\mathrm{d}(X \\boldsymbol{w} - \\boldsymbol{y}) \\right)^T (X \\boldsymbol{w} - \\boldsymbol{y}) + (X \\boldsymbol{w} - \\boldsymbol{y})^T d(X \\boldsymbol{w} - \\boldsymbol{y}) \\\\\n&amp;= \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T \\mathrm{d}(X \\boldsymbol{w} - \\boldsymbol{y})  \\right)^T + (X \\boldsymbol{w} - \\boldsymbol{y})^T \\mathrm{d}(X \\boldsymbol{w} - \\boldsymbol{y}) \\\\\n&amp;= \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d} \\boldsymbol{w} \\right)^T + (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d} \\boldsymbol{w}\n\n\\\\\n\\\\\n\n\\mathrm{d} l &amp;= \\mathrm{tr} \\left( \\mathrm{d} l \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right)^T + (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right)^T \\right) + \\mathrm{tr} \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right) + \\mathrm{tr} \\left( (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right) \\\\\n&amp;= \\mathrm{tr} \\left( 2 (X \\boldsymbol{w} - \\boldsymbol{y})^T X\\mathrm{d}\\boldsymbol{w} \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (2 X^T (X \\boldsymbol{w} - \\boldsymbol{y}))^T \\mathrm{d}\\boldsymbol{w} \\right) \\\\\n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n\n\n\n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n \n\n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n \n\n \n\n \n \n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n\n \n \n\n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n\n\n\n可得 \n\\frac{\\partial l}{\\partial \\boldsymbol{w}} = 2 X^T (X \\boldsymbol{w} - \\boldsymbol{y})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n \n\n。\n接下来求 \n\\frac{\\partial l}{\\partial \\boldsymbol{w}}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 的零点，即\n\n\\begin{align}\n\\frac{\\partial l}{\\partial \\boldsymbol{w}} &amp;= \\boldsymbol{0} \\\\\n2 X^T (X \\boldsymbol{w} - \\boldsymbol{y}) &amp;= \\boldsymbol{0} \\\\\nX^T (X \\boldsymbol{w} - \\boldsymbol{y}) &amp;= \\boldsymbol{0} \\\\\nX^TX \\boldsymbol{w} &amp;= X^T \\boldsymbol{y} \\\\\n\\boldsymbol{w} &amp;= (X^TX)^{-1} X^T \\boldsymbol{y}\n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n \n\n \n \n\n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n\n\n \n \n \n \n\n \n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n \n\n\n \n \n\n \n \n\n \n\n \n\n \n \n\n\n\n \n \n\n \n\n\n\n\n\n例5 [logistic回归]:     \nl = -\\boldsymbol{y}^T \\log \\mathrm{softmax} (W \\boldsymbol{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n \n \n\n,   \nl \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \n\\boldsymbol{y} \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nW \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\boldsymbol{x} \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\mathrm{softmax}(x) = \\frac{\\exp (x)}{\\boldsymbol{1}^T \\exp (x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n,  求 \n\\frac{\\partial l}{\\partial W}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 。\n解：\n\n\\begin{align}\n\\log(\\boldsymbol{u} / c) &amp;= \\log(\\boldsymbol{u}) - \\boldsymbol{1} \\log(c) \\\\\ny^T \\boldsymbol{1} &amp;= 1 \n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n \n\n\n\n\n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n\n \n \n\n\n\n\n\n\n\\begin{align}\nl &amp;= -y^T \\log \\mathrm{softmax} (Wx) \\\\\n&amp;= -y^T (\\log ( \\exp(Wx) ) - \\boldsymbol{1} \\log (\\boldsymbol{1}^T \\exp(Wx))) \\\\\n&amp;= -y^T Wx +  \\log (\\boldsymbol{1}^T \\exp(Wx)) \n\n\\\\\n\\\\\n\n\\mathrm{d} l &amp;= -y^T \\mathrm{d}W x + \\frac{ \\mathrm{d} (\\boldsymbol{1}^T \\exp(Wx)) }{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ \\boldsymbol{1}^T \\mathrm{d} ( \\exp(Wx)) }{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ \\boldsymbol{1}^T ( \\exp(Wx) \\odot \\mathrm{d} (Wx) )}{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ \\boldsymbol{1}^T ( \\exp(Wx) \\odot (\\mathrm{d}Wx) )}{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ (\\boldsymbol{1} \\odot \\exp(Wx))^T \\mathrm{d}Wx }{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ (\\exp(Wx))^T \\mathrm{d}Wx }{\\boldsymbol{1}^T \\exp(Wx)} \\\\\n&amp;= -y^T \\mathrm{d}W x + \\frac{ (\\exp(Wx))^T  }{\\boldsymbol{1}^T \\exp(Wx)} \\mathrm{d}Wx \\\\\n&amp;= -y^T \\mathrm{d}W x + \\mathrm{softmax}(Wx)^T \\mathrm{d}Wx \\\\\n&amp;= ( \\mathrm{softmax}(Wx)^T -y^T ) \\mathrm{d}Wx \\\\\n&amp;= ( \\mathrm{softmax}(Wx) -y )^T \\mathrm{d}Wx \n\n\\\\\n\\\\\n\n\\mathrm{d} l &amp;= \\mathrm{tr} \\left( \\mathrm{d} l \\right) \\\\\n&amp;= \\mathrm{tr} \\left( ( \\mathrm{softmax}(Wx) -y )^T \\mathrm{d}Wx  \\right) \\\\\n&amp;= \\mathrm{tr} \\left( x ( \\mathrm{softmax}(Wx) -y )^T \\mathrm{d}W  \\right) \\\\\n&amp;= \\mathrm{tr} \\left( (( \\mathrm{softmax}(Wx) -y )x^T)^T \\mathrm{d}W  \\right) \\\\\n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n \n \n\n\n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n\n\n\n\n \n\n \n \n \n\n \n \n \n \n\n \n \n\n\n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n \n \n \n\n\n \n \n\n \n \n\n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n \n\n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n\n \n \n\n \n \n \n\n\n\n\n\n\n由此可得， \n\\frac{\\partial l}{\\partial W} = ( \\mathrm{softmax}(Wx) -y )x^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n\n 。\n例6 [二层神经网络]：    \nl = -y^T \\log \\mathrm{softmax} (W_2 \\sigma(W_1 x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n,   \nl \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \n\\boldsymbol{y} \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nW_1 \\in \\mathbb{R}^{p \\times n}\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n\n,  \nW_2 \\in \\mathbb{R}^{m \\times p}\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n\n\n\n,   \n\\boldsymbol{x} \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\mathrm{softmax}(x) = \\frac{\\exp (x)}{\\boldsymbol{1}^T \\exp (x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n,   \n\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n\n \n \n\n \n \n \n\n \n \n \n \n\n\n\n\n,  求 \n\\frac{\\partial l}{\\partial W_1}\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n,   \n\\frac{\\partial l}{\\partial W_2}\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n  。\n解：\n令 \na = W_2 \\sigma(W_1 x)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n ,\n\n\\begin{align}\nl &amp;= -y^T \\log \\mathrm{softmax}(a) \\\\\n&amp;= -y^T \\left( \\log \\exp (a) - \\boldsymbol{1} \\log (\\boldsymbol{1}^T \\exp (a)) \\right) \\\\\n&amp;= -y^Ta + \\log (\\boldsymbol{1}^T \\exp (a))\n\n\\\\\\\\\n\n\\mathrm{d} l &amp;= \\mathrm{d} \\left( -y^Ta + \\log (\\boldsymbol{1}^T \\exp (a)) \\right) \\\\\n&amp;= -y^T \\mathrm{d}a + \\frac{\\boldsymbol{1}^T (\\exp(a) \\odot \\mathrm{d} a)}{\\boldsymbol{1}^T \\exp(a)} \\\\\n&amp;= -y^T \\mathrm{d}a + \\frac{ ( \\boldsymbol{1} \\odot \\exp(a) )^T  \\mathrm{d} a}{\\boldsymbol{1}^T \\exp(a)} \\\\\n&amp;= -y^T \\mathrm{d}a + \\frac{ \\exp(a)^T  \\mathrm{d} a}{\\boldsymbol{1}^T \\exp(a)} \\\\\n&amp;= -y^T \\mathrm{d}a + \\mathrm{softmax}(a)^T \\mathrm{d} a \\\\\n&amp;= \\left( \\mathrm{softmax}(a) -y \\right)^T \\mathrm{d} a \n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n\n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n \n\n\n \n \n\n \n \n\n\n \n\n \n \n \n\n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n\n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n\n\n\n\n\n\\begin{align}\n\n\\mathrm{d} a &amp;= \\mathrm{d}( W_2 \\sigma(W_1 x) ) \\\\\n&amp;= \\mathrm{d} W_2 \\sigma(W_1 x) + W_2(\\sigma'(W_1) x \\odot (\\mathrm{d} W_1 x) ) \\\\\n&amp;= \\mathrm{d} W_2 \\sigma(W_1 x) + (W_2 \\odot \\sigma'(W_1)) \\mathrm{d} W_1 x \n\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n\n\n\n\n\n\\begin{align}\n\\mathrm{d} l &amp;= \\mathrm{tr} \\left( \\mathrm{d} l \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( \\mathrm{softmax}(a) -y \\right)^T \\mathrm{d} a \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( \\mathrm{softmax}(a) -y \\right)^T \\left( \\mathrm{d} W_2 \\sigma(W_1 x) + (W_2 \\odot \\sigma'(W_1)) \\mathrm{d} W_1 x  \\right) \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( \\mathrm{softmax}(a) -y \\right)^T  \\mathrm{d} W_2 \\sigma(W_1 x)  \\right) + \\mathrm{tr} \\left( \\left( \\mathrm{softmax}(a) -y \\right)^T  (W_2 \\odot \\sigma'(W_1)) \\mathrm{d} W_1 x  \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\sigma(W_1 x)  \\left( \\mathrm{softmax}(a) -y \\right)^T  \\mathrm{d} W_2  \\right) + \\mathrm{tr} \\left( x \\left( \\mathrm{softmax}(a) -y \\right)^T  (W_2 \\odot \\sigma'(W_1)) \\mathrm{d} W_1  \\right) \\\\\n&amp;= \\mathrm{tr} \\left( \\left( ( \\mathrm{softmax}(a) -y ) \\sigma(W_1 x)^T  \\right)^T  \\mathrm{d} W_2  \\right) + \\mathrm{tr} \\left( \\left( (W_2 \\odot \\sigma'(W_1))^T ( \\mathrm{softmax}(a) -y ) x^T \\right)^T   \\mathrm{d} W_1  \\right) \n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n\n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n \n\n\n\n \n\n \n \n\n\n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n\n\n \n\n \n \n\n\n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n\n\n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n\n\n\n\n\n由此可得，\n\n\\begin{align}\n\\frac{\\partial l}{\\partial W_1} &amp;= (W_2 \\odot \\sigma'(W_1))^T ( \\mathrm{softmax}(a) -y ) x^T \\\\ \n&amp;= (W_2 \\odot \\sigma'(W_1))^T ( \\mathrm{softmax}(W_2 \\sigma(W_1 x)) -y ) x^T \n\\\\\\\\\n\\frac{\\partial l}{\\partial W_2} &amp;= ( \\mathrm{softmax}(a) -y ) \\sigma(W_1 x)^T  \\\\\n&amp;= ( \\mathrm{softmax}(W_2 \\sigma(W_1 x)) -y ) \\sigma(W_1 x)^T\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n\n\n\n\n \n \n\n\n \n\n \n \n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n\n\n\n\n\n矩阵对矩阵求导定义矩阵对矩阵的导数一个定义，先从向量对向量的导数定义来看\n向量对向量的导数定义：\n\nf \\in \\mathbb{R}^{p \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \nx \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \n\\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{m \\times p}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n\n\n\n \n\n\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_p}{\\partial x_1} \\\\\n\\frac{\\partial f_1}{\\partial x_2} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_p}{\\partial x_2} \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n\\frac{\\partial f_1}{\\partial x_m} &amp; \\frac{\\partial f_2}{\\partial x_m} &amp; \\cdots &amp; \\frac{\\partial f_p}{\\partial x_m} \n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n\n \n\n \n\n \n\n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n \n \n \n \n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\\partial f = \\left( \\frac{\\partial f}{\\partial x} \\right)^T \\mathrm{d} x\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n\n\n矩阵对矩阵的导数的定义：\n矩阵对矩阵的定义需要先将矩阵向量化:\n\nF \\in \\mathbb{R}^{p \\times q}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nX \\in \\mathbb{R}^{m\\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \n\\frac{\\partial F}{\\partial X} \\in \\mathbb{R}^{mn \\times pq}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n\n\\mathrm{vec}(X) = [X_{11}, \\cdots, X_{m1}, X_{12}, \\cdots, X_{m2}, \\cdots, X_{1n}, \\cdots, X_{mn}]^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n\n \n \n\n\n\n\n\\frac{\\partial F}{\\partial X} = \\frac{\\partial \\mathrm{vec} (F)}{\\partial \\mathrm{vec} (X)} = \\frac{\\partial F}{\\partial \\mathrm{vec} (X)} =\\frac{\\partial \\mathrm{vec} (F)}{\\partial X}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n\n\n\n \n\n \n \n \n\n \n \n \n\n\n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n\n\n \n \n\n\n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n\n\n \n\n \n \n \n\n \n \n \n\n\n \n \n\n\n\n\n\n\n\\mathrm{vec} (\\mathrm{d} F) = \\left( \\frac{\\partial F}{\\partial X} \\right)^T \\mathrm{vec}(\\mathrm{d} X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n注： 为了兼容上文中对 \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n 矩阵的导数定义，避免混淆，用记号 \n\\nabla_X f\n\n\n\n\n\n\n \n \n \n\n 上文中的 \n\\frac{\\partial f}{\\partial X} \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n\n \n \n \n\n\n\n ，则新的定义 \n\\frac{\\partial f}{\\partial X} = \\mathrm{vec} (\\nabla_X f) \\in \\mathbb{R}^{mn \\times 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n\n \n\n \n \n \n \n\n\n\n 。\n向量化矩阵的运算法则\n线性：   \n\\mathrm{vec} (A+B) = \\mathrm{vec} (A) + \\mathrm{vec} (B)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n\n \n \n \n\n \n \n \n\n\n\n矩阵乘法：  \n\\mathrm{vec} (AXB) = (B^T \\otimes A) \\mathrm{vec} (X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n转置：    \n\\mathrm{vec} (A^T) = K_{mn} \\mathrm{vec} (A)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n \n\n \n\n \n \n\n\n\n \n \n \n\n \n \n \n\n，  其中 \nK_{mn} \\in \\mathbb{R}^{mn \\times mn}\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n\n \n \n \n \n \n\n\n\n  是交换矩阵，将按列有限的向量化变为按行有限的向量化。\n\n逐元素乘法：  \n\\mathrm{vec} (A \\odot X) = \\mathrm{diag} ( \\mathrm{vec} (A)) \\mathrm{vec} (X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n\n \n\n \n \n \n\n \n \n \n \n\n \n \n \n\n \n \n \n\n,   \n\\mathrm{diag} ( \\mathrm{vec} (A)) \\in \\mathbb{R}^{mn \\times mn}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n\n \n\n\nKronecker乘积和交换矩阵相关的恒等式\n\nA \\otimes ( B + C) = A \\otimes B + A \\otimes C\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n(A + B) \\otimes C = A \\otimes C + B \\otimes C\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n(kA) \\otimes B = A \\otimes (kB) = k (A \\otimes B)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n(A \\otimes B)^T = A^T \\otimes B^T\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n\n\n(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n\n\n\n\\mathrm{vec}(ab^T) = b \\otimes a\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n\n\n\n(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\nK_{mn} = K_{nm}^T\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n\nK_{pm} (A \\otimes B) K_{nq} = B \\otimes A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n \n \n \n \n\n \n\n \n \n\n\n \n \n \n \n\n,  \nA \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nB \\in \\mathbb{R}^{p \\times q}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n\n\n求解方法若矩阵函数 \nF\n\n\n\n\n \n\n 是矩阵 \nX\n\n\n\n\n \n\n 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 \nF\n\n\n\n\n \n\n 求微分，再做向量化并使用技巧将其他项交换至 \n\\mathrm{vec}(\\mathrm{d} X)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n 左侧，对照导数与微分的联系 \n\\mathrm{vec} (\\mathrm{d} F) = \\left( \\frac{\\partial F}{\\partial X} \\right)^T \\mathrm{vec}(\\mathrm{d} X)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n ,  即可得到导数。\n特别地，若矩阵退化为向量，对照导数与微分的联系 \n\\mathrm{d}f = \\left( \\frac{\\partial f}{\\partial x} \\right)^T \\mathrm{d} x \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n \n \n\n,  即可得到导数。\n复合：   假设已求得 \n\\frac{\\partial F}{\\partial Y}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n， 而 \nY\n\n\n\n\n \n\n 是 \nX\n\n\n\n\n \n\n 的函数， 求 \n\\frac{\\partial F}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 。\n解：\n根据链式法则，\n\n\\begin{align}\n\\mathrm{vec} (\\mathrm{d} F) &amp;= \\left( \\frac{\\partial F}{\\partial Y} \\right)^T \\mathrm{vec}(\\mathrm{d} Y) \\\\\n&amp;= \\left( \\frac{\\partial F}{\\partial Y} \\right)^T \\left( \\frac{\\partial Y}{\\partial X} \\right)^T \\mathrm{vec}(\\mathrm{d} X) \\\\\n&amp;= \\left( \\frac{\\partial Y}{\\partial X} \\frac{\\partial F}{\\partial Y}  \\right)^T \\mathrm{vec}(\\mathrm{d} X) \\\\\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n\n\n\n由此可得 \n\\frac{\\partial F}{\\partial X} = \\frac{\\partial Y}{\\partial X} \\frac{\\partial F}{\\partial Y}  \n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n\n\n\n \n \n\n\n \n \n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n\n 。\n案例例1：   \nF = AX\n\n\n\n\n\n\n\n \n \n \n \n\n,   \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   求 \n\\frac{\\partial F}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 。\n解：\n\n\\mathrm{d} F = A \\mathrm{d} X\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n\n\\begin{align}\n\\mathrm{vec} (\\mathrm{d} F) &amp;= \\mathrm{vec} (A \\mathrm{d} X) \\\\\n&amp;= (I_n \\otimes A) \\mathrm{vec} (\\mathrm{d} X) \\\\\n&amp;= (I_n \\otimes A^T)^T \\mathrm{vec} (\\mathrm{d} X)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n\n \n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n\n\n\n由此可得 \n\\frac{\\partial F}{\\partial X} = I_n \\otimes A^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n\n \n \n\n \n\n \n \n\n\n 。\n例2：  \nF = A \\exp(XB)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n \n \n \n \n\n,   \nA \\in \\mathbb{R}^{l \\times m}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \nX \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   \nB \\in \\mathbb{R}^{n \\times p}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,   求 \n\\frac{\\partial F}{\\partial X}\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n\n 。 \n解：\n\n\\begin{align}\n\\mathrm{d} F &amp;= A \\mathrm{d} ( \\exp(XB) ) \\\\\n&amp;= A (\\exp(XB) \\odot (\\mathrm{d} X B))\n\n\\\\\\\\\n\n\\mathrm{vec}(\\mathrm{d} F) &amp;= \\mathrm{vec} \\left( A (\\exp(XB) \\odot (\\mathrm{d} X B)) \\right) \\\\\n&amp;= (I_p \\otimes A) \\mathrm{vec} \\left( \\exp(XB) \\odot (\\mathrm{d} X B) \\right) \\\\\n&amp;= (I_p \\otimes A) \\mathrm{diag} (\\mathrm{vec} (\\exp(XB)) ) \\mathrm{vec} \\left( \\mathrm{d} X B \\right) \\\\\n&amp;= (I_p \\otimes A) \\mathrm{diag} (\\mathrm{vec} (\\exp(XB)) ) (B^T \\otimes I_m) \\mathrm{vec} \\left( \\mathrm{d} X \\right) \\\\\n&amp;= \\left( (B^T \\otimes I_m)^T \\mathrm{diag} (\\mathrm{vec} (\\exp(XB)) ) (I_p \\otimes A)^T \\right)^T  \\mathrm{vec} \\left( \\mathrm{d} X \\right) \\\\\n&amp;= \\left( (B \\otimes I_m) \\mathrm{diag} (\\mathrm{vec} (\\exp(XB)) ) (I_p \\otimes A^T) \\right)^T  \\mathrm{vec} \\left( \\mathrm{d} X \\right)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n \n \n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n\n \n \n \n\n\n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n\n \n \n \n\n\n \n \n \n \n \n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n\n\n\n \n\n \n \n \n \n\n \n \n\n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n \n \n\n\n \n \n \n \n\n\n\n\n\n\n由此可得 \n\\frac{\\partial F}{\\partial X} = (B \\otimes I_m) \\mathrm{diag} (\\mathrm{vec} (\\exp(XB)) ) (I_p \\otimes A^T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n \n \n\n \n \n\n \n\n \n \n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n\n 。\n例3：[logistic 回归]    \nl = -\\boldsymbol{y}^T \\log \\mathrm{softmax} (W \\boldsymbol{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n \n \n\n,   \nl \\in \\mathbb{R}\n\n\n\n\n\n\n \n \n \n\n,  \n\\boldsymbol{y} \\in \\mathbb{R}^{m \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \nW \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\boldsymbol{x} \\in \\mathbb{R}^{n \\times 1}\n\n\n\n\n\n\n\n\n\n \n \n\n \n\n \n \n \n\n\n\n,  \n\\mathrm{softmax}(x) = \\frac{\\exp (x)}{\\boldsymbol{1}^T \\exp (x)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n\n,  求 \n\\nabla_W l\n\n\n\n\n\n\n \n \n \n\n  和 \n\\nabla_W^2 l\n\n\n\n\n\n\n\n \n \n \n \n\n 。\n解：\n上文标量对矩阵求导的案例中已求出 :\n\n\\nabla_W l = ( \\mathrm{softmax}(Wx) -y )x^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n\n\n\n接下来求 \n\\nabla_W^2 l\n\n\n\n\n\n\n\n \n \n \n \n\n : \n定义 \na = Wx\n\n\n\n\n\n\n\n \n \n \n \n\n  ,\n\n\\begin{align}\n\\mathrm{d} \\nabla_Wl  &amp;= \\mathrm{d}(\\mathrm{softmax}(a))  x^T \\\\\n&amp;= \\mathrm{d} \\left( \\frac{\\exp (a)}{\\boldsymbol{1}^T \\exp (a)} \\right) x^T \\\\\n&amp;= \\left( \\frac{\\exp (a) \\odot \\mathrm{d} a}{\\boldsymbol{1}^T \\exp (a)} - \\frac{\\exp (a) (\\boldsymbol{1}^T (\\exp (a) \\odot \\mathrm{d} a))}{(\\boldsymbol{1}^T \\exp (a))^2} \\right) x^T \\\\\n&amp;= \\left( \\frac{ \\mathrm{diag}( \\exp (a) ) \\mathrm{d} a}{\\boldsymbol{1}^T \\exp (a)} - \\frac{\\exp (a) (\\exp (a)^T \\mathrm{d} a)}{(\\boldsymbol{1}^T \\exp (a))^2} \\right) x^T \\\\\n&amp;= \\left( \\frac{ \\mathrm{diag}( \\exp (a) ) }{\\boldsymbol{1}^T \\exp (a)} - \\frac{\\exp (a) \\exp (a)^T }{(\\boldsymbol{1}^T \\exp (a))^2} \\right) (\\mathrm{d} a) x^T \\\\\n&amp;= \\left( \\mathrm{diag}\\left( \\frac{ \\exp (a) }{\\boldsymbol{1}^T \\exp (a)} \\right)  - \\left( \\frac{ \\exp (a) }{\\boldsymbol{1}^T \\exp (a)} \\right) \\left( \\frac{ \\exp (a) }{\\boldsymbol{1}^T \\exp (a)} \\right)^T \\right) (\\mathrm{d} a) x^T \\\\\n&amp;= \\left( \\mathrm{diag}\\left( \\mathrm{softmax}(a) \\right)  - (\\mathrm{softmax}(a)) ( \\mathrm{softmax}(a))^T \\right) (\\mathrm{d} a) x^T \\\\\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n\n\n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n\n\n\n \n \n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n \n \n\n\n\n \n\n \n\n\n\n\n \n \n \n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n\n \n\n \n \n\n\n \n \n \n\n \n \n \n\n \n \n\n\n\n\n \n\n\n \n \n\n\n\n \n\n \n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n\n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n \n \n \n\n \n \n \n\n \n \n\n\n\n\n \n\n\n \n \n\n\n\n \n\n \n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n\n\n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n \n\n\n\n \n\n \n \n\n\n \n \n \n\n \n \n \n\n \n \n\n\n\n\n \n\n \n \n \n \n\n \n \n\n\n\n \n\n\n \n \n\n\n \n \n \n \n\n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n \n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n\n\n \n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n \n \n \n\n \n \n \n\n\n\n \n \n\n\n \n \n\n\n \n \n \n \n\n \n \n\n\n\n \n\n \n\n \n \n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n\n \n \n \n \n\n \n \n\n\n\n\n\n\n令 \nD(a) = \\mathrm{diag}\\left( \\mathrm{softmax}(a) \\right)  - (\\mathrm{softmax}(a)) ( \\mathrm{softmax}(a))^T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n\n ,\n\n\\begin{align}\n\\mathrm{d} \\nabla_W l  &amp;= D(a)(\\mathrm{d} a) x^T \\\\\n&amp;= D(Wx) \\mathrm{d} (Wx)x^T \\\\\n&amp;= D(Wx) \\mathrm{d} W x x^T \n\n\\\\\\\\\n\n\\mathrm{vec}(\\mathrm{d} \\nabla_W l) &amp;= \\mathrm{vec}\\left( D(Wx) \\mathrm{d} W x x^T  \\right) \\\\\n&amp;= \\left( (x x^T)^T \\otimes D(Wx) \\right) \\mathrm{vec}(\\mathrm{d} W) \\\\\n&amp;= \\left( (x x^T) \\otimes (D(Wx))^T \\right)^T \\mathrm{vec}(\\mathrm{d} W) \\\\\n&amp;= \\left( (x x^T) \\otimes D(Wx) \\right)^T \\mathrm{vec}(\\mathrm{d} W) \n\n\\\\\\\\\n\n\\nabla_W^2 l &amp;= (x x^T) \\otimes D(Wx) \\\\\n&amp;= (x x^T) \\otimes ( \\mathrm{diag}\\left( \\mathrm{softmax}(Wx) \\right)  - (\\mathrm{softmax}(Wx)) ( \\mathrm{softmax}(Wx))^T )\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n\n \n \n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n\n\n \n\n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n\n\n \n\n \n \n \n\n \n \n\n\n \n \n\n \n \n \n \n \n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n \n \n\n\n \n \n \n\n \n \n \n \n\n\n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n \n \n\n\n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n\n \n\n\n\n\n\nReference\n矩阵求导术 (上) | 长躯鬼侠\n矩阵求导术 (下) | 长躯鬼侠\n\n","plink":"https://yuxinzhao.net/matrix-calculus/"},{"title":"Docker基础","date":"2019-10-02T18:09:05.000Z","updated":"2022-01-04T08:35:48.567Z","content":"本文讲述 docker 的基本概念和使用。 \n\n\n\n\n容器Docker 利用容器来运行应用。\n容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。\n可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\n用容器执行指令1docker run ubuntu:15.10 /bin/echo \"hello world\"\n\n进行交互式的容器12docker run -i -t ubuntu:15.10 /bin/bashroot@dc005c79503:/#\n\n\n-t : 再新容器内指定一个伪终端或终端\n-i : 允许你对容器内的标准输入进行交互\n\n可以通过运行exit命令或者使用CTRL+D来退出容器\n使用CTRL+P+Q可以退出容器但不关闭容器\n后台启动容器1docker run -d &lt;image&gt; &lt;command&gt;\n\n1docker run -d ubuntu:15.10 /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n\n\n-d : 让容器后台运行\n\n输出不会打印 “hello world” 而是容器的ID\n容器命名1docker run -d -P --name container_name &lt;image&gt; &lt;command&gt;\n\n将容器的端口映射到本机的端口上12docker run -d -P &lt;image&gt; &lt;command&gt;docker run -d -p 8000:5000 &lt;image&gt; &lt;command&gt;\n\n\n-d : 让容器后台运行\n-P : 将容器内部使用的网络端口映射到我们使用的主机上(随机)\n-p : 将容器内部使用的端口映射到我们使用 的主机上(将容器内的5000端口映射到本地主机的8000端口上)\n\n1docker run -d -p 192.168.99.100:8000:5000/udp &lt;image&gt; &lt;command&gt; # 指定host和port\n\n查看所有的容器123docker ps     # 查看正在允许的容器docker ps -a  # 查看所有容器docker ps -l  # 查看最后一次创建的容器\n\n查看容器输出12docker logs &lt;name/id&gt;docker logs -f &lt;name/id&gt;\n\n\n-f : 让 docker logs 在输出后不是直接退出，而是一直接收该容器的输出\n\n查看容器内部运行的进程1docker top &lt;name/id&gt;\n\n查看容器的配置和状态信息1docker inspect &lt;name/id&gt;\n\n查看网络端口1docker port &lt;name/id&gt;\n\n停止容器1docker stop &lt;name/id&gt;\n\n重启容器将已经停止的容器再启动\n1docker start &lt;name/id&gt;\n\n将一个运行态的容器终止再重新启动启动\n1docker restart &lt;name/id&gt;\n\n移除容器1docker rm &lt;name/id&gt;\n\n:warning: 移除容器时，该容器必须是停止状态，否则会报错\n在运行的容器中执行命令1docker exec -d -i -t &lt;container name/id&gt; &lt;command&gt;\n\n\n-d : 在后台运行\n-t : 分配一个伪终端(pseudo-tty)并绑定到容器的stdin上\n-i : 保持容器的stdin打开\n\n进入容器1docker attach &lt;container&gt;\n\n123docker run -d -i -t --name my_python pythondocker attach my_python&gt;&gt;&gt; print(\"hello\")\n\n但是使用 attach 命令有时候并不方便。当多个窗口同时 attach 到同一个容器的时候，所有窗口都会同步显示。当某个窗口因命令阻塞时,其他窗口也无法执行操作了。\n导出容器12docker export -o &lt;output_file_name&gt; &lt;container name/id&gt;docker export &lt;container name/id&gt; &gt; &lt;output_file_name&gt;\n\n1docker export my_python &gt; my_python.tar\n\n导入容器为镜像1docker import &lt;input_file&gt; &lt;new container name&gt;\n\n12docker import my_python.tar new-python:devdocker mport http://example.com/exampleimage.tgz example/imagerepo\n\n用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。\n镜像Docker 镜像就是一个只读的模板。\nDocker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。\n列出镜像列表1docker images\n\n查找镜像可以再 Docker Hub 网站上查找也可以用命令行查找\n1docker search &lt;image&gt;\n\n拖取镜像1docker pull &lt;image&gt;\n\n修改/更新镜像12docker run -t -i ubuntu:14.01 /bin/bashroot@&lt;container id&gt;:/#\n\n即可使用容器内的shell镜像操作，容器的内容会因此发生变化，需要提交容器的副本\n1docker commit -m=\"has update\" -a=\"my name\" &lt;container id&gt; myname/ubuntu:v2\n\n\n-m : 提交的描述信息\n-a : 指定镜像作者\n&lt;container id&gt;: 容器ID\nmyname/ubuntu:v2 : 指定要创建的目标镜像名, 其中 v2 为tag\n\n使用这个新的镜像启动容器\n1docker run -t -i myname/ubuntu:v2 /bin/bash\n\nDockerfile 构建镜像从零开始创建一个新的 Docker 镜像，我们需要创建一个 Dockerfile 文件，其中包含一组指令来告诉 Docker 如何构建我们的镜像。\n1234567891011FROM    centos:6.7MAINTAINER  my name \"my@mail.com\"RUN     /bin/echo 'root:123456' |chpasswdRUN     useradd mynameRUN     /bin/echo 'myname:123456' |chpasswdRUN     /bin/echo -e \"LANG=\\\"en_US.UTF-8\\\"\" &gt;/etc/default/localADD\t\tmyAPP  /var/wwwEXPOSE  22EXPOSE  80CMD     /usr/sbin/sshd -D\n\n每一个指令都会在镜像上创建一个新的层，:warning: 一个镜像不能超过127层，每一个指令的前缀都必须是大写的。\n\nFROM : 指定使用哪个镜像作为基础\nRUN : 指令会在创建中运行，可以用来安装些环境\nADD : 复制本地文件到镜像（此处的myAPP是本地的一个文件夹）\nEXPOSE : 向外部开放端口\nCMD : 描述容器启动后运行的指令\n\n构建镜像\n1docker build -t myname/centos:6.7 .\n\n\n-t : 指定要创建的目标镜像名和tag\n. : Dockerfile文件所在目录，这里是当前目录\n\n设置镜像标签1docker tag &lt;image id&gt; &lt;image name: tag&gt;\n\n12docker tag 860c279d2fec myname/centos:devdocker images\n\n123REPOSITORY       TAG    IMAGE ID        CREATED        SIZErunoob/centos    6.7    860c279d2fec    5 hours ago    190.6 MBrunoob/centos    dev    860c279d2fec    5 hours ago    190.6 MB\n\n导出镜像到本地1docker save -o &lt;output_file_name&gt; &lt;image&gt;\n\n1docker save -o python_from_docker.tar python:latest\n\n从本地载入镜像123docker load --input &lt;input_file_name&gt;# 或docker load &lt; &lt;input_file_name&gt;\n\n12docker load --input python_from_docker.tardocker load &lt; python_from_docker.tar\n\n移除镜像1docker rmi &lt;image&gt;\n\n1docker rmi python:lastest\n\n仓库仓库（Repository）是集中存放镜像的地方。\n一个容易混淆的概念是注册服务器（Registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址，ubuntu 是仓库名。\nDocker Hub目前 Docker 官方维护了一个公共仓库 Docker Hub，其中已经包括了超过 15,000 的镜像。大部分需求，都可以通过在 Docker Hub 中直接下载镜像来实现。\n登录123docker loginUsername: your_docker_hub_user_namePassword: your_docker_hub_password\n\n注册成功后，本地用户目录的 .dockercfg 中将保存用户的认证信息。\n搜索镜像1docker search &lt;image&gt;\n\n拉取镜像1docker pull &lt;image&gt;\n\n推送镜像到Docker Hub1docker push &lt;image&gt;\n\n自动创建自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。 有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。。\n而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub 或 BitBucket）上的项目，一旦项目发生新的提交，则自动执行创建。\n要配置自动创建，包括如下的步骤：\n\n创建并登录 Docker Hub，以及目标网站；\n在目标网站中连接帐户到 Docker Hub；\n在 Docker Hub 中 配置一个自动创建；\n选取一个目标网站中的项目（需要含 Dockerfile）和分支；\n指定 Dockerfile 的位置，并提交创建。\n\n之后，可以 在Docker Hub 的自动创建页面中跟踪每次创建的状态。\n私有仓库docker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。\n容器安装docker-registry在安装了 Docker 后，可以通过获取官方 registry 镜像来运行。\n1docker run -d -p 5000:5000 registry\n\n这将使用官方的 registry 镜像来启动本地的私有仓库。 用户可以通过指定参数来配置私有仓库位置，例如配置镜像存储到 Amazon S3 服务。\n123456789docker run \\    -e SETTINGS_FLAVOR=s3 \\    -e AWS_BUCKET=acme-docker \\    -e STORAGE_PATH=/registry \\    -e AWS_KEY=AKIAHSHB43HS3J92MXZ \\    -e AWS_SECRET=xdDowwlK7TJajV1Y7EoOZrmuPEJlHYcNP2k4j49T \\    -e SEARCH_BACKEND=sqlalchemy \\    -p 5000:5000 \\    registry\n\n此外，还可以指定本地路径（如 /home/user/registry-conf ）下的配置文件。\n1docker run -d -p 5000:5000 -v /home/user/registry-conf:/registry-conf -e DOCKER_REGISTRY_CONFIG=/registry-conf/config.yml registry\n\n默认情况下，仓库会被创建在容器的 /tmp/registry 下。可以通过 -v 参数来将镜像文件存放在本地的指定路径。 例如下面的例子将上传的镜像放到 /opt/data/registry 目录。\n1docker run -d -p 5000:5000 -v /opt/data/registry:/tmp/registry registry\n\n本地安装docker-registry对于 Ubuntu 或 CentOS 等发行版，可以直接通过源安装。\n\nUbuntu\n\n12$ sudo apt-get install -y build-essential python-dev libevent-dev python-pip liblzma-dev$ sudo pip install docker-registry\n\n\nCentOS\n\n12$ sudo yum install -y python-devel libevent-devel python-pip gcc xz-devel$ sudo python-pip install docker-registry\n\n也可以从 docker-registry 项目下载源码进行安装。\n1234$ sudo apt-get install build-essential python-dev libevent-dev python-pip libssl-dev liblzma-dev libffi-dev$ git clone https://github.com/docker/docker-registry.git$ cd docker-registry$ sudo python setup.py install\n\n然后修改配置文件，主要修改 dev 模板段的 storage_path 到本地的存储仓库的路径。\n1$ cp config/config_sample.yml config/config.yml\n\n之后启动 Web 服务。\n1$ sudo gunicorn -c contrib/gunicorn.py docker_registry.wsgi:application\n\n或者\n1$ sudo gunicorn --access-logfile - --error-logfile - -k gevent -b 0.0.0.0:5000 -w 4 --max-requests 100 docker_registry.wsgi:application\n\n此时使用 curl 访问本地的 5000 端口，看到输出 docker-registry 的版本信息说明运行成功。\n\n注：config/config_sample.yml 文件是示例配置文件。\n\n在私有仓库上传、下载、搜索镜像创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库，别的机器上就可以下载下来了。例如私有仓库地址为 192.168.7.26:5000。\n先在本机查看已有的镜像。\n1234docker imagesREPOSITORY      TAG        IMAGE ID         CREATED            VIRTUAL SIZEubuntu          latest     ba5877dc9bec     6 weeks ago        192.7 MBubuntu          14.04      ba5877dc9bec     6 weeks ago        192.7 MB\n\n使用docker tag 将 ba58 这个镜像标记为 192.168.7.26:5000/test（格式为 docker tag IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]）。\n123456docker tag ba58 192.168.7.26:5000/testroot ~ # docker imagesREPOSITORY               TAG        IMAGE ID        CREATED        VIRTUAL SIZEubuntu                   14.04      ba5877dc9bec    6 weeks ago    192.7 MBubuntu                   latest     ba5877dc9bec    6 weeks ago    192.7 MB192.168.7.26:5000/test   latest     ba5877dc9bec    6 weeks ago    192.7 MB\n\n使用 docker push 上传标记的镜像。\n1234567891011docker push 192.168.7.26:5000/testThe push refers to a repository [192.168.7.26:5000/test] (len: 1)Sending image listPushing repository 192.168.7.26:5000/test (1 tags)Image 511136ea3c5a already pushed, skippingImage 9bad880da3d2 already pushed, skippingImage 25f11f5fb0cb already pushed, skippingImage ebc34468f71d already pushed, skippingImage 2318d26665ef already pushed, skippingImage ba5877dc9bec already pushed, skippingPushing tag for rev [ba5877dc9bec] on &#123;http://192.168.7.26:5000/v1/repositories/test/tags/latest&#125;\n\n用 curl 查看仓库中的镜像。\n12curl http://192.168.7.26:5000/v1/search&#123;&quot;num_results&quot;: 7, &quot;query&quot;: &quot;&quot;, &quot;results&quot;: [&#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/miaxis_j2ee&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/tomcat&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/ubuntu&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/ubuntu_office&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/desktop_ubu&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;dockerfile/ubuntu&quot;&#125;, &#123;&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/test&quot;&#125;]&#125;\n\n这里可以看到 {&quot;description&quot;: &quot;&quot;, &quot;name&quot;: &quot;library/test&quot;}，表明镜像已经被成功上传了。\n现在可以到另外一台机器去下载这个镜像。\n1234567891011docker pull 192.168.7.26:5000/testPulling repository 192.168.7.26:5000/testba5877dc9bec: Download complete511136ea3c5a: Download complete9bad880da3d2: Download complete25f11f5fb0cb: Download completeebc34468f71d: Download complete2318d26665ef: Download complete$ sudo docker imagesREPOSITORY                         TAG                 IMAGE ID            CREATED             VIRTUAL SIZE192.168.7.26:5000/test             latest              ba5877dc9bec        6 weeks ago         192.7 MB\n\n可以使用 这个脚本 批量上传本地的镜像到注册服务器中，默认是本地注册服务器 127.0.0.1:5000。例如：\n123456789101112131415161718192021222324252627$ wget https://github.com/yeasy/docker_practice/raw/master/_local/push_images.sh; sudo chmod a+x push_images.sh$ ./push_images.sh ubuntu:latest centos:centos7The registry server is 127.0.0.1Uploading ubuntu:latest...The push refers to a repository [127.0.0.1:5000/ubuntu] (len: 1)Sending image listPushing repository 127.0.0.1:5000/ubuntu (1 tags)Image 511136ea3c5a already pushed, skippingImage bfb8b5a2ad34 already pushed, skippingImage c1f3bdbd8355 already pushed, skippingImage 897578f527ae already pushed, skippingImage 9387bcc9826e already pushed, skippingImage 809ed259f845 already pushed, skippingImage 96864a7d2df3 already pushed, skippingPushing tag for rev [96864a7d2df3] on &#123;http://127.0.0.1:5000/v1/repositories/ubuntu/tags/latest&#125;Untagged: 127.0.0.1:5000/ubuntu:latestDoneUploading centos:centos7...The push refers to a repository [127.0.0.1:5000/centos] (len: 1)Sending image listPushing repository 127.0.0.1:5000/centos (1 tags)Image 511136ea3c5a already pushed, skipping34e94e67e63a: Image successfully pushed70214e5d0a90: Image successfully pushedPushing tag for rev [70214e5d0a90] on &#123;http://127.0.0.1:5000/v1/repositories/centos/tags/centos7&#125;Untagged: 127.0.0.1:5000/centos:centos7Done\n\n仓库配置文件Docker 的 Registry 利用配置文件提供了一些仓库的模板（flavor），用户可以直接使用它们来进行开发或生产部署。\n模板在 config_sample.yml 文件中，可以看到一些现成的模板段：\n\ncommon：基础配置\nlocal：存储数据到本地文件系统\ns3：存储数据到 AWS S3 中\ndev：使用 local 模板的基本配置\ntest：单元测试使用\nprod：生产环境配置（基本上跟s3配置类似）\ngcs：存储数据到 Google 的云存储\nswift：存储数据到 OpenStack Swift 服务\nglance：存储数据到 OpenStack Glance 服务，本地文件系统为后备\nglance-swift：存储数据到 OpenStack Glance 服务，Swift 为后备\nelliptics：存储数据到 Elliptics key/value 存储\n\n用户也可以添加自定义的模版段。\n默认情况下使用的模板是 dev，要使用某个模板作为默认值，可以添加 SETTINGS_FLAVOR 到环境变量中，例如\n1export SETTINGS_FLAVOR=dev\n\n另外，配置文件中支持从环境变量中加载值，语法格式为 _env:VARIABLENAME[:DEFAULT]。\n示例配置1234567891011121314151617181920212223242526common:    loglevel: info    search_backend: \"_env:SEARCH_BACKEND:\"    sqlalchemy_index_database:        \"_env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-registry.db\"prod:    loglevel: warn    storage: s3    s3_access_key: _env:AWS_S3_ACCESS_KEY    s3_secret_key: _env:AWS_S3_SECRET_KEY    s3_bucket: _env:AWS_S3_BUCKET    boto_bucket: _env:AWS_S3_BUCKET    storage_path: /srv/docker    smtp_host: localhost    from_addr: docker@myself.com    to_addr: my@myself.comdev:    loglevel: debug    storage: local    storage_path: /home/myself/dockertest:    storage: local    storage_path: /tmp/tmpdockertmp\n\n数据卷 /数据管理数据卷是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性：\n\n数据卷可以在容器之间共享和重用\n对数据卷的修改会立马生效\n对数据卷的更新，不会影响镜像\n卷会一直存在，直到没有容器使用。数据卷的使用，类似于 Linux 下对目录或文件进行 mount\n\n创建数据卷在用 docker run 命令的时候，使用 -v 标记来创建一个数据卷并挂载到容器里。在一次 run 中多次使用可以挂载多个数据卷。\n下面创建一个 web 容器，并加载一个数据卷到容器的 /webapp 目录。\n1docker run -d -P --name web -v /webapp training/webapp python app.py\n\n\n-v : 创建数据卷或挂载文件夹/文件到容器中\n\n\n 注意：也可以在 Dockerfile 中使用 VOLUME 来添加一个或者多个新的卷到由该镜像创建的任意容器。\n\n挂在一个主机目录作为数据卷1docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n\n上面的命令加载主机的 /src/webapp 目录到 /opt/webapp 目录。 这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，如果目录不存在 Docker 会自动为你创建它。\n\n注意：Dockerfile 中不支持这种用法，这是因为 Dockerfile 是为了移植和分享用的。然而，不同操作系统的路径格式不一样，所以目前还不能支持。\n\nDocker 挂载数据卷的默认权限是读写，用户也可以通过 :ro 指定为只读。\n12$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp:rotraining/webapp python app.py\n\n挂载一个本地主机文件作为数据卷1docker run --rm -i -t -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n\n数据卷容器 / 容器间共享持续更新的数据创建一个命名的数据卷容器 dbdata。\n1docker run -d -v /dbdata --name dbdata &lt;container&gt; &lt;command&gt;\n\n1docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres\n\n在其他容器中使用 --volumes-from 来挂载 dbdata 容器中的数据卷。\n12docker run -d --volumes-from dbdata --name db1 training/postgresdocker run -d --volumes-from dbdata --name db2 training/postgres\n\n还可以使用多个 --volumes-from 参数来从多个容器挂载多个数据卷。 也可以从其他已经挂载了数据卷的容器来挂载数据卷。\n1docker run -d --name db3 --volumes-from db1 training/postgres\n\n:warning: 使用 --volumes-from 参数所挂载数据卷的容器自己并不需要保持在运行状态。\n:warning: 如果删除了挂载的容器（包括 dbdata、db1 和 db2），数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时使用 docker rm -v 命令来指定同时删除关联的容器。 这可以让用户在容器之间升级和移动数据卷。\n数据卷备份首先使用 --volumes-from 标记来创建一个加载 dbdata 容器卷的容器，并从本地主机挂载当前到容器的 /backup 目录。\n1docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n\n容器启动后，使用了 tar 命令来将 dbdata 卷备份为本地的 /backup/backup.tar。\n数据卷恢复如果要恢复数据到一个容器，首先创建一个带有数据卷的容器 dbdata2。\n1docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n然后创建另一个容器，挂载 dbdata2 的容器，并使用 untar 解压备份文件到挂载的容器卷中。\n12docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf/backup/backup.tar\n\n容器网络容器互联使用 --link 参数可以让容器之间安全的进行交互。\n创建一个数据库容器。\n1docker run -d --name db training/postgres\n\n然后创建一个 web 容器，并将它连接到 db 容器\n1docker run -d -P --name web --link db:db training/webapp python app.py\n\n此时，db 容器和 web 容器建立互联关系。\n--link 参数的格式为 --link name:alias，其中 name 是要链接的容器的名称，alias 是这个连接的别名。\n使用 docker ps 来查看容器的连接\n1234docker psIMAGE                     ...  PORTS                    NAMEStraining/postgres:latest  ...  5432/tcp                 db, web/dbtraining/webapp:latest    ...  0.0.0.0:49154-&gt;5000/tcp  web\n\n可以看到自定义命名的容器，db 和 web，db 容器的 names 列有 db 也有 web/db。这表示 web 容器链接到 db 容器，web 容器将被允许访问 db 容器的信息。\nDocker 在两个互联的容器之间创建了一个安全隧道，而且不用映射它们的端口到宿主主机上。在启动 db 容器的时候并没有使用 -p 和 -P 标记，从而避免了暴露数据库端口到外部网络上。\nDocker 通过 2 种方式为容器公开连接信息：\n\n环境变量\n更新 /etc/hosts 文件\n\n使用 env 命令来查看 web 容器的环境变量\n123456789docker run --rm --name web2 --link db:db training/webapp env. . .DB_NAME=/web2/dbDB_PORT=tcp://172.17.0.5:5432DB_PORT_5000_TCP=tcp://172.17.0.5:5432DB_PORT_5000_TCP_PROTO=tcpDB_PORT_5000_TCP_PORT=5432DB_PORT_5000_TCP_ADDR=172.17.0.5. . .\n\n其中 DB_ 开头的环境变量是供 web 容器连接 db 容器使用，前缀采用大写的连接别名。\n除了环境变量，Docker 还添加 host 信息到父容器的 /etc/hosts 的文件。下面是父容器 web 的 hosts 文件\n12345docker run -t -i --rm --link db:db training/webapp /bin/bashroot@aed84ee21bde:/opt/webapp# cat /etc/hosts172.17.0.7  aed84ee21bde. . .172.17.0.5  db\n\n这里有 2 个 hosts，第一个是 web 容器，web 容器用 id 作为他的主机名，第二个是 db 容器的 ip 和主机名。 可以在 web 容器中安装 ping 命令来测试跟db容器的连通。\n123456root@aed84ee21bde:/opt/webapp# apt-get install -yqq inetutils-pingroot@aed84ee21bde:/opt/webapp# ping dbPING db (172.17.0.5): 48 data bytes56 bytes from 172.17.0.5: icmp_seq=0 ttl=64 time=0.267 ms56 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.250 ms56 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.256 ms\n\n用 ping 来测试db容器，它会解析成 172.17.0.5。\nDockerfile基本结构Dockerfile 由一行行命令语句组成，并且支持以 # 开头的注释行。\n一般的，Dockerfile 分为四部分：\n\n基础镜像信息\n维护者信息\n镜像操作指令\n容器启动时执行指令\n\n12345678910111213141516171819202122232425262728293031323334353637383940# Nginx## VERSION               0.0.1FROM      ubuntuMAINTAINER Victor Vieux &lt;victor@docker.com&gt;RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server# Firefox over VNC## VERSION               0.3FROM ubuntu# Install vnc, xvfb in order to create a 'fake' display and firefoxRUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefoxRUN mkdir /.vnc# Setup a passwordRUN x11vnc -storepasswd 1234 ~/.vnc/passwd# Autostart firefox (might not be the best way, but it does the trick)RUN bash -c 'echo \"firefox\" &gt;&gt; /.bashrc'EXPOSE 5900CMD    [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"]# Multiple images example## VERSION               0.1FROM ubuntuRUN echo foo &gt; bar# Will output something like ===&gt; 907ad6c2736fFROM ubuntuRUN echo moo &gt; oink# Will output something like ===&gt; 695d7793cbe4# You᾿ll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with# /oink.\n\n指令FROM格式为 \n12FROM &lt;image&gt;FROM &lt;image&gt;:&lt;tag&gt;\n\n第一条指令必须为 FROM 指令。并且，如果在同一个Dockerfile中创建多个镜像时，可以使用多个 FROM 指令（每个镜像一次）。\nMAINTAINER格式为 MAINTAINER &lt;name&gt;，指定维护者信息。\nRUN格式有两种\n12RUN &lt;command&gt;RUN [\"executable\", \"param1\", \"param2\"]`\n\n:warning: 前者将在 shell 终端中运行命令，即 /bin/sh -c；后者则使用 exec 执行。指定使用其它终端可以通过第二种方式实现，例如 RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;]。\n每条 RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \\ 来换行。\nCMD指定启动容器时执行的命令，:warning: 每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。\n如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。\n支持三种格式\n123CMD [\"executable\",\"param1\",\"param2\"] # 使用 exec 执行，推荐方式CMD command param1 param2 # 在 /bin/sh 中执行，提供给需要交互的应用CMD [\"param1\",\"param2\"] # 提供给 ENTRYPOINT 的默认参数\n\nEXPOSE格式为 EXPOSE &lt;port&gt; [&lt;port&gt;...]。\n告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。\nENV格式为 ENV &lt;key&gt; &lt;value&gt;。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。\n例如\n1234ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-MATHJAX-SSR-1739PATH\n\nADD格式为 ADD &lt;src&gt; &lt;dest&gt;。\n该命令将复制指定的 &lt;src&gt; 到容器中的 &lt;dest&gt;。 其中 &lt;src&gt; 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）\nCOPY格式为 COPY &lt;src&gt; &lt;dest&gt;。\n复制本地主机的 &lt;src&gt;（为 Dockerfile 所在目录的相对路径）到容器中的 &lt;dest&gt;。\n当使用本地目录为源目录时，推荐使用 COPY。\nENTRYPOINT两种格式：\n12ENTRYPOINT [\"executable\", \"param1\", \"param2\"]ENTRYPOINT command param1 param2 #（shell中执行）\n\n配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。\n:warning: 每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。\nVOLUME格式为 VOLUME [&quot;/data&quot;]。\n创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。\nUSER格式为 USER daemon。\n指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。\n当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如：RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres。要临时获取管理员权限可以使用 gosu，而不推荐 sudo。\nWORKDIR格式为 WORKDIR /path/to/workdir。\n为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。\n可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如\n1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd\n\n则最终路径为 /a/b/c。\nONBUILD格式为 ONBUILD [INSTRUCTION]。\n配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。\n例如，Dockerfile 使用如下的内容创建了镜像 image-A。\n1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...]\n\n如果基于 image-A 创建新的镜像时，新的Dockerfile中使用 FROM image-A指定基础镜像时，会自动执行 ONBUILD 指令内容，等价于在后面添加了两条指令。\n12345FROM image-A#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src\n\n使用 ONBUILD 指令的镜像，推荐在标签中注明，例如 ruby:1.9-onbuild。\n使用Dockerfile创建镜像编写完成 Dockerfile 之后，可以通过 docker build 命令来创建镜像。\n基本的格式为 docker build [选项] 路径，该命令将读取指定路径下（包括子目录）的 Dockerfile，并将该路径下所有内容发送给 Docker 服务端，由服务端来创建镜像。因此一般建议放置 Dockerfile 的目录为空目录。也可以通过 .dockerignore 文件（每一行添加一条匹配模式）来让 Docker 忽略路径下的目录和文件。\n要指定镜像的标签信息，可以通过 -t 选项，例如\n1docker build -t myrepo/myapp /tmp/test1/\n\nWEB 应用案例运行一个WEB应用在容器内运行一个 Python Flask 应用\n12docker pull training/webappdocker run -d -P training/webapp python app.py\n\n\n-d : 让容器后台运行\n-P : 将容器内部使用的网络端口映射到我们使用的主机上(随机)\n\n1docker run -d -p 8000:5000 training/webapp python app.py\n\n\n-p : 将容器内部使用的端口映射到我们使用 的主机上(将容器内的5000端口映射到本地主机的8000端口上)\n\n查看 WEB 应用容器1docker ps\n\n123CONTAINER ID        IMAGE               COMMAND             PORTS184e850e9001        training/webapp     &quot;python app.py&quot;     0.0.0.0:8000-&gt;5000/tcpdad8d5714980        training/webapp     &quot;python app.py&quot;     0.0.0.0:32768-&gt;5000/tcp\n\n这里多了个端口信息\n123PORTS0.0.0.0:8000-&gt;5000/tcp0.0.0.0:32768-&gt;5000/tcp\n\nDocker 开放了 5000 端口(默认Python Flask端口) 映射到主机端口32768和8000上。\n这时可以通过 http://192.168.99.100:32768/ 和 http://192.168.99.100:8000/ 访问WEB应用。192.168.99.100是我的vm虚拟机的ip。\n快捷查看网络端口12docker port &lt;name/id&gt;5000/tcp -&gt; 0.0.0.0:8000\n\n查看 WEB 应用程序日志1docker logs -f &lt;name/id&gt;\n\n1234 * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)192.168.99.1 - - [30/Aug/2019 03:52:56] &quot;GET / HTTP/1.1&quot; 200 -192.168.99.1 - - [30/Aug/2019 03:52:56] &quot;GET /favicon.ico HTTP/1.1&quot; 404 -192.168.99.1 - - [30/Aug/2019 03:53:05] &quot;GET / HTTP/1.1&quot; 200 -\n\n\n-f : 让 docker logs 在输出后不是直接退出，而是一直接收该容器的输出\n\n查看 WEB 应用程序容器内运行的进程查看容器内部运行的进程\n1docker top &lt;name/id&gt;\n\n12UID       PID      PPID     C      STIME      TTY     TIME          CMDroot      3445     3423     0      03:49      ?       00:00:00      python app.py\n\n查看 Docker 容器的配置与状态信息1docker inspect &lt;name/id&gt;\n\n1234567891011121314151617181920212223[    &#123;        &quot;Id&quot;: &quot;184e850e9001ae4527da694f7337c6a95c49555524d4f535072ca7f8556e367e&quot;,        &quot;Created&quot;: &quot;2019-08-30T03:49:51.305996931Z&quot;,        &quot;Path&quot;: &quot;python&quot;,        &quot;Args&quot;: [            &quot;app.py&quot;        ],        &quot;State&quot;: &#123;            &quot;Status&quot;: &quot;running&quot;,            &quot;Running&quot;: true,            &quot;Paused&quot;: false,            &quot;Restarting&quot;: false,            &quot;OOMKilled&quot;: false,            &quot;Dead&quot;: false,            &quot;Pid&quot;: 3445,            &quot;ExitCode&quot;: 0,            &quot;Error&quot;: &quot;&quot;,            &quot;StartedAt&quot;: &quot;2019-08-30T03:49:51.601670982Z&quot;,            &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;        &#125;,        ...]\n\n移除 WEB 应用容器12docker stop &lt;name/id&gt;    # 移除容器前要先停止容器docker rm &lt;name/id&gt;\t\t # 移除容器\n\n查找宿主机进程所在的容器假设要查找的进程是40850\n123456789docker_query_pid=40850for docker_id in $(docker ps -q | xargs)do    if `expr \n(docker top \n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\ndocker_id | grep $docker_query_pid | wc -l) &gt; 0`    then        docker ps | grep $docker_id        docker top \ndocker_id | grep \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\ndocker_query_pid    fidone\n\n","plink":"https://yuxinzhao.net/docker-basic/"},{"title":"Golang基础","date":"2019-10-02T16:48:16.000Z","updated":"2022-01-04T08:35:48.595Z","content":"本文是我学习 golang 时的笔记。\n\n\n\n\nGo packagepackage &lt;pkgName&gt;（在我们的例子中是package main）这一行告诉我们当前文件属于哪个包，而包名main则告诉我们它是一个可独立运行的包，它在编译后会产生可执行文件。除了main包之外，其它的包最后都会生成*.a文件（也就是包文件）并放置在\nGOPATH/pkg/\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\nGOOS_\nGOARCH`` 中（以Mac为例就是``\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n中\n\n\n（\n\n\n以\n\n \n \n \n\n为\n\n\n例\n\n\n就\n\n\n是\n\n \n \n\nGOPATH/pkg/darwin_amd64）。\n\n每一个可独立运行的Go程序，必定包含一个package main，在这个main包中必定包含一个入口函数main，而这个函数既没有参数，也没有返回值。\n\n\n包名和包所在的文件夹名可以是不同的，此处的&lt;pkgName&gt;即为通过package &lt;pkgName&gt;声明的包名，而非文件夹名\n\n变量123456789var variablename typevar vname1, vname2, vname3 type var vname1, vname2, vname3 type = value1, value2, value3var vname1, vname2, vname3 = value1, value2, value3vname1, vname2, vname3 := value1, value2, value3 // 只能用在函数内部，局部变量var(   vname1 type   vname2 type) // 只能用在函数外，全局变量\n\n常量123456const constantName type = valueconst constantName = valueconst(\tname1 = value1    name2 = value2)\n\n内置基础类型数值类型int, float,uint, rune, int8, int16, int32, int64和byte, uint8, uint16, uint32, uint64，complex32, complex64。\n其中，rune是int32的别称，byte是uint8的别称。\n\n注意： 不同类型之间不能互相赋值或操作，否则编译错误\n如int8,  int32不能互相赋值或相加等\n另外，尽管int的长度是32 bit, 但int 与 int32并不可以互用。\n\n字符串Go中的字符串都是采用UTF-8字符集编码。字符串是用一对双引号（&quot;&quot;）或反引号（` ` ）括起来定义，它的类型是string。\n在Go中字符串是不可变的.\n12m := `hello\tworld`\n\n` 括起的字符串为Raw字符串，即字符串在代码中的形式就是打印时的形式，它没有字符转义，换行也将原样输出。\n错误类型Go内置有一个error类型，专门用来处理错误信息，Go的package里面还专门有一个包errors来处理错误：\n1234err := errors.New(\"emit macho dwarf: elf header corrupted\")if err != nil &#123;\tfmt.Print(err)&#125;\n\niota枚举Go里面有一个关键字iota，这个关键字用来声明enum的时候采用，它默认开始值是0，const中每增加一行加1：\n123456789101112131415161718192021222324252627282930package mainimport (\t&quot;fmt&quot;)const (\tx = iota // x == 0\ty = iota // y == 1\tz = iota // z == 2\tw        // 常量声明省略值时，默认和之前一个值的字面相同。这里隐式地说w = iota，因此w == 3。其实上面y和z可同样不用&quot;= iota&quot;)const v = iota // 每遇到一个const关键字，iota就会重置，此时v == 0const (\th, i, j = iota, iota, iota //h=0,i=0,j=0 iota在同一行值相同)const (\ta       = iota //a=0\tb       = &quot;B&quot;\tc       = iota             //c=2\td, e, f = iota, iota, iota //d=3,e=3,f=3\tg       = iota             //g = 4)func main() &#123;\tfmt.Println(a, b, c, d, e, f, g, h, i, j, x, y, z, w, v)&#125;\n\n\n除非被显式设置为其它值或iota，每个const分组的第一个常量被默认设置为它的0值，第二及后续的常量被默认设置为它前面那个常量的值，如果前面那个常量的值是iota，则它也被设置为iota。\n\n公有与私有\n大写字母开头的变量是可导出的，也就是其它包可以读取的，是公有变量；小写字母开头的就是不可导出的，是私有变量。\n大写字母开头的函数也是一样，相当于class中的带public关键词的公有函数；小写字母开头的就是有private关键词的私有函数。\n\n数组array12345var arr [10]int  // 声明了一个int类型的数组arr[0] = 42      // 数组下标是从0开始的arr[1] = 13      // 赋值操作fmt.Printf(\"The first element is %d\\n\", arr[0])  // 获取数据，返回42fmt.Printf(\"The last element is %d\\n\", arr[9]) //返回未赋值的最后一个元素，默认返回0\n\n由于长度也是数组类型的一部分，因此[3]int与[4]int是不同的类型，数组也就不能改变长度。数组之间的赋值是值的赋值，即当把一个数组作为参数传入函数的时候，传入的其实是该数组的副本，而不是它的指针。如果要使用指针，那么就需要用到后面介绍的slice类型了。\n12345a := [3]int&#123;1, 2, 3&#125; // 声明了一个长度为3的int数组b := [10]int&#123;1, 2, 3&#125; // 声明了一个长度为10的int数组，其中前三个元素初始化为1、2、3，其它默认为0c := [...]int&#123;4, 5, 6&#125; // 可以省略长度而采用`...`的方式，Go会自动根据元素个数来计算长度\n\n多维数组1234567// 声明了一个二维数组，该数组以两个数组作为元素，其中每个数组中又有4个int类型的元素doubleArray := [2][4]int&#123;[4]int&#123;1, 2, 3, 4&#125;, [4]int&#123;5, 6, 7, 8&#125;&#125;// 上面的声明可以简化，直接忽略内部的类型easyArray := [2][4]int&#123;&#123;1, 2, 3, 4&#125;, &#123;5, 6, 7, 8&#125;&#125;fmt.Print(doubleArray[1][3])\n\n切片sliceslice并不是真正意义上的动态数组，而是一个引用类型。slice总是指向一个底层array，slice的声明也可以像array一样，只是不需要长度。\n12var fslice []int // 和声明array一样，只是少了长度slice := []byte &#123;'a', 'b', 'c', 'd'&#125;\n\n12345678910111213// 声明一个含有10个元素元素类型为byte的数组var ar = [10]byte &#123;'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'&#125;// 声明两个含有byte的slicevar a, b []byte// a指向数组的第3个元素开始，并到第五个元素结束，a = ar[2:5]//现在a含有的元素: ar[2]、ar[3]和ar[4]// b是数组ar的另一个sliceb = ar[3:5]// b的元素是：ar[3]和ar[4]\n\n1234567891011121314var a = [5]int&#123;6, 7, 8, 9, 10&#125;var a_slice []inta_slice = a[1:3]fmt.Println(\"len(a)=\", len(a), \", len(a_slice)=\", len(a_slice)) // 长度fmt.Println(\"cap(a)=\", cap(a), \", cap(a_slice)=\", cap(a_slice)) // 容量，切片指向数组上切片的左端到数组的最右端b_slice := append(a_slice, 1, 2, 3) // b_slice = [7 8 1 2 3]  a = [6, 7, 8, 9, 10] 插入的值比超过了容量，不对源切片指向数组进行修改，而是新建一个新的fmt.Println(b_slice)fmt.Println(a)c_slice := append(a_slice, 1, 2)    // c_slice = [7 8 1 2]    a = [6, 7, 8, 1, 2]  插入的值比容量小，对指向数组进行修改fmt.Println(c_slice)fmt.Println(a)\n\n123var a = [5]int&#123;6, 7, 8, 9, 10&#125;var a_slice []inta_slice = a[1:3:4] // 多增加一个4，使得len=3-1=2，但cap=4-1=3\n\n映射map12345678910// 声明一个key是字符串，值为int的字典,这种方式的声明需要在使用之前使用make初始化var numbers map[string]int// 另一种map的声明方式numbers = make(map[string]int)numbers[\"one\"] = 1  //赋值numbers[\"ten\"] = 10 //赋值numbers[\"three\"] = 3fmt.Println(\"第三个数字是: \", numbers[\"three\"]) // 读取数据// 打印出来如:第三个数字是: 3\n\n\n内置的len函数同样适用于map，返回map拥有的key的数量\nmap和其他基本型别不同，它不是thread-safe，在多个go-routine存取时，必须使用mutex lock机制\n\n1234567891011// 初始化一个字典rating := map[string]float32&#123;\"C\":5, \"Go\":4.5, \"Python\":4.5, \"C++\":2 &#125;// map有两个返回值，第二个返回值，如果不存在key，那么ok为false，如果存在ok为truecsharpRating, ok := rating[\"C#\"]if ok &#123;\tfmt.Println(\"C# is in the map and its rating is \", csharpRating)&#125; else &#123;\tfmt.Println(\"We have no rating associated with C# in the map\")&#125;delete(rating, \"C\")  // 删除key为C的元素\n\nmake &amp; newmake用于内建类型（map、slice 和channel）的内存分配。new用于各种类型的内存分配。\n内建函数new本质上说跟其它语言中的同名函数功能一样：new(T)分配了零值填充的T类型的内存空间，并且返回其地址，即一个*T类型的值。用Go的术语说，它返回了一个指针，指向新分配的类型T的零值。有一点非常重要：\n\nnew返回指针。\n\n内建函数make(T, args)与new(T)有着不同的功能，make只能创建slice、map和channel，并且返回一个有初始值(非零)的T类型，而不是*T。本质来讲，导致这三个类型有所不同的原因是指向数据结构的引用在使用前必须被初始化。例如，一个slice，是一个包含指向数据（内部array）的指针、长度和容量的三项描述符；在这些项目被初始化之前，slice为nil。对于slice、map和channel来说，make初始化了内部的数据结构，填充适当的值。\n\nmake返回初始化后的（非零）值。\n\n结构：new(Point)\n切片slice: make([]byte, 2, 6) // len=2， cap=6\n零值关于“零值”，所指并非是空值，而是一种“变量未填充前”的默认值，通常为0。 此处罗列 部分类型 的 “零值”\n1234567891011int     0int8    0int32   0int64   0uint    0x0rune    0 //rune的实际类型是 int32byte    0x0 // byte的实际类型是 uint8float32 0 //长度为 4 bytefloat64 0 //长度为 8 bytebool    falsestring  \"\"\n\n流程管理if1234567891011if x &gt; 10 &#123;\tfmt.Println(\"x is greater than 10\")&#125; else if x &lt; 1 &#123;\tfmt.Println(\"x is less than 1\")&#125; else &#123;    fmt.Println(\"x is between 1 and 10\")&#125;if x := 1; x &lt; 2 &#123;    println(\"x &lt; 2\")&#125;\n\ngoto1234567func myFunc() &#123;\ti := 0Here:   //这行的第一个词，以冒号结束作为标签\tprintln(i)\ti++\tgoto Here   //跳转到Here去&#125;\n\n\n标签名是大小写敏感的。\n\nfor123for expression1; expression2; expression3 &#123;\t//...&#125;\n\n12345sum := 0;for index:=0; index &lt; 10 ; index++ &#123;    sum += index&#125;fmt.Println(\"sum is equal to \", sum)\n\n1234sum := 1for sum &lt; 1000 &#123;\tsum += sum&#125; // 相当于while循环\n\n支持break, continue\nfor配合range可以用于读取slice和map的数据：\n12345678910111213for k,v:=range map &#123;\tfmt.Println(\"map's key:\",k)\tfmt.Println(\"map's val:\",v)&#125;for index, value := range slice &#123;\tfmt.Println(index, \" - \", value)&#125;// 可以用`_`丢弃不需要的值for _, v := range map&#123;\tfmt.Println(\"map's val:\", v)&#125;\n\nswitch12345678910switch sExpr &#123;case expr1:\tsome instructionscase expr2:\tsome other instructionscase expr3:\tsome other instructionsdefault:\tother code&#125;\n\n123456789d, e := 2, 1switch d &#123;    case 1, 3, 5: // 多情况    println(\"one or three or five\")    case 1 + e: // 不必是常量    println(\"two\")    default:    println(\"unknown\")&#125;\n\n\n不需要加break，默认找到后执行完后跳出，不会执行后面的语句\n可以使用fallthrough强制执行后面的case代码\n\n1234567891011121314151617181920integer := 6switch integer &#123;case 4:\tfmt.Println(\"The integer was &lt;= 4\")\tfallthroughcase 5:\tfmt.Println(\"The integer was &lt;= 5\")\tfallthroughcase 6:\tfmt.Println(\"The integer was &lt;= 6\")\tfallthroughcase 7:\tfmt.Println(\"The integer was &lt;= 7\")\tfallthroughcase 8:\tfmt.Println(\"The integer was &lt;= 8\")\tfallthroughdefault:\tfmt.Println(\"default case\")&#125;\n\n123456789switch x.(type)&#123;    case type:       statement(s);          case type:       statement(s);     /* 你可以定义任意个数的case */    default: /* 可选 */       statement(s);&#125;\n\n函数12345func funcName(input1 type1, input2 type2) (output1 type1, output2 type2) &#123;\t//这里是处理逻辑代码\t//返回多个值\treturn value1, value2&#125;\n\n1234567891011func max(a, b int) int &#123;\tif a &gt; b &#123;\t\treturn a\t&#125;\treturn b&#125;//...x := 3y := 4max_xy = max(x, y)\n\n12345func SumAndProduct(A, B int) (add int, Multiplied int) &#123;\tadd = A+B\tMultiplied = A*B\treturn&#125;\n\n变参/不定参数Go函数支持变参。接受变参的函数是有着不定数量的参数的。为了做到这点，首先需要定义函数使其接受变参：\n1func myfunc(arg ...int) &#123;&#125;\n\narg ...int告诉Go这个函数接受不定数量的参数。注意，这些参数的类型全部是int。在函数体中，变量arg是一个int的slice：\n123for _, n := range arg &#123;\tfmt.Printf(\"And the number is: %d\\n\", n)&#125;\n\n传值与传指针12345678910111213141516func swap_val(a, b int) &#123;\ttemp := a\ta = b\tb = temp&#125;func swap_ptr(a, b *int) &#123;\ttemp := *a\t*a = *b\t*b = temp&#125;//...a, b := 1, 2swap_val(a, b)    // a=1, b=2swap_ptr(&amp;a, &amp;b)  // a=2, b=1\n\n\nGo语言中channel，slice，map这三种类型的实现机制类似指针，所以可以直接传递，而不用取地址后传递指针。（注：若函数需改变slice的长度，则仍需要取地址传递指针）\n\n延迟deferGo语言中有种不错的设计，即延迟（defer）语句，你可以在函数中添加多个defer语句。当函数执行到最后时，这些defer语句会按照逆序执行，最后该函数返回。特别是当你在进行一些打开资源的操作时，遇到错误需要提前返回，在返回前你需要关闭相应的资源，不然很容易造成资源泄露等问题。\n123456789101112func ReadWrite() bool &#123;\tfile.Open(\"file\")\tdefer file.Close()\\    //...\tif failureX &#123;\t\treturn false\t&#125;\tif failureY &#123;\t\treturn false\t&#125;\treturn true&#125;\n\n在defer后指定的函数会在函数退出前调用。如果有很多调用defer，那么defer是采用后进先出模式.\n函数作为值、类型在Go中函数也是一种变量，我们可以通过type来定义它，它的类型就是所有拥有相同的参数，相同的返回值的一种类型\n1type typeName func(input1 inputType1 , input2 inputType2 [, ...]) (result1 resultType1 [, ...])\n\n12345678910111213141516171819202122232425262728293031323334353637383940package mainimport \"fmt\"type testInt func(int) bool // 声明了一个函数类型func isOdd(integer int) bool &#123;\tif integer%2 == 0 &#123;\t\treturn false\t&#125;\treturn true&#125;func isEven(integer int) bool &#123;\tif integer%2 == 0 &#123;\t\treturn true\t&#125;\treturn false&#125;// 声明的函数类型在这个地方当做了一个参数func filter(slice []int, f testInt) []int &#123;\tvar result []int\tfor _, value := range slice &#123;\t\tif f(value) &#123;\t\t\tresult = append(result, value)\t\t&#125;\t&#125;\treturn result&#125;func main()&#123;\tslice := []int &#123;1, 2, 3, 4, 5, 7&#125;\tfmt.Println(\"slice = \", slice)\todd := filter(slice, isOdd)    // 函数当做值来传递了\tfmt.Println(\"Odd elements of slice are: \", odd)\teven := filter(slice, isEven)  // 函数当做值来传递了\tfmt.Println(\"Even elements of slice are: \", even)&#125;\n\nPanic 和 RecoverGo没有像Java那样的异常机制，它不能抛出异常，而是使用了panic和recover机制。一定要记住，你应当把它作为最后的手段来使用，也就是说，你的代码中应当没有，或者很少有panic的东西。这是个强大的工具，请明智地使用它。\nPanic\n\n是一个内建函数，可以中断原有的控制流程，进入一个panic状态中。当函数F调用panic，函数F的执行被中断，但是F中的延迟函数会正常执行，然后F返回到调用它的地方。在调用的地方，F的行为就像调用了panic。这一过程继续向上，直到发生panic的goroutine中所有调用的函数返回，此时程序退出。panic可以直接调用panic产生。也可以由运行时错误产生，例如访问越界的数组。\n\nRecover\n\n是一个内建的函数，可以让进入panic状态的goroutine恢复过来。recover仅在延迟函数中有效。在正常的执行过程中，调用recover会返回nil，并且没有其它任何效果。如果当前的goroutine陷入panic状态，调用recover可以捕获到panic的输入值，并且恢复正常的执行。\n下面这个函数演示了如何在过程中使用panic\n\n1234567var user = os.Getenv(\"USER\")func init() &#123;\tif user == \"\" &#123;\t\tpanic(\"no value for $USER\")\t&#125;&#125;\n\n下面这个函数检查作为其参数的函数在执行时是否会产生panic：\n123456789func throwsPanic(f func()) (b bool) &#123;\tdefer func() &#123;\t\tif x := recover(); x != nil &#123;\t\t\tb = true\t\t&#125;\t&#125;()\tf() //执行函数f，如果f中出现了panic，那么就可以恢复回来\treturn&#125;\n\nmain函数和init函数Go里面有两个保留的函数：init函数（能够应用于所有的package）和main函数（只能应用于package main）。这两个函数在定义时不能有任何的参数和返回值。虽然一个package里面可以写任意多个init函数，但这无论是对于可读性还是以后的可维护性来说，我们都强烈建议用户在一个package中每个文件只写一个init函数。\nGo程序会自动调用init()和main()，所以你不需要在任何地方调用这两个函数。每个package中的init函数都是可选的，但package main就必须包含一个main函数。\n程序的初始化和执行都起始于main包。如果main包还导入了其它的包，那么就会在编译时将它们依次导入。有时一个包会被多个包同时导入，那么它只会被导入一次（例如很多包可能都会用到fmt包，但它只会被导入一次，因为没有必要导入多次）。当一个包被导入时，如果该包还导入了其它的包，那么会先将其它包导入进来，然后再对这些包中的包级常量和变量进行初始化，接着执行init函数（如果有的话），依次类推。等所有被导入的包都加载完毕了，就会开始对main包中的包级常量和变量进行初始化，然后执行main包中的init函数（如果存在的话），最后执行main函数。下图详细地解释了整个执行过程：\n\nimport我们在写Go代码的时候经常用到import这个命令用来导入包文件，而我们经常看到的方式参考如下：\n123import(    &quot;fmt&quot;)\n\n然后我们代码里面可以通过如下的方式调用\n1fmt.Println(&quot;hello world&quot;)\n\n上面这个fmt是Go语言的标准库，其实是去GOROOT环境变量指定目录下去加载该模块，当然Go的import还支持如下两种方式来加载自己写的模块：\n\n相对路径\n\n1import “./model” //当前文件同一目录的model目录，但是不建议这种方式来import\n\n\n绝对路径\n\n1import “shorturl/model” //加载gopath/src/shorturl/model模块\n\n上面展示了一些import常用的几种方式，但是还有一些特殊的import，让很多新手很费解，下面我们来一一讲解一下到底是怎么一回事\n\n点操作\n我们有时候会看到如下的方式导入包\n\n\n123import(    . \"fmt\")\n\n   这个点操作的含义就是这个包导入之后在你调用这个包的函数时，你可以省略前缀的包名，也就是前面你调用的fmt.Println(“hello world”)可以省略的写成Println(“hello world”)\n\n别名操作\n别名操作顾名思义我们可以把包命名成另一个我们用起来容易记忆的名字\n\n\n123import(    f \"fmt\")\n\n   别名操作的话调用包函数时前缀变成了我们的前缀，即f.Println(“hello world”)\n\n_操作\n这个操作经常是让很多人费解的一个操作符，请看下面这个import\n\n\n1234import (    \"database/sql\"    _ \"github.com/ziutek/mymysql/godrv\")\n\n​        _操作其实是引入该包，而不直接使用包里面的函数，而是调用了该包里面的init函数。\nstruct类型123456789101112131415type person struct &#123;\tname string\tage int&#125;var P person  // P现在就是person类型的变量了P.name = \"Astaxie\"  // 赋值\"Astaxie\"给P的name属性.P.age = 25  // 赋值\"25\"给变量P的age属性fmt.Printf(\"The person's name is %s\", P.name)  // 访问P的name属性.// 其他声明方式P := person&#123;\"Tom\", 25&#125;             // P 为 person P := person&#123;age:24, name:\"Tom\"&#125;    // P 为 person 若在包外，age和name都是私有的，无法访问，没法用这种方式赋值P := new(person)\t\t\t\t   // P 为 *person\n\nstruct的匿名字段我们上面介绍了如何定义一个struct，定义的时候是字段名与其类型一一对应，实际上Go支持只提供类型，而不写字段名的方式，也就是匿名字段，也称为嵌入字段。\n当匿名字段是一个struct的时候，那么这个struct所拥有的全部字段都被隐式地引入了当前定义的这个struct。\n让我们来看一个例子，让上面说的这些更具体化\n12345678910111213141516171819202122232425262728293031323334353637package mainimport \"fmt\"type Human struct &#123;\tname string\tage int\tweight int&#125;type Student struct &#123;\tHuman  // 匿名字段，那么默认Student就包含了Human的所有字段\tspeciality string&#125;func main() &#123;\t// 我们初始化一个学生\tmark := Student&#123;Human&#123;\"Mark\", 25, 120&#125;, \"Computer Science\"&#125;\t// 我们访问相应的字段\tfmt.Println(\"His name is \", mark.name)\tfmt.Println(\"His age is \", mark.age)\tfmt.Println(\"His weight is \", mark.weight)\tfmt.Println(\"His speciality is \", mark.speciality)\t// 修改对应的备注信息\tmark.speciality = \"AI\"\tfmt.Println(\"Mark changed his speciality\")\tfmt.Println(\"His speciality is \", mark.speciality)\t// 修改他的年龄信息\tfmt.Println(\"Mark become old\")\tmark.age = 46\tfmt.Println(\"His age is\", mark.age)\t// 修改他的体重信息\tfmt.Println(\"Mark is not an athlet anymore\")\tmark.weight += 60\tfmt.Println(\"His weight is\", mark.weight)&#125;\n\n123// 或者mark.Human = Human&#123;\"Marcus\", 55, 220&#125;mark.Human.age -= 1\n\n123456789101112131415// 通过匿名访问和修改字段相当的有用，但是不仅仅是struct字段哦，所有的内置类型和自定义类型都是可以作为匿名字段的。type Skills []stringtype Human struct &#123;\tname string\tage int\tweight int&#125;type Student struct &#123;\tHuman  // 匿名字段，struct\tSkills // 匿名字段，自定义的类型string slice\tint    // 内置类型作为匿名字段\tspeciality string&#125;\n\n方法继承1234567891011121314151617181920212223242526272829type Base struct &#123;&#125;type SubA struct &#123;\tBase&#125;type SubB struct &#123;\tBase&#125;// 基类的Do方法，由于SubB类包含了它的匿名字段，相当于继承，因此SubB相当于继承了这个方法，可以直接调用func (bb *Base) Do() &#123;\tprintln(\"in Base\")&#125;// 重写Base类的Do方法func (sa *SubA) Do() &#123;\tprintln(\"in SubA\")&#125;obj := Base&#123;&#125;obj.Do() // Base.Do()obj = SubA&#123;&#125;obj.Do() // SubA.Do()obj = SubB&#123;&#125;obj.Do() // Base.Do()\n\n接口interface12345678910111213141516171819202122type Student struct &#123;\tname string&#125;type Person interface &#123;\tgetName() string\tsetName(name string)&#125;func (s Student) getName() string &#123;\treturn s.name&#125;func (s *Student) setName(name string) &#123;\ts.name = name&#125;var person Personperson = new(Student)person.setName(\"Tom\")name := person.getName()println(name)\n\n所有的类型都都实现了空interface即interface{}，因此可以用interface{}表示任意类型。\n1234567// 定义a为空接口var a interface&#123;&#125;var i int = 5s := \"Hello world\"// a可以存储任意类型的数值a = ia = s\n\nfmt.Print使用了接口Stringer，只要实现String()方法，即可让结构直接用fmt打印。\n123type Stringer interface &#123;\t String() string&#125;\n\n123456789101112type Human struct &#123;    name string    age int    phone string&#125;func (h Human) String() string &#123;    return \"(\" + name + \", \" + string(age) + \", \" + phone + \")\"&#125;Bob := Human&#123;\"Bob\", 39, \"123456\"&#125;fmt.Println(Bob)\n\n判断interface变量存储的类型Comma-ok断言\n1value, ok = element.(T)\n\nvalue就是变量的值，ok是一个bool类型，element是interface变量，T是断言的类型。如果element里面确实存储了T类型的数值，那么ok返回true，否则返回false。\n嵌入interfaceGo里面真正吸引人的是它内置的逻辑语法，就像我们在学习Struct时学习的匿名字段，多么的优雅啊，那么相同的逻辑引入到interface里面，那不是更加完美了。如果一个interface1作为interface2的一个嵌入字段，那么interface2隐式的包含了interface1里面的method。\n12345678type Interface1 interface &#123;    //...&#125;type Interface2 interface &#123;    Interface1    //...&#125;\n\n1234type ReadWriter interface &#123;\tReader\tWriter&#125;\n\n反射reflectGo语言实现了反射，所谓反射就是能检查程序在运行时的状态。我们一般用到的包是reflect包。\n123456t := reflect.TypeOf(i)    //得到类型的元数据,通过t我们能获取类型定义里面的所有元素v := reflect.ValueOf(i)   //得到实际的值，通过v我们获取存储在里面的值，还可以去改变值tag := t.Elem().Field(0).Tag  //获取定义在struct里面的标签name := v.Elem().Field(0).String()  //获取存储在第一个字段里面的值\n\n12345var x float64 = 3.4v := reflect.ValueOf(x)fmt.Println(\"type:\", v.Type())fmt.Println(\"kind is float64:\", v.Kind() == reflect.Float64)fmt.Println(\"value:\", v.Float())\n\n对反射字段的修改\n1234var x float64 = 3.4p := reflect.ValueOf(&amp;x)v := p.Elem()v.SetFloat(7.1)\n\n并发goroutinegoroutine是Go并行设计的核心。goroutine说到底其实就是协程，但是它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部帮你实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)，当然会根据相应的数据伸缩。也正因为如此，可同时运行成千上万个并发任务。goroutine比thread更易用、更高效、更轻便。\ngoroutine是通过Go的runtime管理的一个线程管理器。goroutine通过go关键字实现了，其实就是一个普通的函数。\n1go hello(a, b, c)\n\n1234567891011121314151617181920212223242526272829package mainimport (\t\"fmt\"\t\"runtime\")func say(s string) &#123;\tfor i := 0; i &lt; 5; i++ &#123;\t\truntime.Gosched()\t\tfmt.Println(s)\t&#125;&#125;func main() &#123;\tgo say(\"world\") //开一个新的Goroutines执行\tsay(\"hello\") //当前Goroutines执行&#125;// 以上程序执行后将输出：// hello// world// hello// world// hello// world// hello// world// hello\n\n上面的多个goroutine运行在同一个进程里面，共享内存数据，不过设计上我们要遵循：不要通过共享来通信，而要通过通信来共享。\n\nruntime.Gosched()表示让CPU把时间片让给别人,下次某个时候继续恢复执行该goroutine。\n\n\n默认情况下，在Go 1.5将标识并发系统线程个数的runtime.GOMAXPROCS的初始值由1改为了运行环境的CPU核数。\n\n通道channelgoroutine运行在相同的地址空间，因此访问共享内存必须做好同步。那么goroutine之间如何进行数据的通信呢，Go提供了一个很好的通信机制channel。channel可以与Unix shell 中的双向管道做类比：可以通过它发送或者接收值。这些值只能是特定的类型：channel类型。定义一个channel时，也需要定义发送到channel的值的类型。注意，必须使用make 创建channel：\n123ci := make(chan int)cs := make(chan string)cf := make(chan interface&#123;&#125;)\n\nchannel通过操作符&lt;-来接收和发送数据\n12ch &lt;- v    // 发送v到channel ch.v := &lt;-ch  // 从ch中接收数据，并赋值给v\n\n12345678910111213141516171819202122package mainimport \"fmt\"func sum_chan(a []int, c chan int) &#123;\ttotal := 0\tfor _, v := range a &#123;\t\ttotal += v\t&#125;\tc &lt;- total  // send total to c&#125;func main() &#123;\ta := []int&#123;7, 2, 8, -9, 4, 0&#125;\tc := make(chan int)\tgo sum_chan(a[:len(a)/2], c)\tgo sum_chan(a[len(a)/2:], c)\tx, y := &lt;-c, &lt;-c  // receive from c\tfmt.Println(x, y, x + y)&#125;\n\n默认情况下，channel接收和发送数据都是阻塞的，除非另一端已经准备好，这样就使得Goroutines 同步变的更加的简单，而不需要显式的lock。所谓阻塞，也就是如果读取（value := &lt;-ch）它将会被阻塞，直到有数据接收。其次，任何发送（ch&lt;-5）将会被阻塞，直到数据被读出。无缓冲channel是在多个goroutine之间同步很棒的工具。\nBuffered Channel上面我们介绍了默认的非缓存类型的channel，不过Go也允许指定channel的缓冲大小，很简单，就是channel可以存储多少元素。ch:= make(chan bool, 4)，创建了可以存储4个元素的bool 型channel。在这个channel 中，前4个元素可以无阻塞的写入。当写入第5个元素时，代码将会阻塞，直到其他goroutine从channel 中读取一些元素，腾出空间。\n1ch := make(chan type, value)\n\n当 value = 0 时，channel 是无缓冲阻塞读写的，当value &gt; 0 时，channel 有缓冲、是非阻塞的，直到写满 value 个元素才阻塞写入。\n我们看一下下面这个例子，你可以在自己本机测试一下，修改相应的value值\n12345678910111213package mainimport \"fmt\"func main() &#123;\tc := make(chan int, 2)//修改2为1就报错，修改2为3可以正常运行\tc &lt;- 1\tc &lt;- 2\tfmt.Println(&lt;-c)\tfmt.Println(&lt;-c)&#125;        //修改为1报如下的错误:        //fatal error: all goroutines are asleep - deadlock!\n\nRange和Close上面这个例子中，我们需要读取两次c，这样不是很方便，Go考虑到了这一点，所以也可以通过range，像操作slice或者map一样操作缓存类型的channel，请看下面的例子\n12345678910111213141516171819202122package mainimport (\t\"fmt\")func fibonacci(n int, c chan int) &#123;\tx, y := 1, 1\tfor i := 0; i &lt; n; i++ &#123;\t\tc &lt;- x\t\tx, y = y, x + y\t&#125;\tclose(c)&#125;func main() &#123;\tc := make(chan int, 10)\tgo fibonacci(cap(c), c)\tfor i := range c &#123;\t\tfmt.Println(i)\t&#125;&#125;\n\nfor i := range c能够不断的读取channel里面的数据，直到该channel被显式的关闭。上面代码我们看到可以显式的关闭channel，生产者通过内置函数close关闭channel。关闭channel之后就无法再发送任何数据了，在消费方可以通过语法v, ok := &lt;-ch测试channel是否被关闭。如果ok返回false，那么说明channel已经没有任何数据并且已经被关闭。必须close(c)\n\n记住应该在生产者的地方关闭channel，而不是消费的地方去关闭它，这样容易引起panic\n\n\n另外记住一点的就是channel不像文件之类的，不需要经常去关闭，只有当你确实没有任何发送数据了，或者你想显式的结束range循环之类的\n\nSelect我们上面介绍的都是只有一个channel的情况，那么如果存在多个channel的时候，我们该如何操作呢，Go里面提供了一个关键字select，通过select可以监听channel上的数据流动。\nselect默认是阻塞的，只有当监听的channel中有发送或接收可以进行时才会运行，当多个channel都准备好的时候，select是随机的选择一个执行的。\n12345678910111213141516171819202122232425262728package mainimport &quot;fmt&quot;func fibonacci(c, quit chan int) &#123;\tx, y := 1, 1\tfor &#123;\t\tselect &#123;\t\tcase c &lt;- x:\t\t\tx, y = y, x + y\t\tcase &lt;-quit:\t\t\tfmt.Println(&quot;quit&quot;)\t\t\treturn\t\t&#125;\t&#125;&#125;func main() &#123;\tc := make(chan int)\tquit := make(chan int)\tgo func() &#123;\t\tfor i := 0; i &lt; 10; i++ &#123;\t\t\tfmt.Println(&lt;-c)\t\t&#125;\t\tquit &lt;- 0\t&#125;()\tfibonacci(c, quit)&#125;\n\n在select里面还有default语法，select其实就是类似switch的功能，default就是当监听的channel都没有准备好的时候，默认执行的（select不再阻塞等待channel）。\n123456select &#123;case i := &lt;-c:\t// use idefault:\t// 当c阻塞的时候执行这里&#125;\n\n超时有时候会出现goroutine阻塞的情况，那么我们如何避免整个程序进入阻塞的情况呢？我们可以利用select来设置超时，通过如下的方式实现：\n1234567891011121314151617func main() &#123;\tc := make(chan int)\to := make(chan bool)\tgo func() &#123;\t\tfor &#123;\t\t\tselect &#123;\t\t\t\tcase v := &lt;- c:\t\t\t\t\tprintln(v)\t\t\t\tcase &lt;- time.After(5 * time.Second):\t\t\t\t\tprintln(\"timeout\")\t\t\t\t\to &lt;- true\t\t\t\t\tbreak\t\t\t&#125;\t\t&#125;\t&#125;()\t&lt;- o&#125;\n\nruntime goroutineruntime包中有几个处理goroutine的函数：\n\nGoexit\n退出当前执行的goroutine，但是defer函数还会继续调用\n\nGosched\n让出当前goroutine的执行权限，调度器安排其他等待的任务运行，并在下次某个时候从该位置恢复执行。\n\nNumCPU\n返回 CPU 核数量\n\nNumGoroutine\n返回正在执行和排队的任务总数\n\nGOMAXPROCS\n用来设置可以并行计算的CPU核数的最大值，并返回之前的值。\n\n\nTest测试方法go test go test -v\n\ntest文件的命名为xxx_test.go。前面的xxx最好是要测试的文件的名称，但没有强制要求。\n必须import &quot;testing&quot;。\ntest文件下的每一个test case 都必须以Test开头并且符合TestXXX的形式，否则go test会直接跳过不执行。\n\n1234567891011121314151617181920212223242526272829303132333435package mainimport (\t\"fmt\"\t\"testing\")func Print1to20() int &#123;\tfor i := 1; i &lt;= 20; i++ &#123;\t\tfmt.Println(i)\t&#125;\treturn 210&#125;func TestPrint(t *testing.T) &#123; // 测试函数名必须以Test开头， 参数必须是 *testing.T\tresult := Print1to20()\tif result == 210 &#123;\t\tt.Errorf(\"Wrong\\n\") // 打印错误信息并且当前test case会被跳过\t\tt.SkipNow()         // 加这个否则会继续往后执行\t&#125;\tfmt.Println(\"test - done\")&#125;func TestForSubtest(t *testing.T) &#123; // 使用子测试, 能够保证顺序输出\tt.Run(\"a1\", func(t *testing.T) &#123; fmt.Println(\"a1\") &#125;)\tt.Run(\"a2\", func(t *testing.T) &#123; fmt.Println(\"a2\") &#125;)\tt.Run(\"a3\", func(t *testing.T) &#123; fmt.Println(\"a3\") &#125;)&#125;func TestMain(m *testing.M) &#123; // TestMain最先初始化，总是第一个被调用\t// 当没有添加TestMain函数时，会自动调用其他的test函数，但当添加TestMain函数后需要手动调用m.Run()调用其他test函数\tfmt.Println(\"test main first\")\tm.Run() // 调用其他test&#125;\n\nbenchmarkgo test -bench=.\n\nbenchmark函数以Benchmark开头\nbenchmark的case一般会跑b.N次，并且每次执行都会如此\n在执行过程中会根据实际case的执行时间是否稳定会增加b.N的次数以达到稳态\n\n123456func BenchmarkAll(b *testing.B) &#123; // 函数名以Benchmark开头，参数为 *testing.B\t// 若TestMain中不加m.Run()则无法执行\tfor i := 0; i &lt; b.N; i++ &#123;\t\tPrint1to20()\t&#125;&#125;\n\n","plink":"https://yuxinzhao.net/golang-basic/"},{"title":"Post for Testing","date":"2019-09-24T21:48:11.000Z","updated":"2022-01-04T08:35:48.615Z","content":"This artice is a example to test the hexo-theme-inside theme.\nMathThe is a inline math \n\\alpha = \\sum_i^n \\frac{1}{i}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n\n\n \n \n\n\n\n.\n\n\\begin{align}\n    p(v_i=1|\\mathbf{h}) &amp; = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\\n    p(h_j=1|\\mathbf{v}) &amp; = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n \n\n\n \n \n\n \n \n\n \n \n \n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n\n \n \n\n \n\n \n \n\n \n\n\n\n \n \n\n \n\n \n \n\n\n \n\n \n \n\n\n\n \n \n\n \n\n \n \n\n \n\n\n\n\n\n\n\n\n\n\nCode Block1234$ git clone http://github.com/name/repo.git$ git log --oneline$ git status$ git push\n\ntemp.cpplink12345#include &lt;stdio.h&gt;int main() &#123;    return 0;&#125;\n\nTable\n\n\nA\nB\nC\n\n\n\nfunc()\na\na\n\n\n\n\\alpha = \\sum_i^n \\frac{1}{i}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n\n\\alpha = \\sum_i^n \\frac{1}{i}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n\n\\alpha = \\sum_i^n \\frac{1}{i}\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n\n\n\n\n \n \n\n\n\n\n\n\ntext\n4\n5\n\n\nH1H2H3H4H5Imagenetwork image\nlocal image\n163 Music\n\n\nBilibili Video123&lt;div class=\"bilibili\"&gt;&lt;iframe src=\"//player.bilibili.com/player.html?aid=65010879&amp;cid=112829638&amp;page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"&gt; &lt;/iframe&gt;&lt;/div&gt;\n\n\n \n","thumbnail":"https://images7.alphacoders.com/818/818437.png","plink":"https://yuxinzhao.net/post-for-test/"},{"title":"Strassen矩阵乘法","date":"2019-09-23T22:01:20.000Z","updated":"2022-01-04T08:35:48.647Z","content":"本文将介绍Strassen矩阵乘法，利用分治思想减少矩阵乘法所需要的计算量。\n\n\n\n\n假定我们要求两个 \nn \\times n\n\n\n\n\n\n \n \n \n\n 矩阵 \nA\n\n\n\n\n \n\n 和 \nB\n\n\n\n\n \n\n 的矩阵乘法结果 \nC\n\n\n\n\n \n\n, 即 \nC = A \\times B\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n。\n假定 \nn = 2^k, k \\in \\mathbb{Z}^+\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n\n \n \n\n\n, 我们可以将 \nA\n\n\n\n\n \n\n, \nB\n\n\n\n\n \n\n, \nC\n\n\n\n\n \n\n 划分为4个 \n(n/2) \\times (n/2)\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n 的子矩阵。\n\n\\begin{align}\n\\begin{bmatrix}\nC_{00} &amp; C_{01} \\\\\nC_{10} &amp; C_{11} \n\\end{bmatrix}\n&amp;= \n\\begin{bmatrix}\nA_{00} &amp; A_{01} \\\\\nA_{10} &amp; A_{11} \n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nB_{00} &amp; B_{01} \\\\\nB_{10} &amp; B_{11} \n\\end{bmatrix}\n\\\\\n&amp;=\n\\begin{bmatrix}\nM_1 + M_4 -M_5 +M_7 &amp; M_3 + M_5 \\\\\nM_2 + M_4 &amp; M_1 + M_3 - M_2 + M_6\n\\end{bmatrix}\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\n\n\n \n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n \n\n \n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n \n\n \n \n\n\n\n \n\n \n \n\n\n\n\n \n\n\n\n \n\n \n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n\n \n \n \n\n \n \n\n\n\n\n\n \n \n \n\n \n \n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n\n\n \n\n\n\n\n\n\n\n\\begin{align}\nM_1 &amp;= (A_{00} + A_{11}) \\times (B_{00}+B_{11}) \\\\\nM_2 &amp;= (A_{10} + A_{11}) \\times B_{00} \\\\\nM_3 &amp;= A_{00} \\times (B_{01} - B_{11}) \\\\\nM_4 &amp;= A_{11} \\times (B_{10} - B_{00}) \\\\\nM_5 &amp;= (A_{00} + A_{01}) \\times B_{11}) \\\\\nM_6 &amp;= (A_{10} - A_{00}) \\times (B_{00} + B_{01}) \\\\\nM_7 &amp;= (A_{01} - A_{11}) \\times (B_{10} + B_{11})\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n\n \n\n \n \n\n\n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n\n \n\n \n\n \n \n\n\n \n\n\n\n\n\n时间复杂度分析：\n乘法次数 \nM(n)\n\n\n\n\n\n\n\n \n \n \n \n\n :\n\nM(n) = \n\\begin{cases}\n7 M(n/2), &amp;n&gt;1 \\\\\n1, &amp;n=1\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n因为 \nn=2^k\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n,\n\nM(2^k) = 7 M(2^{k-1}) = 7 \\left[ 7 M(2^{k-2}) \\right] = \\cdots = 7^k\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n \n \n\n \n\n \n \n \n\n\n \n \n\n \n \n \n\n \n \n\n\n\n因为 \nk= \\log_2 n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n \n\n,\n\nM(n) = 7^{\\log_2 n} = n^{\\log_2 7} \\approx n^{2.807}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n减少的乘法次数是以额外的加减法次数为代价的，因此我们需要计算加减法次数 \nA(n)\n\n\n\n\n\n\n\n \n \n \n \n\n,\n\nA(n) = \n\\begin{cases}\n7 A(n/2) + 18 (n/2)^2 , &amp;n&gt;1 \\\\\n0, &amp;n=1\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n \n \n \n\n \n \n\n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n根据主定理：\n\nT(n) = a T(n/b) + f(n) \\\\\nf(n) \\in \\Theta(n^d) \\\\\nT(n) \\in \n\\begin{cases}\n\\Theta(n^d), &amp;a \\lt b^d \\\\\n\\Theta(n^d \\log n), &amp;a=b^d \\\\\n\\Theta(n^{\\log_b a}), &amp;a&gt;b^d \n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n \n \n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n\n\n\n\n\n可以推出\n\nA(n) \\in \\Theta(n^{\\log_2 7})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n\n于是算法的总时间复杂度为 \n\\left( M(n) + A(n) \\right) \\in \\Theta(n^{log_2 7})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n\n\n \n\n.\n","plink":"https://yuxinzhao.net/strassen-matrix-multiplication/"},{"title":"大整数乘法","date":"2019-09-23T19:57:29.000Z","updated":"2022-01-04T08:35:48.539Z","content":"通常两个n位十进制整数相乘 (如果两个数位数不同则在位数少的数字前填0至位数相同)，第一个数中的n个数字都分别要与第二个数中的n个数字相乘，这样一共就要做 \nn^2\n\n\n\n\n\n \n \n\n 次位乘，因此时间复杂度为 \n\\Theta(n^2)\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n。本文介绍如何优化乘法的时间复杂度。\n\n\n\n\n以两个整数23和14相乘为例，这两个数字可以表示为：\n\n23 = 2 \\times 10^1 + 3 \\times 10^0 \\\\\n14 = 1 \\times 10^1 + 4 \\times 10^0\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n\n\n\n两者相乘为\n\n\\begin{align}\n23 \\times 14 &amp; = (2 \\times 10^1 + 3 \\times 10^0) \\times (1 \\times 10^1 + 4 \\times 10^0) \\\\\n&amp;= (2 \\times 1) \\times 10^2 + (2 \\times 4 + 3 \\times 1) \\times 10^1 + (3 \\times 4) \\times 10^0 \\\\\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n\n \n \n \n\n \n\n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n \n \n \n\n\n\n\n\n\n这个算法跟笔算算法相同，都会进行4次位乘 (\n2 \\times 1\n\n\n\n\n\n\n \n \n \n\n,  \n2 \\times 4\n\n\n\n\n\n\n \n \n \n\n,  \n3 \\times 1\n\n\n\n\n\n\n \n \n \n\n,  \n3 \\times 4\n\n\n\n\n\n\n \n \n \n\n)，但我们可以通过重复使用 \n2 \\times 1\n\n\n\n\n\n\n \n \n \n\n 和 \n3 \\times 4\n\n\n\n\n\n\n \n \n \n\n 的计算结果，通过将中间项 \n(2 \\times 4 + 3 \\times 1)\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n 化简减少位乘的次数:\n\n2 \\times 4 + 3 \\times 1 = (2 + 3) \\times (1 + 4) - (2 \\times 1) - (3 \\times 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n将它一般化一点：\n对于任何两位数 \na = a_1 a_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n 和 \nb = b_1 b_0\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n \n \n\n\n 来说，它们的积 \nc\n\n\n\n\n \n\n 可以用下列公式计算：\n\n\\begin{align}\nc_2 &amp;= a_1 \\times b_1 \\\\\nc_0 &amp;= a_0 \\times b_0 \\\\\nc_1 &amp;= (a_1 + a_0) \\times (b_1 + b_0) - (c_2 + c_0) \\\\\nc &amp;= a \\times b = c_2 \\times 10^2 + c_1 \\times 10^1 + c_0\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n\n\n\n\n\n运用递归将其更一般化：\n对任意n位数 \na\n\n\n\n\n \n\n 和 \nb\n\n\n\n\n \n\n，采用分治法将 n 位整数拆成两个 n/2 位整数，即 \n a = a_1 \\times 10^{n/2} + a_0\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n，\n b = b_1 \\times 10^{n/2} + b_0\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n 。使用与前面两位数的相同方法，可得：\n\n\\begin{align}\nc_2 &amp;= a_1 \\times b_1 \\\\\nc_0 &amp;= a_0 \\times b_0 \\\\\nc_1 &amp;= (a_1 + a_0) \\times (b_1 + b_0) - (c_2 + c_0) \\\\\nc &amp;= a \\times b = c_2 \\times 10^n + c_1 \\times 10^{n/2} + c_0\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n \n\n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n\n\n\n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n\n \n\n \n \n\n\n\n\n\n\n计算时 \na_1 \\times b_1\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n 等 n/2 位数乘法同n位乘法，递归计算下去即可。\n时间复杂度计算：\n因为n位数的乘法需要对n/2位数做3次乘法，乘法次数 \nM(n)\n\n\n\n\n\n\n\n \n \n \n \n\n 的递推式：\n\nM(n) =\n\\begin{cases}\n3 M(n/2), &amp;n&gt;1 \\\\\n1, &amp;n=1\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n当 \nn=2^k\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n 时，\n\nM(2^k) = 3 M(2^{k-1}) = 3 \\left[ 3 M(2^{k-2}) \\right] = \\cdots = 3^k\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n \n \n \n\n \n\n \n \n \n\n\n \n \n \n\n \n \n \n \n\n \n\n \n \n \n\n\n \n \n\n \n \n \n\n \n \n\n\n\n因为 \nk=\\log_2 n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n \n \n\n \n\n ，\n\nM(n) = 3^{\\log_2 n} = n^{log_2 3} \\approx n^{1.585}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n\n \n \n\n \n\n\n \n\n \n\n \n \n \n \n \n\n\n\n\n以上我们只考虑了乘法次数，接下来考虑加法次数 \nA(n)\n\n\n\n\n\n\n\n \n \n \n \n\n：\n每次需要4次加运算和1次减运算，每个运算都有n次位运算，因此：\n\nA(n) = \n\\begin{cases}\n3 A(n/2) + 5n, &amp;n&gt;1 \\\\\n1, &amp;n=1\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n\n\n\n\n \n \n \n\n\n \n \n \n\n\n\n\n\n\n根据主定理：\n\nT(n) = a T(n/b) + f(n) \\\\\nf(n) \\in \\Theta(n^d) \\\\\nT(n) \\in \n\\begin{cases}\n\\Theta(n^d), &amp;a \\lt b^d \\\\\n\\Theta(n^d \\log n), &amp;a=b^d \\\\\n\\Theta(n^{\\log_b a}), &amp;a&gt;b^d \n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n\n \n \n\n \n\n\n \n \n \n \n \n\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n \n \n\n \n \n\n \n \n\n\n \n \n\n \n \n\n\n \n \n \n\n \n \n \n\n\n \n \n\n \n\n \n \n \n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n \n \n\n \n \n\n\n\n\n\n\n\n\n可以推出\n\nA(n) \\in \\Theta(n^{\\log_2 3})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n\n于是算法的总时间复杂度为 \n\\left( M(n) + A(n) \\right) \\in \\Theta(n^{log_2 3})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n\n \n \n\n \n\n\n \n\n.\n","plink":"https://yuxinzhao.net/big-number-multiplication/"},{"title":"[PyTorch学习笔记] PyTorch的可视化工具Visdom","date":"2019-07-28T13:53:15.000Z","updated":"2022-01-04T08:35:48.631Z","content":"Visdom:一个灵活的可视化工具，可用来对于实时，富数据的创建，组织和共享。支持Torch和Numpy还有PyTorch。\nvisdom 可以实现远程数据的可视化，对科学实验有很大帮助。我们可以远程的发送图片和数据，并进行在ui界面显示出来，检查实验结果，或者debug。\n\n\n\n\n安装与启动安装\n1$ pip install visdom\n\n启动\n1$ python -m visdom.server\n\n接口大多数接口可以输入一个tensor(保存数据），和一个可选的tensor Y（标签或者时间戳）, 另外都可以指定窗口win,和汇出图添加到那个env上，另外options输入可以修改默认的绘图属性，输入参数基于表中键的匹配，有一些通用的options可以选择,下面列出了通用的可视化options(除了plot.image和plot.text外）\n\nopts.title : figure title\nopts.width : figure width\nopts.height : figure height\nopts.showlegend : show legend (true or false)\nopts.xtype : type of x-axis (&#39;linear&#39; or &#39;log&#39;)\nopts.xlabel : label of x-axis\nopts.xtick : show ticks on x-axis (boolean)\nopts.xtickmin : first tick on x-axis (number)\nopts.xtickmax : last tick on x-axis (number)\nopts.xtickvals : locations of ticks on x-axis (table of numbers)\nopts.xticklabels : ticks labels on x-axis (table of strings)\nopts.xtickstep : distances between ticks on x-axis (number)\nopts.ytype : type of y-axis (&#39;linear&#39; or &#39;log&#39;)\nopts.ylabel : label of y-axis\nopts.ytick : show ticks on y-axis (boolean)\nopts.ytickmin : first tick on y-axis (number)\nopts.ytickmax : last tick on y-axis (number)\nopts.ytickvals : locations of ticks on y-axis (table of numbers)\nopts.yticklabels : ticks labels on y-axis (table of strings)\nopts.ytickstep : distances between ticks on y-axis (number)\nopts.marginleft : left margin (in pixels)\nopts.marginright : right margin (in pixels)\nopts.margintop : top margin (in pixels)\nopts.marginbottom: bottom margin (in pixels)\n\n使用导入12345678import torchimport numpy as npfrom visdom import Visdom# 新建一个连接客户端# 指定env = u'test1'，默认端口为8097，host是‘localhost'vis = visdom.Visdom(env=\"test1\", use_incoming_socket=False)assert vis.check_connection()\n\n曲线 vis.line123456# win窗口名为 'sinx', 如果不指定回自动新分配一个pane，两次使用同一个win将覆盖# 标题为 'y=sin(x)'# visdom 支持pytorch的tensor 和 numpy的ndarray类型， 但不支持python的int,float等原生类型，需要转换成ndarray或者tensorx = torch.arange(1, 30, 0.01)y = torch.sin(x)vis.line(X=x, Y=y, win='sinx', opts=&#123;'title': 'y=sin(x)'&#125;)\n\n\n追加数据\n1234567# append 追加数据# y = x# 窗口名: polynomialfor i in range(0, 10):    x = torch.Tensor([i])  # 要转成tensor，Visdom不支持python原生的int,float等    y = x    vis.line(X=x, Y=y, win='polynomial', update='append' if i&gt;0 else None)\n\n在同一附图新增曲线，而不覆盖\n123456# update Trace 新增一条线，而不覆盖# y= (x**2)/9# 窗口名: polynomialx = torch.arange(0, 9, 0.1)y = (x ** 2) / 9vis.line(X=x, Y=y, win='polynomial', name='this is a new Trace', update='new')\n\n\n附带label和对轴的控制\n123456789101112131415161718# y = x**3x = torch.arange(0, 2, 0.05)y = x**3vis.line(    X = x,     Y = y,     win = 'x3',     opts = dict(        title = 'x^3',        legend = ['y = x^3'],        xlabel = 'x dim',        ylabel = 'y dim',        xtickmin = 0,        xtickmax = 1,        ytickmin = 0,        ytickmax = 1    ))\n\n\n图像 vis.image / vis.images12345678910111213# 图像 vis.image / vis.images# image的画图功能可分为如下两类：# image接收一个二维或三维向量，H x W 或 3 x H x W，前者是黑白图像，后者是彩色图像。# images接收一个四维向量 N x C x H x W ，C可以是1或3，分别代表黑白和彩色图像。可实现类似torchvision中make_grid的功能，将多张图片拼接在一起。images也可以接收一个二维或三维的向量，此时它所实现的功能与image一致。# 可视化一个随机的黑白图片vis.image(torch.randn(64, 64).numpy())# 可视化一个随机的彩色图片vis.image(torch.randn(3, 64, 64).numpy(), win='random2')# 可视化36张随机彩色图片，每行6张vis.images(torch.randn(36, 3, 64, 64).numpy(), nrow=6, win='random3', opts=&#123;'title': 'random images'&#125;)\n\n\n12345678import numpy as np import cv2 as cv # visdom 显示图像img = cv.imread('e:/image/avator.jpg')    # H x W x C (BGR)img = cv.cvtColor(img, cv.COLOR_BGR2RGB)  # H x W x C (RGB)img = np.transpose(img, [2, 0, 1])        # C x H x Hvis.image(img, win='image_example')\n\n\n文本 vis.text123456# 文本 vis.text# 支持所有html标签vis.text(u''' &lt;h1&gt; Hello Visdom &lt;/h1&gt; &lt;br&gt; 测试 ''',        win='visdom',        opts=&#123;'title': 'visdom简介'&#125;)\n\n\n散点图（2D/3D） vis.scatter1234# 散点图 vis.scatterx = torch.rand(255, 2)          # [0, 1) 均匀分布y = (torch.randn(255) &gt; 0) + 1  # 标准正太分布vis.scatter(X=x, Y=y, win='scatter', opts=&#123;'title': '散点图', 'maskersize':10, 'legend':['men', 'women']&#125;)\n\n\n条形图 vis.bar1234567891011121314151617# 条形图 barvis.bar(X=np.random.rand(20))vis.bar(    X = np.abs(np.random.rand(5, 3)),    opts = dict(        stacked = True,        legend = ['Facebook', 'Google', 'Twitter'],        rownames = ['2012', '2013', '2014', '2015', '2016']    ))vis.bar(    X = np.random.rand(20, 3),    opts = dict(        stacked = False,        legend = ['The Netherlands', 'France', 'United States']    ))\n\n\n直方图 vis.histogram12# 直方图 histogram (横坐标0-1（最大值与最小值之间）， 纵坐标频数)vis.histogram(X=np.random.rand(10000), opts=dict(numbins=20, title='histogarm'))\n\n\n热程图 vis.heatmap123456789# heatmapvis.heatmap(    X=np.outer(np.arange(1, 6), np.arange(1, 11)),    opts=dict(        columnnames=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'],        rownames=['y1', 'y2', 'y3', 'y4', 'y5'],        colormap='Electric',    ))\n\n\n等值线图 vis.contour12345# 等值线图 contourx = np.tile(np.arange(1, 101), (100, 1))y = x.transpose()X = np.exp((((x - 50) ** 2) + ((y - 50) ** 2)) / -(20.0 ** 2))vis.contour(X=X, opts=dict(colormap='Viridis'))\n\n\n表面图 vis.surf12# 表面图 surfacevis.surf(X=X, opts=dict(colormap='Hot'))\n\n\n视频 vis.video123# 视频 videovideofile = 'e:/video/001.mp4'vis.video(videofile=videofile, opts=&#123;'width': 864, 'height':480&#125;)\n\n\n保存与读取创建时指定保存文件路径\n1vis = visdom.Visdom(env=\"test1\", log_to_filename=\"./log/test1.log\")\n\n读取时\n12import visdomvisdom.Visdom().replay_log(\"./log/test1.log\")\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-visdom/"},{"title":"[PyTorch学习笔记] 数据的加载和预处理","date":"2019-07-23T17:20:42.000Z","updated":"2022-01-04T08:35:48.627Z","content":"PyTorch通过torch.utils.data对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。 并且torchvision已经预先实现了常用图像数据集，包括CIFAR-10，ImageNet、COCO、MNIST、LSUN等数据集，可通过torchvision.datasets方便地调用\n\n\n\n\nDatasetDataset是一个抽象类, 为了能够方便的读取，需要将要使用的数据包装为Dataset类。 自定义的Dataset需要继承它并且实现两个成员方法：\n\n__getitem__() 该方法定义用索引(0 到 len(self))获取一条数据或一个样本\n__len__() 该方法返回数据集的总长度\n\n下面我们使用kaggle上的一个竞赛bluebook for bulldozers自定义一个数据集，为了方便介绍，我们使用里面的数据字典来做说明（因为条数少）\n1234567891011121314# 定义一个数据集class BulldozerDataset(Dataset):    \"\"\" 数据集演示 \"\"\"    def __init__(self, csv_file):        \"\"\" 实现初始化方法，在初始化的时候将数据载入 \"\"\"        self.dataframe = pd.read_csv(csv_file)    def __len__(self):        \"\"\" 返回dataframe的长度 \"\"\"        return len(self.dataframe)    def __getitem__(self, idx):        \"\"\" 根据 idx 返回一行数据 \"\"\"        return self.dataframe.iloc[idx].SalePrice\n\n至此，我们的数据集已经定义完成了，我们可以实例话一个对象访问他\n1dataset_demo= BulldozerDataset('median_benchmark.csv')\n\n我们可以直接使用如下命令查看数据集数据\n1234#实现了 __len__ 方法所以可以直接使用len获取数据总数len(dataset_demo)#用索引可以直接访问对应的数据, 对应 __getitem__ 方法dataset_demo[0]\n\n121157324000.0\n\n自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据\nDataloaderDataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小), shuffle(是否进行shuffle操作), num_workers(加载数据的时候使用几个子进程)，下面做一个简单的操作\n1dataloader = torch.utils.data.DataLoader(dataset_demo, batch_size=10, shuffle=True, num_workers=0)\n\nDataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据\n12idata = iter(dataloader)print(next(idata))\n\n常见的用法是使用for循环对其进行遍历\n12for i, data in enumerate(dl):    print(i,data)\n\ntorchvision 包torchvision 是PyTorch中专门用来处理图像的库，PyTorch官网的安装教程中最新的pip install torchvision 就是安装这个包。\ntorchvision.datasetstorchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用：\n\nMNIST\nCOCO\nCaptions\nDetection\nLSUN\nImageFolder\nImagenet-12\nCIFAR\nSTL10\nSVHN\nPhotoTour\n\n123456import torchvision.datasets as datasetstrainset = datasets.MNIST(root='./data',  # 表示 MNIST 数据的加载的目录                          train=True,     # 表示是否加载数据库的训练集，False的时候加载测试集                          download=True,  # 表示是否自动下载 MNIST 数据集                          transform=None) # 表示是否需要对数据进行预处理，None为不进行预处理\n\ntorchvision.modelstorchvision不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习 torchvision.models模块的 子模块中包含以下模型结构。\n\nAlexNet\nVGG\nResNet\nSqueezeNet\nDenseNet\n\n1234#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的import torchvision.models as modelsresnet18 = models.resnet18(pretrained=True)\n\ntorchvision.transformstransforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强\n123456789from torchvision import transforms as transformstransform = transforms.Compose([    transforms.RandomCrop(32, padding=4),  #先四周填充0，在把图像随机裁剪成32*32    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转    transforms.RandomRotation((-45,45)), #随机旋转    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差 mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]])\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-6/"},{"title":"[PyTorch学习笔记] Data Parallelism","date":"2019-07-23T11:10:54.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] Data Parallelism 的学习笔记\n本文将学习如何使用 DataParallel 来使用多GPU。\n\n\n\n\n\n\nPyTorch非常容易就可以使用多GPU，用如下方式把一个模型放到GPU上：\n12device = torch.device(\"cuda:0\")model.to(device)\n\n然后复制所有的张量到GPU上：\n1mytensor = my_tensor.to(device)\n\n\n只调用my_tensor.to(device)并没有复制张量到GPU上，而是返回了一个copy。所以你需要把它赋值给一个新的张量并在GPU上使用这个张量。\n\n在多GPU上执行前向和反向传播是自然而然的事。 但是PyTorch默认将只使用一个GPU。\n使用DataParallel可以轻易的让模型并行运行在多个GPU上。\n1model = nn.DataParallel(model)\n\n导入模块和定义参数123456789# Parameters and DataLoadersinput_size = 5output_size = 2batch_size = 30data_size = 100# Devicedevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n虚拟数据集制作一个虚拟（随机）数据集， 你只需实现 __getitem__\n1234567891011121314class RandomDataset(Dataset):    def __init__(self, size, length):        self.len = length        self.data = torch.randn(length, size)    def __getitem__(self, index):        return self.data[index]    def __len__(self):        return self.lenrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)\n\n简单模型作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。 说明：DataParallel能在任何模型（CNN，RNN，Capsule Net等）上使用。\n12345678910class Model(nn.Module):        def __init__(self, input_size, output_size):        super(Model, self).__init__()        self.fc = nn.Linear(input_size, output_size)    def forward(self, input):        output = self.fc(input)        print(\"\\t In Model: input size\", input.size(), \"output size\", output.size())        return output\n\n创建一个模型和数据并行首先，我们需要创建一个模型实例和检测我们是否有多个GPU。 如果有多个GPU，使用nn.DataParallel来包装我们的模型。 然后通过model.to(device)把模型放到GPU上。\n1234567model = Model(input_size, output_size)if torch.cuda.device_count() &gt; 1:    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")    # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs    model = nn.DataParallel(model)model.to(device)\n\n运行模型1234for data in rand_loader:    input = data.to(device)    output = model(input)    print(\"Outside: input size\", input.size(), \"output_size\", output.size())\n\n当没有或者只有一个GPU时，对30个输入和输出进行批处理，得到了期望的一样得到30个输入和输出，但是如果你有多个GPU，你得到如下的结果。\n2 GPUs\n12345678910111213Let's use 2 GPUs!    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-5/"},{"title":"[PyTorch学习笔记] Training A Classifier","date":"2019-07-23T09:40:02.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] Training A Classifier 的学习笔记\n\n\n\n\n一般情况下处理图像、文本、音频和视频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。 然后把这个数组转换成 torch.*Tensor。\n\n图像可以使用 Pillow, OpenCV\n音频可以使用 scipy, librosa\n文本可以使用原始Python和Cython来加载，或者使用 NLTK或 SpaCy 处理\n\n特别的，对于图像任务，torchvision 包 包含了处理一些基本图像数据集的方法。这些数据集包括 Imagenet, CIFAR10, MNIST 等。除了数据加载以外，torchvision 还包含了图像转换器， torchvision.datasets 和 torch.utils.data.DataLoader。\ntorchvision包不仅提供了巨大的便利，也避免了代码的重复。\n以下使用的案例中，将使用CIFAR10数据集，它有如下10个类别 ：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR-10的图像都是 3x32x32大小的，即，3颜色通道，32x32像素。\n\n训练一个图像分类器依次按照下列顺序进行：\n\n使用torchvision加载和归一化CIFAR10训练集和测试集\n定义一个卷积神经网络\n定义损失函数\n在训练集上训练网络\n在测试集上测试网络\n\n读取和归一化 CIFAR10使用torchvision可以非常容易地加载CIFAR10。\n123import torchimport torchvisionimport torchvision.transforms as transforms\n\ntorchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1, 1]的张量。\n1234567891011transform = transforms.Compose(    [transforms.ToTensor(),     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n展示一些训练图像12345678910111213141516171819import matplotlib.pyplot as plt import numpy as np # 展示图像的函数def imshow(img):    img = img / 2 + 0.5  # unnormalize    npimg = img.numpy()    plt.imshow(np.transpose(npimg, (1, 2, 0)))    plt.show()# 获取随机数据dataiter = iter(trainloader)images, labels = dataiter.next()# 展示图像imshow(torchvision.utils.make_grid(images))# 显示图像标签print(\" \".join(\"%5s\" % classes[labels[j]] for j in range(4)))\n\n\n1truck horse   dog  frog\n\n定义一个卷积神经网络123456789101112131415161718192021222324import torch.nn as nnimport torch.nn.functional as F class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()                self.conv1 = nn.Conv2d(3, 6, 5) # 3 input channels, 6 output channels, 5x5 convolutional kernel        self.pool = nn.MaxPool2d(2, 2)  # 池化层可以共用        self.conv2 = nn.Conv2d(6, 16, 5)        self.fc1 = nn.Linear(16 * 5 * 5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)    def forward(self, x):        x = self.pool(F.relu(self.conv1(x)))        x = self.pool(F.relu(self.conv2(x)))        x = x.view(-1, 16 * 5 * 5)        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        return x    net = Net()\n\n定义损失函数和优化器1234import torch.optim as optimcriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n训练网络我们只需在数据迭代器上循环，将数据输入给网络，并优化。\n1234567891011121314151617181920212223for epoch in range(2):  # 多批次循环    running_loss = 0.0    for i, data in enumerate(trainloader, 0):        # 获取输入        inputs, labels = data        # 梯度置0        optimizer.zero_grad()        # 正向传播，反向传播，优化        outputs = net(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        # 打印状态信息        running_loss += loss.item()        if i % 2000 == 1999: # 每2000批次打印一次            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))            running_loss = 0.0print(\"Finished Training\")\n\n在测试集上测试网络我们在整个训练集上进行了2次训练，但是我们需要检查网络是否从数据集中学习到有用的东西。 通过预测神经网络输出的类别标签与实际情况标签进行对比来进行检测。 如果预测正确，我们把该样本添加到正确预测列表。 第一步，显示测试集中的图片并熟悉图片内容。然后用\n1234567891011dataiter = iter(testloader)images, labels = dataiter.next()# 输出是10个标签的能量。 一个类别的能量越大，神经网络越认为它是这个类别。所以让我们得到最高能量的标签。outputs = net(images) _, predicted = torch.max(outputs, 1) # (maxvalues, indices) is returned# 显示图片imshow(torchvision.utils.make_grid(images))print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))print('Predicted:   ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))\n\n\n12GroundTruth:    cat  ship  ship planePredicted:     bird plane plane   dog\n\n接下来让看看网络在整个测试集上的结果如何。\n1234567891011correct = 0total = 0with torch.no_grad():    for data in testloader:        images, labels = data        outputs = net(images)        _, predicted = torch.max(outputs.data, 1)        total += labels.size(0)        correct += (predicted == labels).sum().item()print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n\n1Accuracy of the network on the 10000 test images: 49 %\n\n在识别哪一类的时候好，哪一类不好呢？\n123456789101112131415class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))with torch.no_grad():    for data in testloader:        images, labels = data        outputs = net(images)        _, predicted = torch.max(outputs, 1)        c = (predicted == labels).squeeze()        for i in range(4):            label = labels[i]            class_correct[label] += c[i].item()            class_total[label] += 1for i in range(10):    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n\n12345678910Accuracy of plane : 69 %Accuracy of   car : 69 %Accuracy of  bird : 15 %Accuracy of   cat : 43 %Accuracy of  deer : 44 %Accuracy of   dog : 22 %Accuracy of  frog : 72 %Accuracy of horse : 58 %Accuracy of  ship : 41 %Accuracy of truck : 58 %\n\n在GPU上训练把一个神经网络移动到GPU上训练就像把一个Tensor转换GPU上一样简单。并且这个操作会递归遍历有所模块，并将其参数和缓冲区转换为CUDA张量。\n1234567device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")# 确认我们的电脑支持CUDA，然后显示CUDA信息print(device)# 将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量net.to(device)\n\n记住: inputs 和 labels 也要转换。\n1inputs, labels = inputs.to(device), labels.to(device)\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-4/"},{"title":"[PyTorch学习笔记] Neural Networks","date":"2019-07-22T20:48:56.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] Neural Networks 的学习笔记\n使用torch.nn包来构建神经网络。\nnn包依赖autograd包来定义模型并求导。 一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。\n\n它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。\n神经网络的典型训练过程如下：\n\n定义包含一些可学习的参数(或者叫权重)神经网络模型；\n在数据集上迭代；\n通过神经网络处理输入；\n计算损失(输出结果和正确值的差值大小)；\n将梯度反向传播回网络的参数；\n更新网络的参数，主要使用如下简单的更新原则： weight = weight - learning_rate * gradient\n\n\n\n\n\n定义网络123456789101112131415161718192021222324252627282930313233343536373839404142import torchimport torch.nn as nnimport torch.nn.functional as F class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        # 1 input image channel        # 6 output image channel        # 5x5 square convolution kernel        self.conv1 = nn.Conv2d(1, 6, 5)        self.conv2 = nn.Conv2d(6, 16, 5)        # an affine operation: y = Wx + b        self.fc1 = nn.Linear(16 * 5 * 5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)    def forward(self, x):        # Max pooling over a (2, 2) window        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))        # if the size is a square you can only specify a single number        x = F.max_pool2d(F.relu(self.conv2(x)), 2)        x = x.view(-1, self.num_flat_features(x))        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        return x    def num_flat_features(self, x):        size = x.size()[1:]  # all dimensions except the batch dimension                num_features = 1        for s in size:            num_features *= s        return num_features    net = Net()print(net)\n\n1234567Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))\n\n在模型中必须要定义 forward 函数，backward 函数（用来计算梯度）会被autograd自动创建。 可以在 forward 函数中使用任何针对 Tensor 的操作。\nnet.parameters()返回可被学习的参数（权重）列表和值\n123params = list(net.parameters())print(len(params))print(params[0].size())  # params[0] == net.conv1.weight\n\n1210torch.Size([6, 1, 5, 5]\n\nnet.named_parameters可同时返回可学习的参数及名称。\n12for name,parameters in net.named_parameters():    print(name,':',parameters.size())\n\n12345678conv1.weight : torch.Size([6, 1, 5, 5])conv1.bias : torch.Size([6])conv2.weight : torch.Size([16, 6, 5, 5])conv2.bias : torch.Size([16])fc1.weight : torch.Size([120, 400])fc1.bias : torch.Size([120])fc2.weight : torch.Size([10, 120])fc2.bias : torch.Size([10])\n\n测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。\n123input = torch.randn(1, 1, 32, 32)out = net(input)print(out)\n\n123tensor([[ 0.1052, -0.0361,  0.1122,  0.1072,  0.0887,  0.0477, 0.0916, -0.0594,         -0.1450,  0.0574]], grad_fn=&lt;AddmmBackward&gt;)\n\n将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：\n12net.zero_grad()out.backward(torch.randn(1, 10))\n\n\ntorch.nn 只支持小批量输入。整个 torch.nn 包都只支持小批量样本，而不支持单个样本。 例如，nn.Conv2d 接受一个4维的张量， 每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）。 如果你有单个样本，只需使用 input.unsqueeze(0) 来添加其它的维数\n\n我们回顾一下到目前为止用到的类。\n回顾:\n\ntorch.Tensor：一个用过自动调用 backward()实现支持自动梯度计算的 多维数组 ， 并且保存关于这个向量的梯度\nnn.Module：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。\nnn.Parameter：一种变量，当把它赋值给一个Module时，被 自动地注册为一个参数。\nautograd.Function：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个Tensor的操作都h会创建一个接到创建Tensor和 编码其历史 的函数的Function节点。\n\n损失函数一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。\nnn包中有很多不同的损失函数。 nn.MSELoss是一个比较简单的损失函数，它计算输出和目标间的均方误差， 例如：\n12345678input = torch.randn(1, 1, 32, 32)output = net(input)target = torch.randn(10)  # 随机值作为样例target = target.view(1, -1)  # 使target和output的shape相同criterion = nn.MSELoss()loss = criterion(output, target)print(loss)\n\n1tensor(1.1509, grad_fn=&lt;MseLossBackward&gt;)\n\n常见损失函数nn.L1Loss\nloss(x, y) = \n\\begin{cases}\n\\frac1n \\sum \\left| x_i - y_i \\right|, &amp; \\text{if reduction='mean'} \\\\\n\\sum \\left| x_i - y_i \\right|, &amp; \\text{if reduction='sum'}\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n\n\n\n\n\n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\nnn.NLLLossNegative Log Liklihood(NLL) Loss f 负对数似然损失函数\n\nloss(x, class) = -x_{class}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n\n\n\n\nnn.MSELossMean Squrare Error(MSE) Loss 均方损失函数\n\nloss(x, y) = \\frac1n \\sum(x_i - y_i)^2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n \n \n\n \n \n\n \n\n \n \n\n\n \n \n\n\n\nnn.CrossEntropyLoss多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数,可以理解为CrossEntropyLoss()=log_softmax() + NLLLoss()\n\nloss(x, class) = -\\log \\frac{\\exp(x_{class})}{\\sum_j \\exp(x_j)} = - x_{class} + \\log \\left(\\sum_j \\exp(x_j) \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n\n\n\n\n\n \n \n \n \n\n \n\n \n \n \n \n \n\n\n \n\n\n \n \n\n \n \n \n\n \n\n \n \n\n \n\n\n\n \n \n\n \n\n \n \n \n \n \n\n\n \n\n \n \n \n\n\n \n\n \n \n\n\n \n \n \n\n \n\n \n \n\n \n \n\n\n\nnn.BCELossBinary Cross Entropy\n\nloss(x, t) = -\\frac1n \\sum_i \\left(t_i *\\log(x_i) + (1-t_i)*\\log(1-x_i) \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n\n\n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n \n \n \n \n\n \n \n\n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n \n\n\n\n反向传播调用loss.backward()获得反向传播的误差。\n但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。\n现在，我们将调用loss.backward()，并查看conv1层的偏差（bias）项在反向传播前后的梯度。\n123456789net.zero_grad()  # 清除梯度print('conv1.bias.grad before backward')print(net.conv1.bias.grad)loss.backward()print('conv1.bias.grad after backward')print(net.conv1.bias.grad)\n\n1234conv1.bias.grad before backwardNoneconv1.bias.grad after backwardtensor([ 0.0027, -0.0142,  0.0197,  0.0021, -0.0018,  0.0001])\n\n更新权重在实践中最简单的权重更新规则是随机梯度下降（SGD）：\n1weight = weight - learning_rate * gradient\n\n我们可以使用简单的Python代码实现这个规则：\n123learning_rate = 0.01for f in net.parameters():    f.data.sub_(f.grad.data * learning_rate)\n\n但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包torch.optim实现了所有的这些规则。 使用它们非常简单：\n123456789101112import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)criterion = nn.MSELoss()# in your training loopoptimizer.zero_grad()   # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()    # calculate gradientsoptimizer.step()   # Does the update\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-3/"},{"title":"[PyTorch学习笔记] AUTOGRAD: Automatic Differentiation","date":"2019-07-22T19:20:28.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] AUTOGRAD: Automatic Differentiation 的学习笔记\n\n\n\n\nAutograd: 自动求导机制PyTorch 中所有神经网络的核心是 autograd 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。\nautograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。\n张量 Tensortorch.Tensor是这个包的核心类。如果设置 .requires_grad 为 True，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 .backward()，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 .grad 属性。\n要阻止张量跟踪历史记录，可以调用.detach()方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。\n为了防止跟踪历史记录（和使用内存），可以将代码块包装在with torch.no_grad()：中。 在评估模型时特别有用，因为模型可能具有requires_grad = True的可训练参数，但是我们不需要梯度计算。\n在自动梯度计算中还有另外一个重要的类Function.\nTensor 和 Function互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个.grad_fn属性，这个属性引用了一个创建了Tensor的Function（除非这个张量是用户手动创建的，即，这个张量的 grad_fn 是 None）。\n如果需要计算导数，你可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数， 但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。\n\n在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在这里\n\n创建一个张量并设置 requires_grad=True 用来追踪他的计算历史\n1234567891011x = torch.ones(2, 2, requires_grad=True)print(x)y = x + 2  # 对x进行操作print(y)print(\"y.grad_fn: \", y.grad_fn)  # 结果y已经被计算出来了，所以，grad_fn已经被自动生成了z = y * y * 3  # 对y进行操作out = z.mean()print(z)print(out)\n\n12345678tensor([[1., 1.],        [1., 1.]], requires_grad=True)tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)y.grad_fn:  &lt;AddBackward0 object at 0x00000202022DAE48&gt;tensor([[27., 27.],        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)tensor(27., grad_fn=&lt;MeanBackward0&gt;)\n\n.requires_grad_( ... ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False。\n1234567a = torch.ones(2, 2)a = (a * 3) / (a - 1)print(a.requires_grad)a.requires_grad_(True)print(a.requires_grad)b = (a * a).sum()print(b.grad_fn)\n\n123FalseTrue&lt;SumBackward0 object at 0x0000028141CBADA0&gt;\n\n梯度反向传播 因为 out是一个纯量（scalar），out.backward() 等于out.backward(torch.tensor(1))。\n1out.backward()\n\nprint gradients \n\\frac{d(out)}{dx}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n \n \n\n\n\n\n1234567x = torch.ones(2, 2, requires_grad=True)y = x + 2  z = y * y * 3  out = z.mean()out.backward()print(x.grad)\n\n12tensor([[4.5000, 4.5000],        [4.5000, 4.5000]])\n\n可以用自动求导做更多操作\n1234567891011x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000:    y = y * 2print(y)gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(gradients)print(x.grad)\n\n12tensor([-790.8533,  793.1236,  307.1018], grad_fn=&lt;MulBackward0&gt;)tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n\n如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:\n123456print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad():    print(x.requires_grad)    print((x ** 2).requires_grad)\n\n1234TrueTrueTrueFalse\n\n\n更多阅读： autograd 和 Function 的官方文档\n\n拓展Autograd如果需要自定义autograd扩展新的功能，就需要扩展Function类。因为Function使用autograd来计算结果和梯度，并对操作历史进行编码。 在Function类中最主要的方法就是forward()和backward()他们分别代表了前向传播和反向传播。\n一个自定义的Function需要一下三个方法：\n12345__init__ (optional)：如果这个操作需要额外的参数则需要定义这个Function的构造函数，不需要的话可以忽略。forward()：执行前向传播的计算代码backward()：反向传播时梯度计算的代码。 参数的个数和forward返回值的个数一样，每个参数代表传回到此操作的梯度。\n\n12345678910111213141516171819# 引入Function便于扩展from torch.autograd.function import Function# 定义一个乘以常数的操作(输入参数是张量)# 方法必须是静态方法，所以要加上@staticmethod class MulConstant(Function):        @staticmethod     def forward(ctx, tensor, constant):        # ctx 用来保存信息这里类似self，并且ctx的属性可以在backward中调用        ctx.constant=constant        return tensor *constant        @staticmethod    def backward(ctx, grad_output):        # 返回的参数要与输入的参数一样.        # 第一个输入为3x3的张量，第二个为一个常数        # 常数的梯度必须是 None.        return grad_output, None\n\n定义完我们的新操作后，我们来进行测试\n1234a=torch.rand(3,3,requires_grad=True)b=MulConstant.apply(a,5)print(\"a:\"+str(a))print(\"b:\"+str(b)) # b为a的元素乘以5\n\n反向传播，返回值不是标量，所以backward方法需要参数\n1b.backward(torch.ones_like(a))\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-2/"},{"title":"[PyTorch学习笔记] What is PyTorch?","date":"2019-07-22T18:11:52.000Z","updated":"2022-01-04T08:35:48.627Z","content":"本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] What is PyTorch? 的学习笔记\n\n\n\n\n\n\nPyTorch 是什么?基于Python的科学计算包，服务于以下两种场景:\n\n作为NumPy的替代品，可以使用GPU的强大计算能力\n提供最大的灵活性和高速的深度学习研究平台\n\nTensor 张量Tensors与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.\n12from __future__ import print_functionimport torch\n\n创建一个  5x5 的矩阵，但未初始化:\n12x = torch.empty(5, 3)print(x)\n\n12345tensor([[9.5511e-39, 1.0102e-38, 4.6837e-39],        [4.9592e-39, 5.0510e-39, 9.9184e-39],        [9.0000e-39, 1.0561e-38, 1.0653e-38],        [4.1327e-39, 8.9082e-39, 9.8265e-39],        [9.4592e-39, 1.0561e-38, 1.0653e-38]])\n\n创建一个随机初始化的矩阵:\n12x = torch.rand(5, 3)print(x)\n\n12345tensor([[0.6004, 0.9095, 0.5525],        [0.2870, 0.2680, 0.1937],        [0.9153, 0.0150, 0.5165],        [0.7875, 0.7397, 0.9305],        [0.8575, 0.1453, 0.2655]])\n\n创建一个0填充的矩阵，数据类型为long:\n12x = torch.zeros(5, 3, dtype=torch.long)print(x)\n\n12345tensor([[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]])\n\n创建tensor并使用现有数据初始化:\n12x = torch.tensor([5.5, 3])print(x)\n\n1tensor([5.5000, 3.0000])\n\n根据现有的张量创建张量。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖\n12345678x = torch.tensor([5.5, 3])print(x)x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象print(x)x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化\n\n1234567891011tensor([5.5000, 3.0000])tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]], dtype=torch.float64)tensor([[-0.5648,  1.4639, -0.1247],        [ 0.4187,  0.0255, -0.0938],        [-1.2237,  0.3889,  0.9847],        [-0.2423, -3.3706, -0.3511],        [-1.1498, -1.1044,  0.4582]])\n\n获取 size使用size方法与Numpy的shape属性返回的相同，张量也支持shape属性\n12345x = torch.ones(5, 3)print(x)print(\"x.size(): \", x.size())print(\"x.shape:  \", x.shape)\n\n1234567tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]])x.size():  torch.Size([5, 3])x.shape:   torch.Size([5, 3])\n\n\n torch.Size()返回值是tuple类型，所以它支持tuple类型的所有操作\n\n基础类型基本类型Tensor的基本数据类型有五种：\n\n32位浮点型：torch.FloatTensor。 (默认)\n64位整型：torch.LongTensor。\n32位整型：torch.IntTensor。\n16位整型：torch.ShortTensor。\n64位浮点型：torch.DoubleTensor。\n\n除以上数字类型外，还有 byte和chart型\n123456789tensor = torch.tensor([3.1433223])  # tensor([3.1433])long = tensor.long()  # tensor([3])half = tensor.half()  # tensor([3.1426], dtype=torch.float16)int_t = tensor.int()  # tensor([3], dtype=torch.int32)flo = tensor.float()  # tensor([3.1433])short = tensor.short()  # tensor([3], dtype=torch.int16)ch = tensor.char()  # tensor([3], dtype=torch.int8)bt = tensor.byte()  # tensor([3], dtype=torch.uint8)\n\nOperation 操作加法123456x = torch.rand(5, 3)y = torch.rand(5, 3)sum = x + y                 # 加法1，操作符sum = torch.add(x, y)       # 加法2，函数torch.add(x, y, out=sum)    # 加法3，提供输出张量sum作为参数\n\n替换12# add x to yy.add_(x)\n\n\n任何 以_ 结尾的操作都会用结果替换原变量. 例如: x.copy_(y), x.t_(), 都会改变 x.\n\n截取1print(x[:, 1])\n\nview / reshapetorch.view 可以改变张量的维度和大小，与numpy的reshape类似\n1234x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)  #  size -1 从其他维度推断print(x.size(), y.size(), z.size())\n\n1torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n\n只有一个元素的张量取值如果你有只有一个元素的张量，使用.item()来得到Python数据类型的数值\n123x = torch.randn(1)print(x)print(x.item())\n\n12tensor([-0.2036])-0.203627809882164\n\n\n更多操作点击此处\n\nNumpy 转换Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。\nTorch Tensor 转换成 NumPy数组123456789a = torch.ones(5)print(a)b = a.numpy()print(b)a.add_(1)print(a)print(b)\n\n1234tensor([1., 1., 1., 1., 1.])[1. 1. 1. 1. 1.]tensor([2., 2., 2., 2., 2.])[2. 2. 2. 2. 2.]\n\nNumPy数组 转换成 Torch Tensor12345678a = np.ones(5)b = torch.from_numpy(a)print(a)print(b)np.add(a, 1, out=a)print(a)print(b)\n\n1234[1. 1. 1. 1. 1.]tensor([1., 1., 1., 1., 1.], dtype=torch.float64)[2. 2. 2. 2. 2.]tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n\n\n所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换.\n\nCUDA 张量使用.to 方法 可以将Tensor移动到任何设备中\n1234567891011x = torch.rand(1)# is_available 函数判断是否有cuda可以使用# torch.device 将张量移动到指定的设备中if torch.cuda.is_available():    device = torch.device(\"cuda\")          # a CUDA 设备对象    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量    x = x.to(device)                       # 或者直接使用 .to(\"cuda\") 将张量移动到cuda中    z = x + y    print(z)    print(z.to(\"cpu\", torch.double))       # .to 也会对变量的类型做更改\n\n12tensor([1.2840], device='cuda:0')tensor([1.2840], dtype=torch.float64)\n\n一般情况下可以使用.cuda方法和 .cpu 方法将tensor在cpu和gpu之间移动 （需要cuda设备支持）\n12345678cpu_a = torch.rand(4, 3)cpu_a.type()  # torch.FloatTensorgpu_a = cpu_a.cuda()gpu_a.type()  # torch.cuda.FloatTensorcpu_b = gpu_a.cpu()cpu_b.type()  # torch.FloatTensor\n\n创建变量与结果变量.is_leaf：记录是否是叶子节点。通过这个属性来确定这个变量的类型 在官方文档中所说的“graph leaves”,“leaf variables”，都是指像x,y这样的手动创建的、而非运算得到的变量，这些变量成为创建变量。 像z这样的，是通过计算后得到的结果称为结果变量。\n12print(\"x.is_leaf=\"+str(x.is_leaf))print(\"z.is_leaf=\"+str(z.is_leaf))\n\n12x.is_leaf=Truez.is_leaf=False\n\n","thumbnail":"/static/image/pytorch.png","plink":"https://yuxinzhao.net/pytorch-learning-note-1/"},{"title":"Windows PATH 长度限制最多2047个字符","date":"2019-07-20T09:53:59.000Z","updated":"2022-01-04T08:35:48.659Z","content":"\n最近使用windows安装一些软件的过程中，出现了环境变量PATH最长限制2047个字符的问题。所幸我习惯下载软件的zip包，自己解压后再将其添加到环境变量中，因此才能及时发现这个问题。用安装包安装可能还不会提示这个问题，出情况也就比较难去定位问题了。\n\n\nProblem 问题\n环境变量PATH (或者是Path，windows的环境变量不区分大小写) 最大长度为2047个字符，要添加新的位置到环境变量中就超出了限制。\n\nSolution 解决方法用一个新的环境变量PATH2来拓展PATH，具体步骤如下：\n\n打开 计算机 &gt;&gt; 属性 &gt;&gt; 高级系统设置 &gt;&gt; 环境变量\n在系统变量中找到 PATH (不区分大小写)， 将其名称改成 PATH2。\n新建一环境变量 PATH，在其中添加一个值%PATH2% 用于将 PATH2 包含进来。\n\n完成。\n由于新的 PATH 的只有 %PATH2% 一个值，后续要添加到环境变量中的位置都可以添加到 PATH 中，相当于扩容了 PATH，而 PATH2 是透明的，对以后的操作并不会有什么影响。\n","plink":"https://yuxinzhao.net/windows-path-limited-length/"},{"title":"[论文笔记] R-C3D: Region Convolutional 3D Network for Temporal Activity Detection","date":"2019-07-17T18:44:03.000Z","updated":"2022-01-04T08:35:48.635Z","content":"\n作者提出了R-C3D模型用于连续视频的行为检测(Activity Detection in Continuous Videos)。连续视频的行为检测需要完成两个目标：\n\n识别出行为的类别\n定位行为发生的时间范围\n\n这两个问题正是R-C3D着力解决的。\n\n\n\n\n术语缩写\n\n\n缩写\n全称\n\n\n\nR-C3D\nRegion Convolutional 3D Network\n\n\nRoI\nRegion of Interest\n\n\nRPN\nRegion Proposal Network\n\n\nR-C3D 的特点\n端到端的训练方式；\n可以检测出任意时长的行为；\n检测速度快，一次性能计算的帧仅受限于GPU内存；\n推广Faster-RCNN的Region Proposal Network到时域；\n推广Faster-RCNN的RoI Pooling算法到时域，提出3D RoI Pooling。\n有监督学习(特别之处是允许一个视频中包含多种行为，且行为的时间范围有重叠)\n\nR-C3D 的网络结构\n\n\n 上图为本人理解后绘制的网络结构图，如有错误，欢迎批评指正。\n\n作者提出的 R-C3D 网络包含3个组件：\n\na shared 3D ConvNet feature extractor\na temporal proposal stage\nan activity classification and refinement stage\n\n3D Convolutional Feature HierarchiesInput: sequence of RGB video frames with dimension \n\\mathbb{R}^{3 \\times L \\times H \\times W}\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n\n\n. \n\nThe input to the model is of variable length (\nL\n\n\n\n\n \n\n can be arbitrary and is only limited by memory)\nAdopt the convolutional layers (conv1a to conv5b) of C3D\n本例中，\nH=W=112\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n\n\n\nOutput: a feature map \nC_{conv5b} \\in \\mathbb{R}^{512 \\times \\frac{L}8 \\times \\frac{H}{16} \\times \\frac{W}{16} }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n\n \n\n \n\n \n \n \n \n\n\n\n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n\n\n\n\n\n\n (512 is the channel dimension of the layer conv5b) (activations)\n\n\nC_{conv5b}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n\n\n activations are the shared input to the proposal and classification subnets.\n\nTemporal Proposal Subnet\nFunction: predicts potential proposal segments with respect to anchor segments and a binary label indicating whether the predicted proposal contains an activity or not.\n\nThe anchor segments are pre-defined multi-scale windows centered at \nL/8\n\n\n\n\n\n\n \n \n \n\n uniformly distributed temporal locations.\nEach temporal locaiton specifies \nK\n\n\n\n\n \n\n anchor segments, each at a different fixed scale. Thus, the total number of anchor segments is \n(L/8) * K\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n .\na 3D convolutional filter with kernel size \n3 \\times 3 \\times 3\n\n\n\n\n\n \n \n \n \n \n\n on top of \nC_{conv5b}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n\n\n to extend the temporal proposal subnet.\na 3D max-pooling filter with kernel size \n1 \\times \\frac{H}{16} \\times \\frac{W}{16}\n\n\n\n\n\n\n\n\n \n \n\n\n\n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n\n\n\n\n to downsample the spatial dimensions (from \n\\frac{H}{16} \\times \\frac{W}{16}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n\n to \n1 \\times 1\n\n\n\n\n\n \n \n \n\n) to a temporal only feature map \nC_{tpn} \\in \\mathbb{R}^{512 \\times \\frac{L}8 \\times 1 \\times 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n\n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n\n\n. \nThe 512-dimensional feature vector at each temporal location (\n512 \\times 1 \\times 1 \\times 1\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n) in \nC_{tpn}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is used to predict :\na relative offset \n\\{ \\delta c_i, \\delta l_i \\}\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n\n to the center location\nthe length of each anchor segment \n\\{ c_i, l_i \\},\\, i \\in \\{ 1, \\cdots, K \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \nthe binary scores for each proposal being an activity or background\n\n\ntwo \n1 \\times 1 \\times 1\n\n\n\n\n\n \n \n \n \n \n\n convolutional layers on top of \nC_{tpn}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n to predict proposal offsets and scores.\n\nTraining Temporal Proposal Subnet\nPositive Label:\nif the anchor segment overlaps with some ground-truth activity with IoU &gt; 0.7.\nor if the anchor segment has the highest IoU overlap with some ground-truth activity.\n\n\nNegative Label:\nif the anchor has IoU overlap lower than 0.3 with all ground-truth activities\n\n\nAll others are held out from training\nsample balanced batches with a positive/negative ratio of 1:1\n\nActivity Classification Subnet\nFunctions:\n\nselecting proposal segments from the previous stage.\n\nthree-dimensional region of interest (3D RoI) pooling to extract fixed-size features for selected proposals.\n\nactivity classification and boundary regression for the selected proposals based on the pooled features.\n\ngreedy Non-Maximum Suppression (NMS, threshold=0.7) to eliminate highly overlapping and low confidence proposals.\n\n\n3D RoI Pooling\n对于一个 \nl \\times h \\times w\n\n\n\n\n\n\n\n \n \n \n \n \n\n 的不定输入张量（每次输入的张量的尺寸可以不一样），3D RoI Pooling 将其规约到固定的大小 \nl_s \\times h_s \\times w_s\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n .\n\nl \\times h \\times w\n\n\n\n\n\n\n\n \n \n \n \n \n\n 先被分成 \nl_s \\times h_s \\times w_s\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n 个大小约为 \n\\frac{l}{l_s} \\times \\frac{h}{h_s} \\times \\frac{w}{w_s}\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n \n\n\n\n \n\n \n \n\n\n\n\n 的小张量，每个张量内做max pooling 得到 \nl_s \\times h_s \\times w_s\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n\n 的张量。\n\n\n在本例中，由于\nH=W=112\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n\n，\nC_{conv5b} \\in \\mathbb{R}^{512 \\times \\frac{L}8 \\times 7 \\times 7}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n\n \n\n \n\n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n\n\n，将 \n512 \\times \\frac{L}8 \\times 7 \\times 7\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n \n \n\n\n \n \n \n \n\n  的张量固定为 \n512 \\times 1 \\times 4 \\times 4\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n.\nThe output of the 3D RoI pooling is fed to a series of two fully connected layers.\n3D RoI pooling 的两层 fully connected layers 之后是 classification layer 和 regression layer.\nclassification layer 和 regression layer 是两个独立的双层 fully connected layers, 它们的输入都是 3D RoI pooling 后 fully connected layers 的输出。\n\n\n\nTraining Classification Subnet\nPositive Label:\nif the proposal has the highest IoU overlap with a ground-truth activity and IoU &gt; 0.5.\n\n\nNegative Label:\nproposals with IoU overlap lower than 0.5 with all grouth-truth activities.\n\n\nPositive : Negative = 1 : 3\n\nOptimizationLoss Function:\n\nLoss = \\frac1{N_{cls}} \\sum_i L_{cls}(a_i, a_i^*) + \\lambda \\frac1{N_{reg}} a_i^* L_{reg}(t_i, t_i^*)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n \n\n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n\nwhere, \n\n\nL_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is softmax loss function\n\nL_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is smooth L1 loss function \n\nN_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is batch size\n\nN_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is the number of anchor/proposal segments\n\n\\lambda\n\n\n\n\n \n\n is the loss trade-off parameter and is set to 1. \n\ni\n\n\n\n\n \n\n is the anchor/proposal segments index in a batch\n\na_i\n\n\n\n\n\n \n \n\n is the predicted probability of the proposal or activities\n\na_i^*\n\n\n\n\n\n\n \n \n \n\n is the ground truth\n\nt_i = \\{ \\delta \\hat{ c_i} , \\delta \\hat{ l_i} \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n\n \n \n\n \n \n \n\n \n\n represents predicted relative offset to anchor segments or proposals.\n\nt_i^* = \\{ \\delta c_i , \\delta l_i \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n represents the coordinate transformation of ground truth segments to anchor segments or proposals.\n\n\n\\begin{cases}\n\\delta c_i = (c_i^* - c_i) / l_i \\\\\n\\delta l_i = \\log (l_i^* / l_i)\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n \n\n \n \n\n \n \n \n\n \n\n \n \n\n \n \n\n \n \n\n\n\n \n\n \n \n\n \n\n \n \n \n\n \n\n \n \n \n\n \n\n \n \n\n \n\n\n\n\n\nwhere \nc_i\n\n\n\n\n\n \n \n\n and \nl_i\n\n\n\n\n\n \n \n\n are the center location and the length of anchor segments or proposals while \nc_i^*\n\n\n\n\n\n\n \n \n \n\n and \nl_i^*\n\n\n\n\n\n\n \n \n \n\n  denote the same for the ground truth activity segment.\n\nThe above loss function is applied for both the temporal proposal subnet and the activity classification subnet.\nIn proposal subnet, \n\nL_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n predicts whether the proposal contains an activity or not.\n\nL_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n optimizes the relative displacement between proposals and ground truths.  \n\n\nIn classification subnet,\n\nL_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n predicts the specific activity class for the proposal. (The number of classes are the number of activities + one for background)\n\nL_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n optimizes the relative displacement between activities and ground truths. \n\n\n\nPrediction\n预测出\nt_i\n\n\n\n\n\n \n \n\n后需要逆变换把相对坐标变成绝对坐标。\n为了充分利用向量化实现的优势，用最后一帧去填充视频较短不足的部分。\nNMS at a lower threshold (0.1 less than the mAP evalution threshold) is appled to the predicted activities to get the final activity predictions.\n\nExperimentsExperiments on THUMOS’14\ndivide 200 untrimmed videos from the validation set into 180 training and 20 held out videos to get the best hyperparameter setting.\nSince the GPU memory is limited, the authors first create a buffer of 768 frames at 25 fps which means approximately 30 seconds of video.\nThe authors create the buffer by sliding from the beginning of the video to the end, denoted as the “one-way buffer”. An additional pass from the end of the video to the beginning is used to increase the amount of training data, denoted as “two-way buffer”.\ninitialize the 3D ConvNet part of our model with C3D weights trained on Sports-1M and finetuned on UCF101.\nallow all the layers of R-C3D to be trained on THUMOS’14 with a fixed learning rate of 0.0001.\nK = 10, with scale values [2, 4, 5, 6, 8, 9, 10, 12, 14, 16]\n\nExperiments on ActivityNet\nsample frames at 3 fps\ninput buffer: 768\nK=20, with scale values [1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, 28, 32, 40, 48, 56, 64]\n\nExperiments on Charades\nsample frames at 5 fps\ninput buffer: 768\nK=18, with scale values [1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, 28, 32, 40, 48]\n\n","plink":"https://yuxinzhao.net/r-c3d/"},{"title":"NMS / soft-NMS / softer-NMS","date":"2019-07-17T14:12:08.000Z","updated":"2022-01-04T08:35:48.603Z","content":"目标检测算法包含了三个要素：Backbone + Head + Postprocess，对于Postprocess部分，最早用的是NMS，后面出现了Soft NMS和Softer NMS，本文将分别解释它们的动机和原理。\n\n\n\nNMSNMS，它的全称为“non-maximum supression”，中文名“非极大值抑制”。为什么要使用NMS呢？因为在目标检测任务中，不管是one-stage还是two-stage的算法，最终算法都会预测出多个proposals。在后处理部分中，需要对这些proposals做筛选。\n动机\n优先选择分类score较高的proposal；\n跟分类score重叠较多的proposals，可以视为冗余的预测框；\n\n步骤\n将算法预测出的所有proposals，按照不同的类别标签分组；\n对于每一个类别的所有proposals，记作\nB\n\n\n\n\n \n\n，筛选后的proposals集合记作\nD\n\n\n\n\n \n\n，执行如下操作，\na. 选择score最高的proposal，记作\nM\n\n\n\n\n \n\n，加入到\nM\n\n\n\n\n \n\n中；\nb. 计算剩余的proposals与\nM\n\n\n\n\n \n\n之间的IoU，若大于阈值\nN_t\n\n\n\n\n\n \n \n\n ，则舍弃，否则保留；\nc. 若步骤b中得到的所有proposals为空，则跳回步骤b，否则执行步骤a。\n\n\n经过后处理之后，所有类别保留的有效proposals集合为 \nS=\\{D_1, D_2, …, D_c\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n ，其中\nc\n\n\n\n\n \n\n表示目标类别的数量；\n\n伪代码\nSoft-NMS动机由上可见，NMS算法保留score最高的预测框，并将与当前预测框重叠较多的proposals视作冗余，显然，在实际的检测任务中，这种思路有明显的缺点，比如对于稠密物体检测，当同类的两个目标距离较近时，如果使用原生的NMS，就会导致其中一个目标不能被召回，为了提高这种情况下目标检测的召回率，Soft-NMS应运而生。对于Faster-RCNN在MS-COCO数据集上的结果，将NMS改成Soft-NMS，mAP提升了1.1%。\n算法思想Soft-NMS，原文的标题为“Improving Object Detection With One Line of Code”。NMS采用“一刀切”的思想，将重叠较多的proposals全部视作冗余，而Soft-NMS，采用了“迂回”战术，它认为重叠较多的proposals也有可能包含有效目标，只不过重叠区域越大可能性越小。参见下图，NMS会将绿色框的score置0，而Soft-NMS会将绿色框的score由0.8下降到0.4，显然Soft-NMS更加合理。\n\n那么问题来了，怎么建立IoU和score之间的联系呢，文章中给出的公式如下:\n\ns_i = s_i e^{-\\frac{\\text{iou}(M, b_i)^2}{\\sigma}}, \\forall b_i \\notin D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n\n \n\n \n\n\n\n\n \n \n \n \n \n \n\n \n \n\n\n \n \n\n\n \n\n\n\n\n \n \n\n \n \n\n \n \n\n\n其中\nD\n\n\n\n\n \n\n表示所有保留的有效框集合, \nb_i\n\n\n\n\n\n \n \n\n表示待过滤的第\ni\n\n\n\n\n \n\n个预测框，\ns_i\n\n\n\n\n\n \n \n\n为第\ni\n\n\n\n\n \n\n个预测框对应的分类score。这里使用了高斯函数作为惩罚项，当\niou=0\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n时，分类score不变，当\n0&lt;iou&lt;1\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n时，分类score会做衰减。以上图为例，绿色框\nb_i\n\n\n\n\n\n \n \n\n和红色框\nM\n\n\n\n\n \n\n的iou大于0，经过Soft-NMS后该绿色框的分类score由0.8衰减到0.4，可以推断出，如果图中有第2个绿色框，且其与红色框的重叠区域更大时，那么这个新的绿色框的分类score可能由0.8衰减到0.01。\n步骤伪代码\nSofter-NMS动机现有方法的问题作者使用VGG-16 faster R-CNN测试了MS-COCO数据集中的图片，论文中贴了两张检测失败的代表图片，如下图\n\n左图存在的问题：检测出来的2个proposals，沿着y坐标轴方向的定位均不准确；\n结论：检测算法预测出来的proposals的坐标不一定准确；\n右图存在的问题：检测出来的2个proposals，右边的框分类score较高，但是却沿着x坐标轴方向的定位不准确；\n结论：分类score高不一定定位score高，也即classification confidence和 localization confidence不具有一致性。\n本文解决方法针对上面的问题，\n\n既然proposals的坐标不准确，那么即便NMS也无能为力了，所以需要重新设计坐标回归的方式)；\n既然分类score高不一定定位score高，那么NMS和Soft-NMS的做法（只基于分类score对proposals做排序）是不准确的，所以需要同时预测出检测框的定位score。\n\n算法Softer-NMS的算法框架如下图，可以看出，它跟fast R-CNN是非常相似的，区别在于回归任务中多了一个Box std分支，这里需要解释一下，比如预测出的bounding box的坐标为\nx_1, y_1, x_2, y_2\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n\n \n \n\n\n，该分支会预测出每个坐标的标准差，显然，当坐标的标准差越小时，表明预测得到的坐标值越可信，也即Box std分支用于表征定位任务的置信度。\nfast R-CNN:\nfaster-NMS:\n\n定位任务在fast R-CNN中，作者使用的是均方误差函数作为定位损失，总的目的是让定位出的坐标点尽可能逼近groundtruth box。本文中为了在定位坐标同时输出定位score，使用了高斯函数建模坐标点的位置分布，公式如下，\n\nP_\\Theta(x) = \\frac1{2\\pi \\sigma^2} e^{-\\frac{(x-x_e)^2}{2\\sigma^2}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n\n\n \n\n \n \n\n \n \n\n\n\n\n\n \n\n \n\n\n\n\n \n \n \n\n \n \n\n\n \n \n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n其中，\nx_e\n\n\n\n\n\n \n \n\n为预测的box位置，\n\\sigma\n\n\n\n\n \n\n表示box位置的标准差，衡量了box位置的不确定性。因为groundtruth位置是确定的，所以groundtruth box的坐标为标准差为0的高斯分布，也即Dirac delta函数，公式如下，\n\nP_D(x) = \\delta (x-x_g)\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n\n \n\n\n其中，\nx_g\n\n\n\n\n\n \n \n\n 为groundtruth box的坐标。\n定位损失回归任务的目的是让预测框尽可能逼近真实框，也即\nP_\\theta(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n和\nP_D(x)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n为同一分布，衡量概率分布的相似性，自然而然会想到KL散度，关于KL散度的概念，大家可以参见维基百科，值得一提的是，KL散度本身具有不对称性，通常，在实际应用中为了使用对称性，使用的是KL散度的变形形式，但本文中没有这么做。对公式做化简后，最终的简化形式如下，\n\nL_{reg} = \\alpha \\left( \\left( x_g - x_e \\right) - \\frac12 \\right) - \\frac12 \\log \\left( \\alpha + \\epsilon \\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n\n \n\n \n\n \n \n\n \n\n \n \n\n \n\n \n\n\n\n \n \n\n\n \n\n \n\n\n\n \n \n\n\n\n \n \n \n\n\n \n \n \n \n \n\n\n\n后处理经过上面的网络部分，Class分支会输出类别score，Box分支会输出box的4个坐标和这4个坐标对应的标准差（定位score），符号表示如下，\n\n\\{ x1_i, y1_i, x2_i, y2_i, s_i, \\sigma_{x1, i}, \\sigma_{y1, i}, \\sigma_{x2, i}, \\sigma_{y2, i} \\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n \n \n\n \n\n \n \n\n \n\n \n\n \n \n \n \n\n\n \n\n \n\n \n \n \n \n\n\n \n\n \n\n \n \n \n \n\n\n \n\n \n\n \n \n \n \n\n\n \n\n\n基于这些信息，新的后处理算法如下图，\n\n显然，softer-NMS基于回归出的定位confidence，对所有与\nM\n\n\n\n\n \n\n的IoU超过阈值\nN_t\n\n\n\n\n\n \n \n\n的proposals，使用加权平均更新其位置坐标，从而达到提高定位精度的目的。因为softer-NMS关注的是单个框的定位精度，而NMS和soft-NMS关注的是单个框的冗余性，显然关注点不同，所以softer-NMS可以和soft-NMS组合使用，此时效果更佳。\n总结NMS：只适用于图片中目标比较稀疏的场景，即目标之间的间距较大；\nsoft-NMS：可以部分解决出现稠密目标的情况；\nsofter-NMS：该后处理方法采用”bagging”的思想，通过后处理提高定位精度，可以和soft-NMS组合使用。\n参考资料[1] 目标检测后处理：从nms到softer nms\n[2] Improving Object Detection With One Line of Code\n[3] Bounding Box Regression with Uncertainty for Accurate Object Detection\n","plink":"https://yuxinzhao.net/nms/"},{"title":"PR/ROC曲线及其相关性能评价指标","date":"2019-07-15T21:05:58.000Z","updated":"2022-01-04T08:35:48.623Z","content":"在分类模型的评价标准中，PR曲线和ROC曲线被广泛应用于模型的性能评估。本文对PR曲线和ROC曲线及其相关的性能指标AUC, EER, AP, mAP, F1-measure进行介绍。\n\n\n\n    \n        \n        Truth\n         \n\\sum\n\n\n\n\n \n\n \n    \n    \n        1\n        0\n    \n    \n         Estimate \n         1\n         TP \n         FP \n         TP+FP \n    \n    \n         0 \n         FN \n         TN \n         FN+TN \n    \n    \n         \n\\sum\n\n\n\n\n \n\n \n         TP+FN \n         FP+TN \n         TP+TN+FP+FN \n    \n\n\n\n\n\n\n真正例 (True Positive, TP): 预测值和真实值都为1\n假正例 (False Positive, FP): 预测值为1，真实值都为0\n真反例 (True Negative, TN): 预测值和真实值都为0\n假反例 (False Negative, FN): 预测值为0，真实值都为1\n\n由这四个指标衍生出的指标：\n\n查准率/准确率:                       \n \\text{Precision} = \\frac{TP}{TP+FP} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n查全率/召回率:                       \n\\text{Recall} = \\frac{TP}{TP+FN}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n真阳率(True Positive Rate): \n\\text{TPR} = \\frac{TP}{TP+FN}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n假阳率(False Positive Rate): \n\\text{FPR} = \\frac{FP}{FP+TN}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n\n\n \n \n \n \n \n\n\n\n\n\n\nROC曲线ROC定义ROC曲线(Receiver Operating Characteristic Curve, 受试者工作特征曲线)是比较分类模型好坏的可视化工具。\n以FPR为x轴，TPR为y轴绘制图。如下图所示。\n\nROC曲线的衍生指标\nEER(equal error rate): TPR=FPR时的值。\nAUC(area under curve): ROC曲线下的面积\n\nPR曲线PR定义PR曲线中P是Precision, R是Recall。\n以Recall为x轴，Precision为y轴。\n\n\n假设一次Object Detection的结果为：（对于目标检测任务，当预测框与真实框IoU大于一定阈值时标记为TP；当预测框与真实框IoU小于一定阈值时标记为FP；一个真实框没有一个任何预测框与其重叠的为FN）\n对其confidence进行排序: \n根据上表的顺序绘制PR曲线： \n\n至此我们得到了一张PR图。\nPR图的衍生指标\nAP (Average Precision) \nmAP (mean Average Precision)\nF1-measure 综合评价指标\n\n下面给出AP, mAP和F-measure的计算方法\n计算APAP 是针对某一类别进行计算的。\n下面我们先从图的角度来理解AP。\n在2010年前，AP的计算方法是用11点插值法(11-point interpolation)：\n分别取 recall = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] 十一个点插值，对每一个插值recall取 recall’ &gt;= recall 的点中precision最大的值作为该插值recall对应的precision。\n计算公式为:\n\nP_{\\text{interpolation}} (r) = \\max_{r' \\ge r}\\left(P(r')\\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n \n\n \n \n \n \n\n\n\n \n \n \n\n \n \n\n \n \n\n\n\n\n计算这11个插值recall对应precision的均值即是AP。\n\n\\begin{align}\nAP &amp;= \\frac1{11} \\sum_{r \\in \\{0,0.1,...,1\\}} P_{\\text{interpolation}} (r) \\\\\n&amp;= \\frac1{11} \\left( 1 + 0.6666 + 0.4285 + 0.4285 + 0.4285 + 0 + 0 + 0 + 0 + 0 + 0  \\right) \\\\\n&amp;= 26.84\\%\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n\n\n\n \n\n \n \n\n\n\n\n \n\n \n \n \n \n \n\n \n \n \n\n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n\n\n \n\n\n\n \n\n \n \n\n\n\n\n \n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n\n \n\n\n\n\n\n2010年后，AP的计算方法不再使用11点插值法，而是考虑所有的点: \n对所有Recall值，将Recall大于等于该Recall值的所有点中的最大precision作为该recall值对应的precision。公式仍是\n\nP_{\\text{interpolation}} (r) = \\max_{r' \\ge r}\\left(P(r')\\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n\n \n \n \n\n \n \n \n \n\n\n\n \n \n \n\n \n \n\n \n \n\n\n\n不同点在于AP的计算公式:\n\nAP = \\int_0^1 P_{\\text{interpolation}}(r) \\, dr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n \n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n\n\n可以画出图帮助理解：\n\n\n按照上面的计算公式，则AP为：\n\n\\begin{align}\nA1 &amp;= (0.0666 - 0) \\times 1 = 0.0666 \\\\\nA2 &amp;= (0.1333-0.0666) \\times 0.6666 = 0.04446222 \\\\\nA3 &amp;= (0.4-0.1333) \\times 0.4285 = 0.11428095 \\\\\nA4 &amp;= (0.4666 - 0.4) \\times 0.3043 = 0.02026638 \\\\\n\\\\\nAP &amp;= A1+A2+A3+A4 \\\\\n&amp;= 0.0666 + 0.04446222 + 0.11428095 + 0.02026638 \\\\\n&amp;= 0.24560955 \\\\\n&amp;= 24.56\\%\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n \n \n\n\n\n\n \n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n\n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n \n \n\n \n\n\n\n\n\n以后的AP计算我会以2010年后的版本为准。\n接下来我将演示用表格来计算AP，而不使用绘图的方式：\n按Confidence置信度来降序做出表格:\n\n    \n        Detection\n        Precision\n        Recall\n        Max Precision for Any Recall \nr' \\ge r\n\n\n\n\n\n\n \n \n \n \n\n\n        Average Precision\n    \n    \n        R\n        1\n        0.0666\n        1\n        24.56%\n    \n    \n        Y\n        0.5\n    \n    \n        J\n        0.6666\n        0.1333\n        0.6666\n    \n    \n        A\n        0.5\n    \n    \n        U\n        0.4\n    \n    \n        C\n        0.3333\n    \n    \n        M\n        0.2857\n    \n    \n        F\n        0.25\n    \n    \n        D\n        0.2222\n    \n    \n        B\n        0.3\n        0.2\n        0.4285\n    \n    \n        H\n        0.2727\n    \n    \n        P\n        0.3333\n        0.2666\n    \n    \n        E\n        0.3846\n        0.3333\n    \n    \n        X\n        0.4285\n        0.4\n    \n    \n        N\n        0.4\n    \n    \n        T\n        0.375\n    \n    \n        K\n        0.3529\n    \n    \n        Q\n        0.3333\n    \n    \n        V\n        0.3157\n    \n    \n        I\n        0.3\n    \n    \n        L\n        0.2857\n    \n    \n        S\n        0.2727\n    \n    \n        G\n        0.3043\n        0.4666\n        0.3043\n    \n    \n        O\n        0.2916\n    \n\n\n计算方法和原理同上，但画出表格可以直接计算AP:\n\n\\begin{align}\nAP &amp;= 1 \\times 0.0666 + 0.6666 \\times (0.1333-0.0666) + 0.4285 \\times(0.4-0.1333) + 0.3043 \\times(0.4666 - 0.4) \\\\\n&amp;= 0.24560955\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n \n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n\n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n \n\n \n \n \n \n \n \n\n \n\n \n \n \n\n \n\n\n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n计算mAPmAP的全程是mean Average Precision，即是所有AP的均值。因为AP只是正对一类的准确率进行评估，而在多类别检测/分类任务中就需要一个指标来对整个多分类任务的性能进行评估，这就是mAP。\n计算公式:\n\nmAP = \\frac{\\sum_c AP_c}{N_{class}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n \n\n \n \n\n\n\n \n\n \n \n \n \n \n\n\n\n\n\n\n计算F1-measure综合评价指标F-measure又称F-Score，是Precision和Recall的加权调和平均，常用于评价分类模型的好坏。\n计算公式:\n\nF_\\alpha = \\frac{(\\alpha^2+1)\\text{Precision}\\times\\text{Recall}}{\\alpha^2 \\text{Precision} + \\text{Recall}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n \n\n \n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n\n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n\n\n\n\n\n常见的F\n_1\n\n\n\n\n \n\n-measure即为F-measure中\n\\alpha=1\n\n\n\n\n\n\n \n \n \n\n的特例:\n\nF_1 = \\frac{2 \\times \\text{Precision}\\times\\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n \n \n\n \n \n \n \n \n \n \n \n \n\n \n\n \n \n \n \n \n \n\n\n\n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n\n\n\n\n\n\n该公式的另外一种形式可以帮助记忆:\n\n\\frac2{F_1} = \\frac1{\\text{Precision}} + \\frac1{\\text{Recall}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n \n\n\n\n \n\n \n \n \n \n \n \n \n \n \n\n\n\n \n\n\n\n \n\n \n \n \n \n \n \n\n\n\n\n\n参考资料[1] rafaelpadilla/Object-Detection-Metrics\n[2] 目标检测中的mAP是什么含义?\n[3] 多标签图像分类任务的评价方法-mAP \n","plink":"https://yuxinzhao.net/pr-roc/"},{"title":"What is IoU","date":"2019-07-15T20:20:32.000Z","updated":"2022-01-04T08:35:48.655Z","content":"在目标检测任务中常常用IoU来计算预测窗口与真实窗口的交叠率。本文介绍IoU的概念。\n\n\n\nIoU (Intersection over Union, 交并比) 是两个窗口的交集与并集面积之比。\n\n区域 Region-1 与 Region-2 的交集如上图黄色区域\n区域 Region-1 与 Region-2 的并集如上图绿色区域\n\nIoU计算公式:\n\nIoU = \\frac{\\text{Region-1} \\cap \\text{Region-2}}{\\text{Region-1} \\cup \\text{Region-2}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n\n\n\n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n \n\n\n\n\n\n\n","plink":"https://yuxinzhao.net/what-is-IoU/"},{"title":"What is RoI pooling","date":"2019-07-15T11:52:13.000Z","updated":"2022-01-04T08:35:48.655Z","content":"上文讲了什么是RoI(Region of Interest, 感兴趣区域)，本文讲述RoI Pooling的概念。\n\n\nRoI Pooling 简介\nRoI Pooling 的作用RoI Pooling是Pooling(池化)的一种，而且是针对RoI的池化。它的作用是输入尺寸不固定的特征图，输出尺寸固定的特征图。\nRoI Pooling 的输入\n特征图 feature map：由原图像通过CNN计算得到的特征图\n感兴趣区域 RoI: 许多候选框，形状为 \n1 \\times 5 \\times 1\n\n\n\n\n\n\n \n \n \n \n \n\n (4 个坐标[x,y,w,h] + 索引[index])\n\n注意\n\nRoI的坐标的参考系是原图(CNN的输入)，而不是feature map。\n\nRoI Pooling 的输出输出batch个张量，其中batch的值等于RoI的个数，张量大小为(channel, w, h)， w和h人为指定。RoI Pooling的过程就是将一个个尺寸不同的RoI都映射成大小固定的(w, h)的RoI。\n图解 RoI Pooling考虑一个 \n8 \\times 8\n\n\n\n\n\n \n \n \n\n 大小的feature map，一个RoI，以及输出大小为 \n2 \\times 2\n\n\n\n\n\n \n \n \n\n.\n\n输入固定大小的feature map\nregion proposal 投影后的位置\n将其划分为 \n(2 \\times 2)\n\n\n\n\n\n\n\n \n \n \n \n \n\n 个sections (因为输出大小为\n2 \\times 2\n\n\n\n\n\n \n \n \n\n)\n对每个section做max pooling，可以得到:\n\n参考引用[1] Region of interest pooling explained\n[2] ROI Pooling层解析\n","plink":"https://yuxinzhao.net/what-is-RoI-pooling/"},{"title":"What is RoI","date":"2019-07-15T11:07:26.000Z","updated":"2022-01-04T08:35:48.659Z","content":"\n\nRoI概念\nRoI的全称是Region of Interest，中文名称是”感兴趣区域”。\nRoI是从图像中选择的一个图像区域，这个区域是你进行图像分析的重点。圈出这块区域可以得到一个子图像(subimage)，之后就可以在这个区域内使用进行进一步处理。\n目的：\n\n减少处理时间\n增加精度\n\n","plink":"https://yuxinzhao.net/what-is-RoI/"},{"title":"[论文笔记] Faster R-CNN: Towards Real-Time Object Detection With Region Proposal Networks","date":"2019-07-14T13:40:12.000Z","updated":"2022-01-04T08:35:48.583Z","content":"Faster R-CNN在Fast R-CNN的基础上做改进，提出用RPN（Region Proposal Network, 一种全卷积神经网络）代替Selective Search，降低检测耗时。Faster R-CNN由RPN和Fast R-CNN构成，RPN和Fast R-CNN共享卷积计算得到的特征图，以此降低计算量，使得Faster R-CNN可以在单GPU上以5fps的速度运行，且精度达到SOTA。\n\n\n\n\n术语缩写\n\n\n缩写\n全称\n\n\n\nR-CNN\nRegion Convolution Neural Network\n\n\nRPN\nRegion Proposal Network\n\n\nFCN\nFully Convolutional Network\n\n\nSS\nSelective Search\n\n\nZF\nZeiler and Fergus model\n\n\nIoU\nIntersection-over-Union\n\n\nRoI\nRegion of Interest\n\n\nFaster R-CNN\nFaster R-CNN is a single, unified network for object detection. The RPN module serves as the ‘attention’ of the unified network.\nFaster R-CNN 由以下两个部分组成:\n\nDeep fully convolutional network that proposes regions\nFast R-CNN detector that uses the proposed regions\n\n\n The RPN module tells the Fast R-CNN module where to look.\n\nRegion Proposal Network\nA Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.\nThe authors model this process with a fully convolutional network.\n\nTo generate region proposals, the authors slide a small network(\nn \\times n\n\n\n\n\n\n \n \n \n\n spatial window, \nn=3\n\n\n\n\n\n\n \n \n \n\n) over the convolutional feature map output by the last shared convolution layer. \n\nEach sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU following). \n\nThis feature is fed into two sibling fully-connected layers – a box-regression layer (\nreg\n\n\n\n\n\n\n \n \n \n\n) and a box-classification layer (\ncls\n\n\n\n\n\n\n \n \n \n\n). This architecture is naturally implemented with and \nn \\times n\n\n\n\n\n\n \n \n \n\n convolutional layer followed by two sibling \n1 \\times 1\n\n\n\n\n\n \n \n \n\n convolutional layers (for \nreg\n\n\n\n\n\n\n \n \n \n\n and \ncls\n\n\n\n\n\n\n \n \n \n\n respectively).\n\n\nAnchorthe number of maximum possible proposals for each location is denoted as \nk\n\n\n\n\n \n\n. \n\n\nreg\n\n\n\n\n\n\n \n \n \n\n layer has \n4k\n\n\n\n\n\n \n \n\n outputs \n(x, y, w, h)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n\n encoding the coordinates of \nk\n\n\n\n\n \n\n boxes\n\ncls\n\n\n\n\n\n\n \n \n \n\n layer has \n2k\n\n\n\n\n\n \n \n\n outputs scores that estimate probability of object or not object for each proposal. (implemented as a two-class softmax layer)\n\nThe \nk\n\n\n\n\n \n\n proposals are parameterized relative to \nk\n\n\n\n\n \n\n reference boxes, which we call anchors.\n\nAn anchor is centered at the sliding window in question, and is associated  with a scale and aspect ratio (宽高比).\nFor a convolutional feature map of a size \nW \\times H \n\n\n\n\n\n\n \n \n \n\n , there are \nWHk\n\n\n\n\n\n\n \n \n \n\n anchors in total.\n\nuse 3 scales with box areas of \n128^2\n\n\n\n\n\n\n \n \n \n \n\n, \n256^2\n\n\n\n\n\n\n \n \n \n \n\n, \n512^2\n\n\n\n\n\n\n \n \n \n \n\n pixels, and 3 aspect ratios of 1:1, 1:2, 2:1.\n\nMulti-Scale Anchors as Regression Reference\n作者采取b)和c)变化窗口形状和大小的方法，而不使用a)变换图片大小的方法。\nLost FunctionPositive anchor:\n\nthe anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth box (作者采用).\nan anchor that has an IoU overlap &gt; 0.7 with any grouth-truth box (作者不采用).\n\nNegative anchor:\n\nnon-positive anchor if its IoU &lt; 0.3 for all grouth-truth boxes.\n\nlost  function for an image\nL(\\{p_i\\}, \\{t_i\\}) = \\frac1{N_{cls}} \\sum_i{L_{cls}} (p_i, p_i^*) + \\lambda \\frac{1}{N_{reg}} \\sum_i {L_{reg}(t_i, t_i^*)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n\n \n\n \n \n \n\n\n \n\n \n \n\n \n\n \n \n \n\n \n \n \n\n\n\n \n\n \n\n \n \n \n\n\n\n\n\n \n \n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n\n\n\n\n\nL_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is log loss over two classes (object \nvs.\n\n\n\n\n\n\n \n \n \n\n not object)\n\nL_{reg}(t_i, t_i^*) = R(t_i - t_i^*)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n \n\n \n \n \n \n\n \n \n\n \n\n \n \n \n\n \n\n where \nR\n\n\n\n\n \n\n is the robust loss function (smooth \nL_1\n\n\n\n\n\n \n \n\n)\n\nN_{cls}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is the mini-batch size (i.e., \nN_{cls}=256\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n \n\n\n)\n\nN_{reg}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n is the number of anchor locations (i.e., \nN_{reg} \\approx 2400\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n \n \n \n \n\n\n) \n\n\\lambda = 10\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n and thus both \ncls\n\n\n\n\n\n\n \n \n \n\n and \nreg\n\n\n\n\n\n\n \n \n \n\n terms are roughly equally weighted.\n\n\nThe normalization as above is not required and could be simplified.\n\n\n\\begin{align}\n&amp;t_x = (x-x_a)/w_a, &amp; t_y = (y-y_a)/h_a, \\\\\n&amp;t_w = \\log(w/w_a), &amp; t_h = log(h/h_a),  \\\\\n&amp;t^*_x = (x-x^*_a)/w_a, &amp; t^*_y = (y^*-y_a)/h_a, \\\\\n&amp;t^*_w = \\log(w^*/w_a), &amp; t^*_h = log(h^*/h_a),  \\\\\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n\n \n \n \n\n \n \n \n\n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n \n \n\n \n \n \n\n \n \n\n \n \n\n \n\n\n \n \n \n \n\n \n \n \n\n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n \n \n \n \n \n \n\n \n \n\n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n \n \n \n\n \n \n\n \n \n\n\n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n \n \n\n \n\n\n \n \n \n \n \n \n \n \n\n \n \n\n \n\n \n \n\n \n \n\n\n\n\n\nTraining RPNsThe RPN can be trained end-to-end by back-propagation and stochastic gradient descent (SGD).\n\n一张图片包含多个正样本和负样本(正样本少于负样本)\n随机采样256个样本用于计算loss of a mini-batch\n初始化: Gaussian distribution (mean=0, standard deviation=0.01)\nmomentum: 0.9,   weight decay: 0.0005\n\nSharing Features for RPN and Fast R-CNN4 - Step Alternating Training\nTrain the RPN as described previously.\nTrain a separated detection network (ImageNet-pre-trained) by Fast R-CNN using the proposals generated by step-1 RPN.\nUse the detector network to initialize RPN training, but fix the shared convolutional layers and only fine-tune the layers unique to RPN.\nkeeping the shared convolutional layers fixed, fine-tune the unique layers of Fast R-CNN.\n\n","thumbnail":"/static/image/faster-r-cnn.png","plink":"https://yuxinzhao.net/faster-r-cnn/"},{"title":"git merge命令提示Already up to date","date":"2019-07-09T14:45:33.000Z","updated":"2022-01-04T08:35:48.591Z","content":"本文描述 git merge &lt;branch&gt; 提示 Already up to date 的错误原因及解决方法。\n\n\n假定我有两个分支master和dev。当前处在dev分支。\n12$ git merge masterAlready up to date.\n\nCause of Errors 错误原因Already up to date意味着已经是最新的了，即dev是在master的基础上修改的，而master自分出dev分支后就没修改过，因此dev分支是最新的，不需要于master合并，因为合并完的仓库和当前的dev分支是一模一样的。\n其git分支图如下：\n123                 E -- F -- G(dev)                /A -- B -- C -- D(master)\n\n其等同于:\n1A -- B -- C -- D(master) -- E -- F -- G(dev)\n\n我们想要做的合并实际上是将master指向dev而已。\nSolution 解决方法多提交一个空的commit，使两条分支叉开，再合并。\n123                 E -- F -- G(dev)                /A -- B -- C -- D -- H(master)\n\n123                 E -- F -- G(dev)                /           \\A -- B -- C -- D -- H ------ I(master)\n\n上图中H是一个不做任何修改的提交，其作用是将master和dev分到两个叉开的分支上，使得master不再是dev的父节点。\n具体命令:\n12345678# 移动到master分支下$ git checkout master# 在master提交一个空内容$ git commit --allow-empty -m \"ready for merging\"# 合并master和dev分支$ git merge dev# 移除dev指针(可选)$ git branch --delete dev","plink":"https://yuxinzhao.net/git-merge-already-up-to-date/"},{"title":"[论文笔记] Two-Stream Convolutional Networks for Action Recognition in Videos","date":"2019-02-24T18:56:15.000Z","updated":"2022-01-04T08:35:48.647Z","content":"The paper is by Karen Simonyan, Andrew Zisserman.\nPublication date : 12 Nov 2014\n\n\n\n\nBasic IdeaThe authors’ contribution is:\n\npropose a two-stream ConvNet architecture which incorporates spatial and temporal networks.\ndemonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance.\nmulti-task learning can increase the amount of training data and improve the performance on both.\n\nOptical flow\nOptical flow stackingA dense optical flow can be seen as a set of displacement vector field \nd_t\n\n\n\n\n\n \n \n\n between the pairs of consecutive frames \nt\n\n\n\n\n \n\n and \nt+1\n\n\n\n\n\n\n \n \n \n\n. \n \nd_t(u,v)\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n denotes the displacement vector at point \n(u,v)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n in frame \nt\n\n\n\n\n \n\n, which moves the point to corresponding point in the following frame \nt+1\n\n\n\n\n\n\n \n \n \n\n.\nThe horizontal and vertical components of the vector field, \nd_t^x\n\n\n\n\n\n\n \n \n \n\n and \nd_t^y\n\n\n\n\n\n\n \n \n \n\n, can be seen as image channels.\nTo represent the motion across a sequence of frames, the authors stack the flow channels \nd_t^{x,y}\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n\n of \nL\n\n\n\n\n \n\n consecutive frames to form a total of \n2L\n\n\n\n\n\n \n \n\n input channels.\nlet \nw\n\n\n\n\n \n\n and \nh\n\n\n\n\n \n\n be the width and height of a video.\na ConvNet input volume \nI_\\tau \\in \\mathbb R^{w \\times h \\times 2L}\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n\n \n \n \n \n \n \n\n\n\n for an arbitrary frame \n\\tau\n\n\n\n\n \n\n is then constructed as follows:\n\n\\begin{align}\nI_\\tau(u,v,2k-1) &amp;= d_{\\tau+k-1}^x(u,v) \\\\\nI_\\tau(u,v,2k) &amp;= d_{\\tau+k-1}^y(u,v) \\\\\nu=[1;w], v&amp;=[1;h], k=[1;L]\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n \n\n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n \n\n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\nFor an arbitrary point \n(u,v)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n, the channels \nI_\\tau(u,v,c), c=[1;2L]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n encode the motion at that point over a sequence of L frames.\n\nTrajectory stackingAn alternative motion representation, inspired by the trajectory-based descriptors, replaces the optical flow, sampled at the same locations across several frames, with the flow, sampled along the motion trajectories.\n\n\\begin{align}\nI_\\tau(u,v,2k-1) &amp;= d_{\\tau+k-1}^x (p_k) \\\\\nI_\\tau(u,v,2k) &amp;= d_{\\tau+k-1}^y (p_k) \\\\\nu=[1;w], v&amp;=[1;h],k=[1;L]\n\\end{align}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n\n\n\n\n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n \n\n\n \n\n \n \n\n \n \n \n \n \n\n\n \n\n \n \n\n \n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\nwhere \np_k\n\n\n\n\n\n \n \n\n is the \nk\n\n\n\n\n \n\n-th point along the trajectory, which starts at the location (u,v) in the frame \n\\tau\n\n\n\n\n \n\n and is defined by the following recurrence relation:\n\np_1=(u,v) \\\\p_k=p_{k-1}+d_{\\tau+k-2}(p_{k-1}), k&gt;1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n\n\n \n \n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n\n \n\n \n \n \n\n\n \n \n \n \n \n\n\n\n\nBi-directional optical flowConstruct an input volume \nI_\\tau\n\n\n\n\n\n \n \n\n by stacking \nL/2\n\n\n\n\n\n\n \n \n \n\n forward flows between frame \n\\tau\n\n\n\n\n \n\n and \n\\tau + L/2\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n and \nL/2\n\n\n\n\n\n\n \n \n \n\n backward flows between frames \n\\tau-L/2\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n and \n\\tau\n\n\n\n\n \n\n.\nMean flow subtractionGiven a pair of frames, the optical flow between them can be dominated by a particular displacement, e.g. caused by the camera movement. A global motion component can be estimated and subtracted from the dense flow. But the authors consider a simpler approach: from each displacement field \nd\n\n\n\n\n \n\n they subtract its mean vector.\nTwo-stream architectureVideo can be decmposed into spatial and temporal components.\n\nSpatial Part\nin the form of individual frame appearance.\ncarries information about scenes and objects depicted in the video.\n\n\nTemporal Part\nin the form of motion across the frame.\nconveys the movement of the observer (the camera) and the objects.\n\n\n\n\nSoftmax scores are fused using either averaging or a linear SVM.\nConvNets configuration\n\nAll hidden weight layers use the rectification (ReLU) activation function.\nMaxpooling is performed over 3 × 3 spatial windows with stride 2\nThe only difference between spatial and temporal ConvNet configuration is that the second normalisation layer from the latter to reduce memory consumption.\n\nTraining\n\noptimizer: mini-batch SGD with momentum (set to 0.9). The learning rate is initially set to \n10^{-2}\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n, and then decreased according to a fixed schedule (see the paper for details).\nAt each iteration, a mini-batch of 256 samples is constructed by sampling 256 training videos.\nIn spatial nets training, a 224 × 224 sub-image is randomly cropped from the selected frame; it then undergoes random horizontal flipping and RGB jittering. The videos are rescaled beforehand, so that the smallest side of the frame equals 256.\nIn temporal nets training, the sub-image is sampled from the whole frame, not just its 256 × 256 center. an optical flow volume \nI\n\n\n\n\n \n\n is computed. From the volume, a fixed-size \n224 × 224 × 2L\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n \n\n \n \n \n\n input is randomly cropped and flipped.\n\nTesting\n\nGiven a video, the authors sample a fixed number of frames (25 in their experiments) with equal temporal spacing between them. From each of the frames they then obtain 10 ConvNet inputs by cropping and flipping four corners and the center of the frame.\nThe class scores for the whole video are then obtained by averaging the scores across the sampled frames and crops therein.\n\nOptical flow is computed using the off-the-shelf GPU implementation from the OpenCV toolbox.\nEvaluationThe evaluation is performed on UCF-101 and HMDB-51 action recognition benchmarks.\n\nUCF-101 contains 13K videos (180 frames/video on average) of 101 actions.\nHMDB-51 includes 6.8K videos of 51 actions.\n\nSpatial ConvNetsThree scenarios are considered:\n\ntraining from scratch on UCF-101.\npre-training on ILSVRC-2012 followed by fine-tuning on UCF-101.\npre-training on ILSVRC-2012 and keeping the pre-trained network fixed, only training the last (classification) layer.\n\n\nTemporal ConvNets\nTwo-stream ConvNets\n\nPer-class recall of a two-stream model on the first split of UCF-101:\n\n","plink":"https://yuxinzhao.net/two-stream-convolutional-network/"},{"title":"[论文笔记] Long-term Recurrent Convolutional Networks for Visual Recognition and Description","date":"2019-02-22T20:27:20.000Z","updated":"2022-01-04T08:35:48.595Z","content":"The paper is by Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell.\nPublication date : 31 May 2016\nCode and pre-trained model are availabled at here.\n\n\n\n\n\n\nBasic Idea\nThe authors propose Long-term Recurrent Convolutional Networks (LRCNs), a class of architectures for visual recognition and description which combines convolutional layers and long-range temporal recursion and is end-to-end trainable.\nLRCN works by passing each visual input \nx_t\n\n\n\n\n\n \n \n\n (input at time t, an image in isolation, or a frame from a video) through a feature transformation \n\\phi_V(.)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n with parameters \nV\n\n\n\n\n \n\n, usually  a CNN, to produce a fixed-length vector representation \n\\phi_V(x_t)\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n. The outputs of \n\\phi_V\n\n\n\n\n\n \n \n\n are then passed into a recurrent sequence learning module with parameters \nW\n\n\n\n\n \n\n. \n\nThe paper consider three vision problems (activity recognition, image captioning and video description), each of which instantiates one of the following broad classes of sequential learning tasks:\n\nSequential input, static output. \n\n\\langle x_1, x_2, \\dots, x_T  \\rangle \\mapsto y\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n. \nactivity recognition.\n\n\nStatic input, sequential output. \n\nx \\mapsto \\langle y_1, y_2, \\dots, y_T \\rangle\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n\n. \nimage captioning.\n\n\nSequential input, sequential output. \n\n\\langle x_1, x_2, \\dots, x_T \\rangle \\mapsto \\langle y_1, y_2, \\dots, y_{T'} \\rangle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n \n\n \n \n \n\n \n \n\n \n\n \n \n\n \n \n \n\n \n\n \n \n\n\n \n\n. \nvideo description. \nThe number of input and output time steps may differ (we may have \nT \\ne T'\n\n\n\n\n\n\n \n \n\n \n \n\n\n).\n\n\n\nFor a training set \n\\mathcal D\n\n\n\n\n \n\n of labeled sequences \n(x_t, y_t)_{t=1}^T \\in \\mathcal D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n\n, we can optimize the parameters \n(V,W)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n of the model’s visual and sequential components to minimize the expected negative log likelihood of a sequence sampled from the training set.\n\n\\mathcal L(V,W, \\mathcal D) = -\\frac{1}{|\\mathcal D|} \\sum_{(x_t, y_t)_{t=1}^T \\in \\mathcal D} \\sum_{t=1}^T \\log P(y_t | x_{1:t}, y_{1:t-1}, V, W)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n \n\n \n \n \n\n\n\n\n \n\n \n\n \n \n\n \n\n \n \n\n\n \n \n\n \n \n \n\n\n \n \n\n\n\n \n\n \n \n \n\n \n\n\n \n \n \n\n \n \n\n \n \n\n \n\n \n\n \n \n \n\n\n \n\n \n\n \n \n \n \n \n\n\n \n \n \n \n \n\n\nThe authors train their LRCN models using stochastic gradient descent, with backpropagation used to compute the gradient \n\\nabla_{V,W} \\mathcal L(V,W,\\mathcal D)\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n\n of the objective \n\\mathcal L\n\n\n\n\n \n\n with respect to all parameters \n(V,W)\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n over \n\\text{minibatches } D \\subset D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n sampled from the training dataset \n\\mathcal D\n\n\n\n\n \n\n.\nActivity Recognition\nThe CNN base of LRCN in their activity recognition experiments is a hybrid of the CaffeNet reference model (a minor variant of AlexNet).\nThe authors consider both RGB and flow as inputs to their recognition system. And they build LRCN RGB model and LRCN flow model.\nFlow is transformed into a “flow image” by shifting x and y flow values to range [-128, 128]. A third channel for flow image is created by calcualting the flow mangnitude.\nSome classes the LRCN flow model outperforms the LRCN RGB model and vice versa. Their explanation is that activities which are better classified by the LRCN RGB model are best determined by which objects are present in the scene, while activities which are better classified by the LRCN flow model are best classified by the kind of motion in the scene. \nBecause RGB and flow signals are complementary, the best models take both into account.\nImage Captioning\nAt time step \nt\n\n\n\n\n \n\n, the input to bottom-most LSTM is the embedded word from the previous time step \ny_{t-1}\n\n\n\n\n\n\n\n \n\n \n \n \n\n\n, and the output \ny_t\n\n\n\n\n\n \n \n\n is encoded as a one-hot vector: vector \ny \\in \\mathbb R^K\n\n\n\n\n\n\n\n \n \n\n \n \n\n\n with a single non-zero component \ny_i=1\n\n\n\n\n\n\n\n \n \n \n \n\n denoting the \ni^{th}\n\n\n\n\n\n\n \n\n \n \n\n\n word in the vocabulary, plus two additional entries for the \n\\text{&lt;BOS&gt;}\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n(beginning of sequence) token and the \n\\text{&lt;EOS&gt;}\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n (end of sequence) token. \n\nThe authors build three sequence model architectural variants. \nThe unfactored variant (\n\\text{LRCN}_{2u}\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n)  performs worse than the other two(\n\\text{LRCN}_{1u}\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n and \n\\text{LRCN}_{2f}\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n). \n\\text{LRCN}_{1u}\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n and \n\\text{LRCN}_{2f}\n\n\n\n\n\n\n\n\n\n \n \n \n \n\n \n \n\n\n perform similarly.\nVideo Description\nThe authors use CRF to predict activity, tool, object, and locations present in the video. The CRF is based on the full video input, so we observe the video as whole at each time step, not incrementally frame by frame.\n\nThe author build three architectures for video description:\n\nLSTM encoder &amp; decoder with CRF max.\nRecognize a semantic representation of the video using the maximum a posteriori (MAP) estimate of a CRF with video features as unaries.\nThe representation, e.g., \n\\langle \\text{knife, cut, carrot, cutting board}  \\rangle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n\n which is translated to a natural language sentence (\n\\text{a person cuts a carrot on the board}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n) using an encoder-decoder LSTM.\n\n\nLSTM decoder with CRF max.\nprovide the full visual input representation at each time step to the LSTM.\n\n\nLSTM decoder with CRF probabilites.\nThe architecture is the same as LSTM decoder with CRF max, but replace max predictions with probability distributions.\n\n\n\nThe LSTM Decoder (CRF-prob) performs best and gets the highest BLEU score.\n","plink":"https://yuxinzhao.net/lrcn/"},{"title":"[论文笔记] Learning Spatiotemporal Features with 3D Convolutional Networks","date":"2019-02-18T15:36:20.000Z","updated":"2022-01-04T08:35:48.539Z","content":"The paper is by Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri.\nPublication date : 7 Oct 2015\nC3D source code and pre-trained model are available at here.\n\n\n\n\nBasic IdeaThe authors build C3D (Convolutional 3D) model to obtain a generic, compact, efficient, and simple video descriptor.\nThe authors’s contributions in this paper are:\n\nThey show 3D convolutional deep networks are good feature learning machines that model appearance and motion simultaneously.\nThey find that 3 × 3 × 3 convolution kernel for all layers to work best among the limited set of explored architectures. \nThe proposed features with a simple linear model outperform or approach the current best methods on 4 different tasks and 6 different benchmarks (see Table 1). They are also compact and efficient to compute.\n\n\n3D Convolution and 3D PoolingThe difference between 2D and 3D convolution operations is showed in Figure 1.\n\nNotationsVideo Clip size:    \nc \\times l \\times h \\times w\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \n\n   (\nc\n\n\n\n\n \n\n is the number of channels, \nl\n\n\n\n\n \n\n is length in number of frames, \nh\n\n\n\n\n \n\n and \nw\n\n\n\n\n \n\n are the height and width of the frame, respectively)\n3D convolution and pooling kernel size:   \nd \\times k \\times k\n\n\n\n\n\n\n \n \n \n \n \n\n   (\nd\n\n\n\n\n \n\n is kernel temporal depth, \nk\n\n\n\n\n \n\n is kernel spatial size)\nExploring kernel temporal depthCommon network settingsAll video frames are resized into 128 × 171, and then the authors use jittering by using random crops with a size of 3 × 16 × 112 × 112 of the input clips during training.\n\nThe networks have 5 convolution layers and 5 pooling layers (each convolution layer is immediately followed by a pooling layer), 2 fully-connected layers and a softmax loss layer to predict action labels.\nAll convolution kernel size:   \nd\n\n\n\n\n \n\n × 3 × 3   (the authors vary the value of \nd\n\n\n\n\n \n\n to search for a good 3D architecture)\nAll pooling kernel size:   2 × 2 × 2   (except for the Pool1 whose size is 1 × 2 × 2, not to merge the temporal signal too early)\ntraining: \n\nfrom scratch\nmini-batch of 30 clips\nInitial learning rate: 0.003. The learning rate is divided by 10 after every 4 epochs. The train is stopped after 16 epochs. \ndataset: UCF101\n\nVarying network architecturesThe author experiment with two types of architectures:\n\nhomogeneous temporal depth\nall convolution layers have the same kernel temporal depth.\nThe authors experiment with 4 networks having kernel temporal depth of \nd\n\n\n\n\n \n\n equal to 1, 3, 5, and 7, and name these networks as depth-d.\ndepth-1, depth-3, depth-5, depth-7.\ndepth-1 is the same as 2D ConvNet.\n\n\nvarying temporal depth\nkernel temporal depth is changing across the layers.\nThe authors experiment 2 networks with temporal depth increasing: 3-3-5-5-7 and decreasing: 7-5-5-3-3 from the first to the fifth convolution layer respectively.\nincrease, decrease.\n\n\n\nResult\n2D ConvNet performs worst and 3D ConvNet with 3 × 3 × 3 kernels performs best among the experimented nets.\nThe best kernel temporal depth is 3.\nC3D (Convolutional 3D)C3D architecture\nC3D net has 8 convolution, 5 max-pooling, and 2 fully connected layers, followed by a softmax output layer. All 3D convolution kernels are 3 × 3 × 3 with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are 2 × 2 × 2, except for pool1 is 1 × 2 × 2 with the intention of preserving the temporal information in the early phase. Each fully connected layer has 4096 output units.\nDatasetDataset:   Sports-1M\n\n1.1 million sport videos.\neach video belongs to one of 487 sports categories.\n\nTraininginput:\n\nrandomly extract five 2-second long clips from every training video.\nClips are resized to have a frame size of 128 × 171.\nOn training, clips are randomly crop into 16 × 112 × 112 crops for spatial and temporal jittering.\nhorizontally flip clips with 50% probability.\n\noptimizer:\n\nSGD with mini-batch size of 30 examples.\nInitial learning rate is 0.003, and is divided by 2 every 150K iterations. The optimization is stopped at 1.9M iterations (about 13 epochs).\n\nThe authors experiment with C3D net trained from scratch and C3D net fine-tuned from the model pre-trained on I380K dataset.\nResult\nC3D video descriptorAfter training, C3D can be used as a feature extractor for other video analysis task.\n\nTo extract C3D feature, a video is split into 16 frame long clips with a 8-frame overlap between two consecutive clips.\nThese clips are passed to the C3D network to extract fc6 activations.\nThese clip fc6 activations are averaged to form a 4096-dim video descriptor/feature which is then followed by an L2-normalization.\n\nC3D is compactThe authors use PCA to project the features into lower dimensions and report the classification accuracy of the projected features on UCF101 using a linear SVM.\n\nAction RecognitonDatasetthe authors use the tree split setting provided with UCF101 dataset which consists of 13,320 videos of 101 human action categories.\nModelThe authors extract C3D features and input them to a multi-class linear SVM for training models.\n\nC3D (1 net) + linear SVM\nC3D is trained on I380K dataset\n\n\nC3D (3 nets) + linear SVM\nuse 3 different nets:\nC3D trained on 1380K dataset\nC3D trained on Sports-1M dataset\nC3D trained on I380K and fine-tuned on Sports-1M\n\n\nconcatenate the L2-normalized C3D features of these nets.\n\n\n\nResult\nAction Similarity LabelingDatasetThe ASLAN dataset consists of 3,631 videos from 432 action classes. The task is to predict if a given pair of videos belong to the same or different action.\nThe authors use the prescribed 10-fold cross validation with the splits provided with the dataset.\nFeatures\nsplit videos into 16-frame clips with an overlap of 8 frames\nextract C3D features: prob(softmax), fc7, fc6, pool5 for each clip.\naveraging the clip features separately for each type of feature, followed by an L2 normalization.\n\nModelThe authors use the model of Kliper-Gross et al. to compute the 12 different distances.\nWith 4 types of features, the authors obtain 48-dimensional feature vector (12 × 4 = 48) for each video pair. \nAs these 48 distances are not comparable to each other, the authors normalize them independently such that each dimension has zero mean and unit variance.\nA linear SVM is trained to classify video pairs into same or different on these 48-dim feature vectors.\nResult\n\nScene and Object RecognitionDatasetScene recognition:\n\nYUPENN: 420 videos of 14 scene categories.\nMaryland: 130 videos of 13 scene categories.\n\nObject recogniton:\n\nan egocentric dataset which consists 42 types of everyday objects.\n\nModelScene recognition:\n\nuse the same setup of feature extraction and linear SVM for classification and follow the same leave-one-out evaluation protocol as described by the authors of these datasets.\n\nObject recognition:\n\nslide a window of 16 frames to extract C3D features. \nchoose the ground truth label for each clip to be the most frequently occurring label of the clip. If the most frequent label in a clip occurs fewer than 8 frames, the authors consider it as negative clip with no object and discard it in both training and testing.\ntrain and test C3D features using linear SVM.\n\nResultScene recognition:\n\nObject recognition:\nC3D obtains 22.3% accuracy and outperforms by 10.3% with only linear SVM where the comparing method used RBF-kernel  on strong SIFT-RANSAC feature matching. Compared with Imagenet baseline, C3D is still 3.4% worse.\nRuntime AnalysisThe authors report runtime of the three above-mentioned methods to extract features (including I/O) for the whole UCF101 dataset using a single CPU or a single K40 Tesla GPU.\n\nC3D is much faster than real-time, processing at 313 fps while the other two methods have a processing speed of less than 4 fps.\n","plink":"https://yuxinzhao.net/c3d/"},{"title":"About Me","date":"2019-09-25T00:19:12.000Z","updated":"2022-01-04T08:35:48.659Z","content":"BiographyI’m a college student of software engineering at the South China University of Technology. My research interests include artificial intelligence, micro service and engineering.\n我是华南理工大学软件工程专业的一名大学生。我的研究兴趣包括人工智能、微服务和软件工程。\nInterests\nArtifical Intelligence\nMicro Service\nEngineering\n\nEducation🎓 BSc in Software Engineering, 2021. — South China University of Technology\nContact\n\n\nInfo\nLink\n\n\n\nEmail\n1322032296@qq.com  zyx1322032296@gmail.com\n\n\nGithub\nYuxinZhaozyx\n\n\nBilibili\n星下旅人\n\n\n","plink":"https://yuxinzhao.net/about/"},{"title":"Links","date":"2019-09-25T13:51:22.000Z","updated":"2022-01-04T08:35:48.659Z","content":"\n    \n        \n        \n            \n            \n            Jireh Chan\n            图形学、NLP大佬\n            \n        \n        \n        \n            \n            \n            Jiaxiao Wen\n            张量分解领域大佬\n            \n        \n        \n        \n            \n            \n            RG-UNIVERSE\n            云计算大佬\n            \n        \n    \n\n\n\n\n","plink":"https://yuxinzhao.net/links/"}]