{"title":"[PyTorch学习笔记] Neural Networks","date":"2019-07-22T20:48:56.000Z","thumbnail":"/static/image/pytorch.png","link":"pytorch-learning-note-3","comments":true,"tags":["pytorch"],"categories":["machine-learning"],"updated":"2022-01-04T08:35:48.627Z","content":"<p>本文是PyTorch官方教程 [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ] Neural Networks 的学习笔记</p>\n<p>使用torch.nn包来构建神经网络。</p>\n<p><code>nn</code>包依赖<code>autograd</code>包来定义模型并求导。 一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法，该方法返回<code>output</code>。</p>\n<p><img src=\"pytorch-learning-note-3/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f6d6e6973742e706e67.png\" alt=\"img\" class=\"article-img\"></p>\n<p>它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。</p>\n<p>神经网络的典型训练过程如下：</p>\n<ol>\n<li>定义包含一些可学习的参数(或者叫权重)神经网络模型；</li>\n<li>在数据集上迭代；</li>\n<li>通过神经网络处理输入；</li>\n<li>计算损失(输出结果和正确值的差值大小)；</li>\n<li>将梯度反向传播回网络的参数；</li>\n<li>更新网络的参数，主要使用如下简单的更新原则： <code>weight = weight - learning_rate * gradient</code></li>\n</ol>\n<a id=\"more\"></a>\n\n\n\n<h2 id=\"定义网络\">定义网络<a href=\"pytorch-learning-note-3#定义网络\"></a></h2><figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super(Net, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 1 input image channel</span></span><br><span class=\"line\">        <span class=\"comment\"># 6 output image channel</span></span><br><span class=\"line\">        <span class=\"comment\"># 5x5 square convolution kernel</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># an affine operation: y = Wx + b</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Max pooling over a (2, 2) window</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"comment\"># if the size is a square you can only specify a single number</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        size = x.size()[<span class=\"number\">1</span>:]  <span class=\"comment\"># all dimensions except the batch dimension</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        num_features = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</span><br><span class=\"line\">            num_features *= s</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num_features</span><br><span class=\"line\">    </span><br><span class=\"line\">net = Net()</span><br><span class=\"line\">print(net)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Net(</span><br><span class=\"line\">  (conv1): Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (conv2): Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (fc1): Linear(in_features=<span class=\"number\">400</span>, out_features=<span class=\"number\">120</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  (fc2): Linear(in_features=<span class=\"number\">120</span>, out_features=<span class=\"number\">84</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  (fc3): Linear(in_features=<span class=\"number\">84</span>, out_features=<span class=\"number\">10</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></div></figure>\n\n<p><strong>在模型中必须要定义 <code>forward</code> 函数</strong>，<code>backward</code> 函数（用来计算梯度）会被<code>autograd</code>自动创建。 可以在 <code>forward</code> 函数中使用任何针对 Tensor 的操作。</p>\n<p><strong><code>net.parameters()</code>返回可被学习的参数（权重）列表和值</strong></p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">params = list(net.parameters())</span><br><span class=\"line\">print(len(params))</span><br><span class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># params[0] == net.conv1.weight</span></span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">10</span></span><br><span class=\"line\">torch.Size([<span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>]</span><br></pre></td></tr></table></div></figure>\n\n<p><strong><code>net.named_parameters</code>可同时返回可学习的参数及名称。</strong></p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> name,parameters <span class=\"keyword\">in</span> net.named_parameters():</span><br><span class=\"line\">    print(name,<span class=\"string\">':'</span>,parameters.size())</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv1.weight : torch.Size([<span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\">conv1.bias : torch.Size([<span class=\"number\">6</span>])</span><br><span class=\"line\">conv2.weight : torch.Size([<span class=\"number\">16</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\">conv2.bias : torch.Size([<span class=\"number\">16</span>])</span><br><span class=\"line\">fc1.weight : torch.Size([<span class=\"number\">120</span>, <span class=\"number\">400</span>])</span><br><span class=\"line\">fc1.bias : torch.Size([<span class=\"number\">120</span>])</span><br><span class=\"line\">fc2.weight : torch.Size([<span class=\"number\">10</span>, <span class=\"number\">120</span>])</span><br><span class=\"line\">fc2.bias : torch.Size([<span class=\"number\">10</span>])</span><br></pre></td></tr></table></div></figure>\n\n<p>测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>)</span><br><span class=\"line\">out = net(input)</span><br><span class=\"line\">print(out)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ <span class=\"number\">0.1052</span>, <span class=\"number\">-0.0361</span>,  <span class=\"number\">0.1122</span>,  <span class=\"number\">0.1072</span>,  <span class=\"number\">0.0887</span>,  <span class=\"number\">0.0477</span>,</span><br><span class=\"line\"> <span class=\"number\">0.0916</span>, <span class=\"number\">-0.0594</span>,</span><br><span class=\"line\">         <span class=\"number\">-0.1450</span>,  <span class=\"number\">0.0574</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></div></figure>\n\n<p>将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.zero_grad()</span><br><span class=\"line\">out.backward(torch.randn(<span class=\"number\">1</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></div></figure>\n\n<blockquote>\n<p><code>torch.nn</code> 只支持小批量输入。整个 <code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。 例如，<code>nn.Conv2d</code> 接受一个4维的张量， <code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。 <strong>如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code> 来添加其它的维数</strong></p>\n</blockquote>\n<p>我们回顾一下到目前为止用到的类。</p>\n<p><strong>回顾:</strong></p>\n<ul>\n<li><code>torch.Tensor</code>：一个用过自动调用 <code>backward()</code>实现支持自动梯度计算的 <em>多维数组</em> ， 并且保存关于这个向量的梯度</li>\n<li><code>nn.Module</code>：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。</li>\n<li><code>nn.Parameter</code>：一种变量，当把它赋值给一个<code>Module</code>时，被 自动地注册为一个参数。</li>\n<li><code>autograd.Function</code>：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个<code>Tensor</code>的操作都h会创建一个接到创建<code>Tensor</code>和 编码其历史 的函数的<code>Function</code>节点。</li>\n</ul>\n<h2 id=\"损失函数\">损失函数<a href=\"pytorch-learning-note-3#损失函数\"></a></h2><p>一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。</p>\n<p><a href=\"https://pytorch.org/docs/nn\" target=\"_blank\" rel=\"noopener\">nn包</a>中有很多不同的<a href=\"https://pytorch.org/docs/nn.html#loss-functions\" target=\"_blank\" rel=\"noopener\">损失函数</a>。 <code>nn.MSELoss</code>是一个比较简单的损失函数，它计算输出和目标间的<strong>均方误差</strong>， 例如：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>)</span><br><span class=\"line\">output = net(input)</span><br><span class=\"line\">target = torch.randn(<span class=\"number\">10</span>)  <span class=\"comment\"># 随机值作为样例</span></span><br><span class=\"line\">target = target.view(<span class=\"number\">1</span>, <span class=\"number\">-1</span>)  <span class=\"comment\"># 使target和output的shape相同</span></span><br><span class=\"line\"></span><br><span class=\"line\">criterion = nn.MSELoss()</span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\">print(loss)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor(<span class=\"number\">1.1509</span>, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></div></figure>\n\n<h3 id=\"常见损失函数\">常见损失函数<a href=\"pytorch-learning-note-3#常见损失函数\"></a></h3><h4 id=\"nn-L1Loss\"><code>nn.L1Loss</code><a href=\"pytorch-learning-note-3#nn-L1Loss\"></a></h4><p style=\"text-align:center\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"51.565ex\" height=\"6.509ex\" style=\"vertical-align: -2.671ex;\" viewbox=\"0 -1652.5 22201.4 2802.6\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">loss(x, y) = \n\\begin{cases}\n\\frac1n \\sum \\left| x_i - y_i \\right|, &amp; \\text{if reduction='mean'} \\\\\n\\sum \\left| x_i - y_i \\right|, &amp; \\text{if reduction='sum'}\n\\end{cases}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-73\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-79\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-7B\" d=\"M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6E\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ1-2211\" d=\"M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-7C\" d=\"M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-69\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-69\" d=\"M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-66\" d=\"M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-72\" d=\"M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-65\" d=\"M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-64\" d=\"M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-75\" d=\"M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-63\" d=\"M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-27\" d=\"M78 634Q78 659 95 676T138 694Q166 694 189 668T212 579Q212 525 190 476T146 403T118 379Q114 379 105 388T95 401Q95 404 107 417T133 448T161 500T176 572Q176 584 175 584T170 581T157 576T139 573Q114 573 96 590T78 634Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6D\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-73\" d=\"M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ3-7B\" d=\"M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"298\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"784\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"1253\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1723\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"2112\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"2685\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-79\" x=\"3130\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"3627\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"4294\" y=\"0\"/>\n<g transform=\"translate(5351,0)\">\n <use xlink:href=\"#E1-MJSZ3-7B\"/>\n<g transform=\"translate(917,0)\">\n<g transform=\"translate(-11,0)\">\n<g transform=\"translate(0,593)\">\n<g transform=\"translate(120,0)\">\n<rect stroke=\"none\" width=\"544\" height=\"60\" x=\"0\" y=\"220\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMAIN-31\" x=\"134\" y=\"629\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6E\" x=\"84\" y=\"-488\"/>\n</g>\n <use xlink:href=\"#E1-MJSZ1-2211\" x=\"951\" y=\"0\"/>\n<g transform=\"translate(2174,0)\">\n <use xlink:href=\"#E1-MJMAIN-7C\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(278,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"1417\" y=\"0\"/>\n<g transform=\"translate(2418,0)\">\n <use xlink:href=\"#E1-MJMATHI-79\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"693\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-7C\" x=\"3253\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"5872\" y=\"0\"/>\n</g>\n<g transform=\"translate(0,-760)\">\n <use xlink:href=\"#E1-MJSZ1-2211\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(1223,0)\">\n <use xlink:href=\"#E1-MJMAIN-7C\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(278,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"1417\" y=\"0\"/>\n<g transform=\"translate(2418,0)\">\n <use xlink:href=\"#E1-MJMATHI-79\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"693\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-7C\" x=\"3253\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"4921\" y=\"0\"/>\n</g>\n</g>\n<g transform=\"translate(7140,0)\">\n<g transform=\"translate(0,593)\">\n <use xlink:href=\"#E1-MJMAIN-69\"/>\n <use xlink:href=\"#E1-MJMAIN-66\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-72\" x=\"835\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-65\" x=\"1227\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-64\" x=\"1672\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-75\" x=\"2228\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-63\" x=\"2785\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-74\" x=\"3229\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-69\" x=\"3619\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"3897\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6E\" x=\"4398\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"4954\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-27\" x=\"5733\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6D\" x=\"6011\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-65\" x=\"6845\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-61\" x=\"7289\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6E\" x=\"7790\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-27\" x=\"8346\" y=\"0\"/>\n</g>\n<g transform=\"translate(0,-760)\">\n <use xlink:href=\"#E1-MJMAIN-69\"/>\n <use xlink:href=\"#E1-MJMAIN-66\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-72\" x=\"835\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-65\" x=\"1227\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-64\" x=\"1672\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-75\" x=\"2228\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-63\" x=\"2785\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-74\" x=\"3229\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-69\" x=\"3619\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"3897\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6E\" x=\"4398\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"4954\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-27\" x=\"5733\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-73\" x=\"6011\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-75\" x=\"6406\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-6D\" x=\"6962\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-27\" x=\"7796\" y=\"0\"/>\n</g>\n</g>\n</g>\n</g>\n</g>\n</svg></p>\n<h4 id=\"nn-NLLLoss\"><code>nn.NLLLoss</code><a href=\"pytorch-learning-note-3#nn-NLLLoss\"></a></h4><p><strong>Negative Log Liklihood(NLL) Loss f 负对数似然损失函数</strong></p>\n<p style=\"text-align:center\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"23.368ex\" height=\"2.843ex\" style=\"vertical-align: -0.838ex;\" viewbox=\"0 -863.1 10061.2 1223.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">loss(x, class) = -x_{class}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-73\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-63\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-61\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"298\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"784\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"1253\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1723\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"2112\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"2685\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-63\" x=\"3130\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"3563\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-61\" x=\"3862\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"4391\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"4861\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"5330\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"5997\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"7054\" y=\"0\"/>\n<g transform=\"translate(7832,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(572,-150)\">\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6C\" x=\"433\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-61\" x=\"732\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1261\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1731\" y=\"0\"/>\n</g>\n</g>\n</g>\n</svg></p>\n<h4 id=\"nn-MSELoss\"><code>nn.MSELoss</code><a href=\"pytorch-learning-note-3#nn-MSELoss\"></a></h4><p><strong>Mean Squrare Error(MSE) Loss 均方损失函数</strong></p>\n<p style=\"text-align:center\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"28.174ex\" height=\"5.176ex\" style=\"vertical-align: -1.838ex;\" viewbox=\"0 -1437.2 12130.4 2228.5\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">loss(x, y) = \\frac1n \\sum(x_i - y_i)^2</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-73\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-79\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6E\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ2-2211\" d=\"M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-69\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"298\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"784\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"1253\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1723\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"2112\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"2685\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-79\" x=\"3130\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"3627\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"4294\" y=\"0\"/>\n<g transform=\"translate(5073,0)\">\n<g transform=\"translate(397,0)\">\n<rect stroke=\"none\" width=\"720\" height=\"60\" x=\"0\" y=\"220\"/>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"110\" y=\"676\"/>\n <use xlink:href=\"#E1-MJMATHI-6E\" x=\"60\" y=\"-686\"/>\n</g>\n</g>\n <use xlink:href=\"#E1-MJSZ2-2211\" x=\"6478\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"7922\" y=\"0\"/>\n<g transform=\"translate(8312,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"9451\" y=\"0\"/>\n<g transform=\"translate(10452,0)\">\n <use xlink:href=\"#E1-MJMATHI-79\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"693\" y=\"-213\"/>\n</g>\n<g transform=\"translate(11286,0)\">\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMAIN-32\" x=\"550\" y=\"583\"/>\n</g>\n</g>\n</svg></p>\n<h4 id=\"nn-CrossEntropyLoss\"><code>nn.CrossEntropyLoss</code><a href=\"pytorch-learning-note-3#nn-CrossEntropyLoss\"></a></h4><p>多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数,可以理解为CrossEntropyLoss()=log_softmax() + NLLLoss()</p>\n<p style=\"text-align:center\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"65.045ex\" height=\"7.676ex\" style=\"vertical-align: -3.338ex;\" viewbox=\"0 -1867.7 28005.4 3304.9\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">loss(x, class) = -\\log \\frac{\\exp(x_{class})}{\\sum_j \\exp(x_j)} = - x_{class} + \\log \\left(\\sum_j \\exp(x_j) \\right)</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-73\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-63\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-61\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6C\" d=\"M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-67\" d=\"M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-65\" d=\"M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-78\" d=\"M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-70\" d=\"M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ1-2211\" d=\"M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6A\" d=\"M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ2-2211\" d=\"M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ4-28\" d=\"M758 -1237T758 -1240T752 -1249H736Q718 -1249 717 -1248Q711 -1245 672 -1199Q237 -706 237 251T672 1700Q697 1730 716 1749Q718 1750 735 1750H752Q758 1744 758 1741Q758 1737 740 1713T689 1644T619 1537T540 1380T463 1176Q348 802 348 251Q348 -242 441 -599T744 -1218Q758 -1237 758 -1240Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ4-29\" d=\"M33 1741Q33 1750 51 1750H60H65Q73 1750 81 1743T119 1700Q554 1207 554 251Q554 -707 119 -1199Q76 -1250 66 -1250Q65 -1250 62 -1250T56 -1249Q55 -1249 53 -1249T49 -1250Q33 -1250 33 -1239Q33 -1236 50 -1214T98 -1150T163 -1052T238 -910T311 -727Q443 -335 443 251Q443 402 436 532T405 831T339 1142T224 1438T50 1716Q33 1737 33 1741Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"298\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"784\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"1253\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1723\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"2112\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"2685\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-63\" x=\"3130\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"3563\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-61\" x=\"3862\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"4391\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"4861\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"5330\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"5997\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"7054\" y=\"0\"/>\n<g transform=\"translate(7999,0)\">\n <use xlink:href=\"#E1-MJMAIN-6C\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-67\" x=\"779\" y=\"0\"/>\n</g>\n<g transform=\"translate(9278,0)\">\n<g transform=\"translate(286,0)\">\n<rect stroke=\"none\" width=\"5007\" height=\"60\" x=\"0\" y=\"220\"/>\n<g transform=\"translate(235,770)\">\n <use xlink:href=\"#E1-MJMAIN-65\"/>\n <use xlink:href=\"#E1-MJMAIN-78\" x=\"444\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-70\" x=\"973\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1529\" y=\"0\"/>\n<g transform=\"translate(1919,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(572,-150)\">\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6C\" x=\"433\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-61\" x=\"732\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1261\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1731\" y=\"0\"/>\n</g>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"4147\" y=\"0\"/>\n</g>\n<g transform=\"translate(60,-771)\">\n <use xlink:href=\"#E1-MJSZ1-2211\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6A\" x=\"1494\" y=\"-405\"/>\n<g transform=\"translate(1614,0)\">\n <use xlink:href=\"#E1-MJMAIN-65\"/>\n <use xlink:href=\"#E1-MJMAIN-78\" x=\"444\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-70\" x=\"973\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"3144\" y=\"0\"/>\n<g transform=\"translate(3533,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6A\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"4498\" y=\"0\"/>\n</g>\n</g>\n</g>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"14970\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"16027\" y=\"0\"/>\n<g transform=\"translate(16805,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(572,-150)\">\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6C\" x=\"433\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-61\" x=\"732\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1261\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-73\" x=\"1731\" y=\"0\"/>\n</g>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2B\" x=\"19256\" y=\"0\"/>\n<g transform=\"translate(20257,0)\">\n <use xlink:href=\"#E1-MJMAIN-6C\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-67\" x=\"779\" y=\"0\"/>\n</g>\n<g transform=\"translate(21536,0)\">\n <use xlink:href=\"#E1-MJSZ4-28\"/>\n<g transform=\"translate(792,0)\">\n <use xlink:href=\"#E1-MJSZ2-2211\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6A\" x=\"815\" y=\"-1536\"/>\n</g>\n<g transform=\"translate(2403,0)\">\n <use xlink:href=\"#E1-MJMAIN-65\"/>\n <use xlink:href=\"#E1-MJMAIN-78\" x=\"444\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-70\" x=\"973\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"3933\" y=\"0\"/>\n<g transform=\"translate(4322,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-6A\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"5286\" y=\"0\"/>\n <use xlink:href=\"#E1-MJSZ4-29\" x=\"5676\" y=\"0\"/>\n</g>\n</g>\n</svg></p>\n<h4 id=\"nn-BCELoss\"><code>nn.BCELoss</code><a href=\"pytorch-learning-note-3#nn-BCELoss\"></a></h4><p>Binary Cross Entropy</p>\n<p style=\"text-align:center\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"56.235ex\" height=\"6.343ex\" style=\"vertical-align: -3.005ex;\" viewbox=\"0 -1437.2 24212 2730.8\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">loss(x, t) = -\\frac1n \\sum_i \\left(t_i *\\log(x_i) + (1-t_i)*\\log(1-x_i) \\right)</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6C\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6F\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-73\" d=\"M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-78\" d=\"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-74\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-6E\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJSZ2-2211\" d=\"M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-69\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2217\" d=\"M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6C\" d=\"M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-67\" d=\"M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMAIN-2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-6C\" x=\"0\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-6F\" x=\"298\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"784\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-73\" x=\"1253\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"1723\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"2112\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2C\" x=\"2685\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMATHI-74\" x=\"3130\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"3491\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-3D\" x=\"4158\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"5215\" y=\"0\"/>\n<g transform=\"translate(5993,0)\">\n<g transform=\"translate(120,0)\">\n<rect stroke=\"none\" width=\"720\" height=\"60\" x=\"0\" y=\"220\"/>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"110\" y=\"676\"/>\n <use xlink:href=\"#E1-MJMATHI-6E\" x=\"60\" y=\"-686\"/>\n</g>\n</g>\n<g transform=\"translate(7120,0)\">\n <use xlink:href=\"#E1-MJSZ2-2211\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"848\" y=\"-1536\"/>\n</g>\n<g transform=\"translate(8732,0)\">\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"0\" y=\"0\"/>\n<g transform=\"translate(389,0)\">\n <use xlink:href=\"#E1-MJMATHI-74\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"511\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-2217\" x=\"1317\" y=\"0\"/>\n<g transform=\"translate(2040,0)\">\n <use xlink:href=\"#E1-MJMAIN-6C\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-67\" x=\"779\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"3319\" y=\"0\"/>\n<g transform=\"translate(3709,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"4626\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2B\" x=\"5237\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"6238\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"6627\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"7350\" y=\"0\"/>\n<g transform=\"translate(8351,0)\">\n <use xlink:href=\"#E1-MJMATHI-74\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"511\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"9057\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2217\" x=\"9668\" y=\"0\"/>\n<g transform=\"translate(10391,0)\">\n <use xlink:href=\"#E1-MJMAIN-6C\"/>\n <use xlink:href=\"#E1-MJMAIN-6F\" x=\"278\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-67\" x=\"779\" y=\"0\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-28\" x=\"11671\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-31\" x=\"12060\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-2212\" x=\"12783\" y=\"0\"/>\n<g transform=\"translate(13784,0)\">\n <use xlink:href=\"#E1-MJMATHI-78\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"809\" y=\"-213\"/>\n</g>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"14700\" y=\"0\"/>\n <use xlink:href=\"#E1-MJMAIN-29\" x=\"15090\" y=\"0\"/>\n</g>\n</g>\n</svg></p>\n<h2 id=\"反向传播\">反向传播<a href=\"pytorch-learning-note-3#反向传播\"></a></h2><p>调用<code>loss.backward()</code>获得反向传播的误差。</p>\n<p>但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p>\n<p>现在，我们将调用<code>loss.backward()</code>，并查看conv1层的偏差（bias）项在反向传播前后的梯度。</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.zero_grad()  <span class=\"comment\"># 清除梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'conv1.bias.grad before backward'</span>)</span><br><span class=\"line\">print(net.conv1.bias.grad)</span><br><span class=\"line\"></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'conv1.bias.grad after backward'</span>)</span><br><span class=\"line\">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></div></figure>\n\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv1.bias.grad before backward</span><br><span class=\"line\"><span class=\"literal\">None</span></span><br><span class=\"line\">conv1.bias.grad after backward</span><br><span class=\"line\">tensor([ <span class=\"number\">0.0027</span>, <span class=\"number\">-0.0142</span>,  <span class=\"number\">0.0197</span>,  <span class=\"number\">0.0021</span>, <span class=\"number\">-0.0018</span>,  <span class=\"number\">0.0001</span>])</span><br></pre></td></tr></table></div></figure>\n\n<h2 id=\"更新权重\">更新权重<a href=\"pytorch-learning-note-3#更新权重\"></a></h2><p>在实践中最简单的权重更新规则是随机梯度下降（SGD）：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></div></figure>\n\n<p>我们可以使用简单的Python代码实现这个规则：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">learning_rate = <span class=\"number\">0.01</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></div></figure>\n\n<p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包<code>torch.optim</code>实现了所有的这些规则。 使用它们非常简单：</p>\n<figure class=\"highlight\"><div><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># create your optimizer</span></span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">criterion = nn.MSELoss()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># in your training loop</span></span><br><span class=\"line\">optimizer.zero_grad()   <span class=\"comment\"># zero the gradient buffers</span></span><br><span class=\"line\">output = net(input)</span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\">loss.backward()    <span class=\"comment\"># calculate gradients</span></span><br><span class=\"line\">optimizer.step()   <span class=\"comment\"># Does the update</span></span><br></pre></td></tr></table></div></figure>\n\n","prev":{"title":"[PyTorch学习笔记] Training A Classifier","link":"pytorch-learning-note-4"},"next":{"title":"[PyTorch学习笔记] AUTOGRAD: Automatic Differentiation","link":"pytorch-learning-note-2"},"plink":"https://yuxinzhao.net/pytorch-learning-note-3/","toc":[{"title":"定义网络","id":"定义网络","index":"1"},{"title":"损失函数","id":"损失函数","index":"2","children":[{"title":"常见损失函数","id":"常见损失函数","index":"2.1","children":[{"title":"<code>nn.L1Loss</code>","id":"nn-L1Loss","index":"2.1.1"},{"title":"<code>nn.NLLLoss</code>","id":"nn-NLLLoss","index":"2.1.2"},{"title":"<code>nn.MSELoss</code>","id":"nn-MSELoss","index":"2.1.3"},{"title":"<code>nn.CrossEntropyLoss</code>","id":"nn-CrossEntropyLoss","index":"2.1.4"},{"title":"<code>nn.BCELoss</code>","id":"nn-BCELoss","index":"2.1.5"}]}]},{"title":"反向传播","id":"反向传播","index":"3"},{"title":"更新权重","id":"更新权重","index":"4"}],"reward":true,"copyright":{"author":"Yuxin Zhao","link":"<a href=\"https://yuxinzhao.net/pytorch-learning-note-3/\" title=\"[PyTorch学习笔记] Neural Networks\">https://yuxinzhao.net/pytorch-learning-note-3/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}